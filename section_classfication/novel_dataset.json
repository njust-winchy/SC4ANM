[{"id": "O9DAoNnYVlM", "paper_content": "Title\nFederated Learning via Plurality Vote\nAbstract\nFederated learning allows collaborative workers to solve a machine learning problem while preserving data privacy. Recent studies have tackled various challenges in federated learning, but the joint optimization of communication overhead, learning reliability, and deployment efficiency is still an open problem. To this end, we propose a new scheme named federated learning via plurality vote (FedVote). In each communication round of FedVote, workers transmit binary or ternary weights to the server with low communication overhead. The model parameters are aggregated via weighted voting to enhance the resilience against Byzantine attacks. When deployed for inference, the model with binary or ternary weights is resource-friendly to edge devices. We show that our proposed method can reduce quantization error and converges faster compared to the methods directly quantizing the model updates.\n1 INTRODUCTION\nFederated learning enables multiple workers to solve a machine learning problem under the coordination of a central server (Kairouz et al., 2021). Throughout the training stage, client data will be kept locally and only model weights or model updates will be shared with the server. Federated averaging (FedAvg) (McMahan et al., 2017) was proposed as a generic federated learning solution. Although FedAvg takes advantage of distributed client data while maintaining their privacy, it leaves the following two challenges unsolved. First, transmitting high-dimensional messages between a client and the server for multiple rounds can incur significant communication overhead. Quantization has been incorporated into federated learning in recent studies (Reisizadeh et al., 2020; Haddadpour et al., 2021). However, directly quantizing the gradient vector may not provide the optimal trade-off between communication efficiency and model accuracy 1. Second, the aggregation rule in FedAvg is vulnerable to Byzantine attacks (Blanchard et al., 2017). Prior works tackled this issue by using robust statistics such as coordinate-wise median and geometric median in the aggregation step (Blanchard et al., 2017; Yin et al., 2018). Another strategy is to detect and reject updates from malicious attackers (Mun\u0303oz-Gonza\u0301lez et al., 2019; Sattler et al., 2020). The robustness of the algorithm is enhanced at the cost of additional computation and increased complexity of algorithms.\nIn this paper, we propose a new method called federated learning via plurality vote (FedVote). We train a neural network at each worker with a range normalization function applied to model parameters. After local updating, binary/ternary weight vectors are obtained via stochastic rounding and sent to the server. The global model is updated by a voting procedure, and the voting results are sent back to each worker for further optimization in the next round. The contributions of the paper are summarized as follows.\n1. We present FedVote as a novel federated learning solution to jointly optimize the communication overhead, learning reliability, and deployment efficiency.\n2. We theoretically and experimentally verify the effectiveness of our FedVote design. In bandwidth-limited scenarios, FedVote is particularly advantageous in simultaneously achieving a high compression ratio and good test accuracy. Given a fixed communication cost, FedVote improves model accuracy on the CIFAR-10 dataset by 5\u201310%, 15\u201320%,\n1Predictive coding in video and image compression (Li et al., 2015; Gonzalez & Woods, 2014) is an example that directly quantizing the raw signal we intend to transmit does not provide the best trade-off between the coding efficiency and the utility/bitrate.\nand 25\u201330% compared with FedPAQ (Reisizadeh et al., 2020), signSGD (Bernstein et al., 2018), and FedAvg, respectively.\n3. We extend FedVote to incorporate reputation-based voting. The proposed method, Byzantine-FedVote, exhibits much better resilience to Byzantine attacks in the presence of close to half attackers without incurring excessive computation compared with existing algorithms.\n2 RELATED WORK\nCommunication-Efficient Federated Learning. In the prior study of federated learning, various strategies have been proposed to reduce communication cost. One research direction is to reduce the size of messages in each round. For example, Bernstein et al. (2018) showed that sign-based gradient descent schemes can converge well in the homogeneous data distribution scenario, while Chen et al. (2020); Jin et al. (2020); Safaryan & Richtarik (2021) extended it to the heterogeneous data distribution setting. In parallel, FedAvg adopts a periodic averaging scheme and targets at reducing the number of communication rounds (McMahan et al., 2017). Hybrid methods consider simultaneous local updates and accumulative gradient compression (Reisizadeh et al., 2020; Haddadpour et al., 2021). In this work, we improve communication efficiency by employing binary/ternary weights in the neural network.\nQuantized Neural Networks. Quantized neural networks aim to approximate the full-precision networks using quantized weights while keeping their generalizability. As a special case, binary neural networks (BNNs) are gaining popularity in recent years. By restricting model weight values to {\u22121,+1}, BNNs can reduce computational cost, memory requirement, and energy consumption. Hubara et al. (2016) introduced real-valued latent weights and used the sign operator for binarization. In contrast, Shayer et al. (2018) let the neural network learn the distribution of the binary or ternary weights. Gong et al. (2019) added a soft quantization function to the real-valued weights, thus avoiding the gradient mismatch between the forward and the backward passes. A more thorough survey on BNN optimization can be found in Qin et al. (2020a).\nDistributed Optimization of Quantized Neural Networks. A few recent works have explored BNN optimization in a distributed setting, which is more relevant to our work. Lin et al. (2020) conducted the case study of 1-bit quantized local models aggregated via ensemble distillation. The aggregation becomes complicated due to the separate optimization stage of knowledge distillation, especially when the distillation algorithm does not converge well in practice. In addition, their BNN optimization is not tailored to the federated learning setting. Hou et al. (2019) theoretically analyzed the convergence of distributed quantized neural networks, which was later implemented in the application of intrusion detection (Qin et al., 2020b). In comparison, FedVote is presented as a new algorithm by leveraging client local updates to accelerate the training. Different from Hou et al. (2019), we do not assume a convex and twice differentiable objective function and bounded gradients. Therefore, the analyses in Hou et al. (2019) cannot be directly applied to our study.\nByzantine Resilience in Federated Learning. An attack is Byzantine if arbitrary outputs are produced due to the adversary (Kairouz et al., 2021). Blanchard et al. (2017) showed that FedAvg cannot tolerate a single Byzantine attacker. They proposed an aggregation-rule-based remedy using the similarity of local updates. Similarly, Yin et al. (2018) took the advantage of coordinatewise median and trimmed-mean to robustify the aggregation. Mun\u0303oz-Gonza\u0301lez et al. (2019) and Sattler et al. (2020) detected the adversaries and filtered out their model updates. In this paper, we propose a reputation-based voting strategy for FedVote that is shown to have good convergence performance in the Byzantine setting.\n3 PRELIMINARIES\nSymbol conventions are as follows. Bold lower cases of letters such as vm denote column vectors, and vm,i is used to denote its ith entry. For a scalar function, it applies elementwise operation when a vector input is given. 1 = [1, . . . , 1]> denotes a vector with all entries equal to 1.\n3.1 FEDERATED LEARNING\nThe goal of federated learning is to build a machine learning model based on the training data distributed among multiple workers. To facilitate the learning procedure, a server will coordinate the training without seeing the raw data (Kairouz et al., 2021). In a supervised learning scenario, let Dm = {(xm,j ,ym,j)}nmj=1 denote the training dataset on the mth worker, with the input xm,j \u2208 Rd1 and the label ym,j \u2208 Rd2 in each training pair. The local objective function fm with a model weight vector \u03b8 \u2208 Rd is given by\nfm(\u03b8) , fm(\u03b8;Dm) = 1\nnm nm\u2211 j=1 `(\u03b8; (xm,j ,ym,j)), (1)\nwhere ` is a loss function quantifying the error of model \u03b8 predicting the label ym,j for an input xm,j . A global objective function may be formulated as\nmin \u03b8\u2208Rd\nf(\u03b8) = 1\nM M\u2211 m=1 fm(\u03b8). (2)\n3.2 QUANTIZED NEURAL NETWORKS\nConsider a neural network g with the weight vector \u03b8 \u2208 Rd. A forward pass for an input x \u2208 Rd1 and a prediction y\u0302 \u2208 Rd2 can be written as y\u0302 = g(\u03b8,x). In quantized neural networks, the realvalued \u03b8 is replaced by w \u2208 Ddn, where Dn is a discrete set with a number n of quantization levels. For example, we may have D2 = {\u22121, 1} for a binary neural network. For a given training set {(xj ,yj)}Nj=1 and the loss function `, the goal is to find an optimal w\u2217 such that the averaged loss is minimized over a search space of quantized weight vectors:\nw? = argmin w\u2208Ddn\n1\nN N\u2211 j=1 `(w; (xj ,yj)). (3)\nPrior studies tried to solve (3) by optimizing a real-valued latent weight vector h \u2208 Rd (Hubara et al., 2016; Shayer et al., 2018; Gong et al., 2019). The interpretations of the latent weight vary when viewed from different perspectives. Hubara et al. (2016) used the sign operation to quantize the latent weight into two levels during the forward pass. The binary weights can be viewed as an approximation of their latent real-valued counterparts. Shayer et al. (2018) trained a stochastic binary neural network, and the normalized latent parameters are interpreted as the Bernoulli distribution parameter \u03d1i: \u03d1i , P\u0302(wi = 1) = S(hi), wi \u2208 {0, 1}, (4) where S : R\u2192 (0, 1) is the sigmoid function. In the forward pass, instead of using the binary vector w, its expected value,\nw\u0303sto-bnn , E[w] = \u22121\u00d7[1\u2212S(h)] + 1\u00d7S(h) = 2S(h)\u2212 1, (5) will participate in the actual convolution or matrix multiplication operations. In other words, the neural network function becomes y\u0302 = g(w\u0303sto-bnn,x). Likewise, Gong et al. (2019) normalized the latent weight but interpreted it from a different viewpoint. They approximated the staircase quantization function with a differentiable soft quantization (DSQ) function, i.e.,\nw\u0303dsq , tanh(ah), (6) where tanh : R \u2192 (\u22121, 1) is the hyperbolic tangent function, and a controls the shape of the function. The neural network function thus becomes y\u0302 = g(w\u0303dsq,x).\nWe now summarize the latent-weight-based BNN training methods and depict an example of a single-layer network in Figure 1. First, a real-valued vector h \u2208 Rd is introduced and its range is restricted using a differentiable and invertible normalization function \u03d5 : R\u2192 (\u22121, 1). The forward pass is then calculated with the normalized weight vector w\u0303. The procedure is described as:\ny\u0302 = g(w\u0303,x), w\u0303 , \u03d5(h). (7) Second, in the back propagation, the latent weight vector h is updated with its gradient, i.e., h(t+1) = h(t) \u2212 \u03b7\u2207h`. Finally, the normalized weight vector w\u0303 are mapped to the discrete space to approximate w\u2217 via thresholding or stochastic rounding.\n4 PROPOSED FEDVOTE ALGORITHM\nIn this section, we present our proposed method with an emphasis on uplink communication efficiency and enhanced Byzantine resilience. We follow the widely adopted analysis framework in wireless communication to investigate only the worker uplink overhead, assuming that the downlink bandwidth is much larger and the server will have enough transmission power (Tran et al., 2019). To reduce the message size per round, we train a quantized neural network under the federated learning framework. The goal is to find a quantized weight vector w\u2217 that minimizes the global objective function f formulated in (2), i.e.,\nw\u2217 = argmin w\u2208Ddn f(w). (8)\nFor the simplicity of presentation, we mainly focus on the BNN case with D2 = {\u22121, 1}. We illustrate the procedure in Figure 2 and provide the pseudo code in Appendix B. Below, we explain each step in more detail.\n4.1 LOCAL MODEL TRAINING AND TRANSMISSION\nWe optimize a neural network with a learnable latent weight vector h. In the kth communication round, we assume all workers are identically initialized by the server, namely, \u2200 m \u2208 {1, . . . ,M}, h(k,0)m = h\n(k). To reduce the total number of communication rounds, we first let each worker conduct local updates to learn the binary weights. For each local iteration step, the local latent weight vector h(k,t+1) is updated by the gradient descent:\nh(k,t+1)m = h (k,t) m \u2212 \u03b7 \u2207hfm ( \u03d5(h(k,t)m ); \u03be (k,t) m ) , t \u2208 {0, . . . , \u03c4 \u2212 1}, (9)\nwhere \u03be(k,t)m is a mini-batch randomly drawn fromDm at the tth iteration of round k. After updating for \u03c4 steps, we obtain h(k,\u03c4)m and the corresponding normalized weight vector w\u0303 (k,\u03c4) m \u2208 (\u22121, 1)d defined as follows: w\u0303(k,\u03c4)m , \u03d5(h (k,\u03c4) m ). (10)\nTo reduce the message size, we use the stochastic rounding to draw a randomly quantized version w (k,\u03c4) m using w\u0303 (k,\u03c4) m , namely,\nw (k,\u03c4) m,i =\n{ +1, with probability \u03c0(k,\u03c4)m,i = 1 2 [ w\u0303 (k,\u03c4) m,i + 1 ] ,\n\u22121, with probability 1\u2212 \u03c0(k,\u03c4)m,i . (11)\nIt can be shown that the stochastic rounding is an unbiased procedure, i.e., E[w(k,\u03c4)m \u2223\u2223w\u0303(k,\u03c4)m ] = w\u0303 (k,\u03c4) m . After quantization, the local worker will send the binary weights w (k,\u03c4) m to the server for the global model aggregation.\n4.2 GLOBAL MODEL AGGREGATION AND BROADCAST\nOnce the server gathers the binary weights from all workers, it will perform the aggregation via plurality vote, i.e., w(k+1) = sign (\u2211M m=1 w (k,\u03c4) m ) . A tie in vote will be broken randomly. In the following lemma, we show that the probability of error reduces exponentially as the number of workers increases. The proof can be found in Appendix D.1. Lemma 1 (One-Shot FedVote) Let w\u2217 \u2208 Dd2 be the optimal solution defined in (8). For the mth worker, \u03b5m,i , P(w(k,\u03c4)m,i 6= w\u2217i ) denotes the error probability of the voting result of the ith coordinate. Suppose the error events {w(k,\u03c4)m,i 6= w\u2217i }Mm=1 are mutually independent, and the mean error probability si = 1M \u2211M m=1 \u03b5m,i is smaller than 1 2 . For the voted weight w (k+1), we have\nP ( w\n(k+1) i 6= w \u2217 i ) 6 [ 2si exp(1\u2212 2si) ]M 2 . (12)\nIn practice, the number of available workers may be limited in each round, and the local data distribution is often heterogenous or even time-variant. Therefore, it is almost always desirable to execute FedVote in a multiple-round fashion. In this case, we first use the soft voting to build an empirical distribution of global weight w, i.e.,\nP\u0302(w(k+1)i = 1) = 1\nM M\u2211 m=1 1 ( w (k,\u03c4) m,i = 1 ) , (13)\nwhere 1 (\u00b7) \u2208 {0, 1} is the indicator function. Let p(k+1)i , P\u0302(w (k+1) i = 1) and p (k+1) = [p (k+1) 1 , . . . , p (k+1) d ] >. The global latent parameters can be constructed by following (10):\nh(k+1) = \u03d5\u22121(2p(k+1) \u2212 1), (14) where \u03d5\u22121 : (\u22121, 1) \u2192 R is the inverse of the normalization function \u03d5. We further apply clipping to restrict the range of the probability, namely, clip(p(k+1)i ) = max(pmin,min(pmax, p (k+1) i )), where pmin, pmax \u2208 (0, 1) are predefined thresholds. To keep the notation consistent, we denote w\u0303(k+1) , \u03d5(h(k+1)) as the global normalized weight. After broadcasting the soft voting results p(k+1), all workers are synchronized with the same latent weight h(k+1) and normalized weight w\u0303(k+1). The learning procedure will repeat until a termination condition is satisfied. We relate FedVote to FedAvg in the following lemma. The detailed proof can be found in Appendix D.2. Lemma 2 (Relationship with FedAvg) For the normalized weights, FedVote recovers FedAvg in\nexpectation: E [ w\u0303(k+1) ] = 1M M\u2211 m=1 w\u0303 (k,\u03c4) m , where w\u0303(k+1) = \u03d5(h(k+1)) and w\u0303 (k,\u03c4) m = \u03d5(h (k,\u03c4) m ).\n4.3 REPUTATION-BASED BYZANTINE-FEDVOTE\nLemma 2 shows that FedVote is related to FedAvg in expectation. As we have reviewed in Section 2, FedAvg cannot tolerate a single Byzantine attacker. It indicates that FedVote will exhibit similar poor performance in the presence of multiple adversaries (see Appendix C.2). We improve the design of FedVote based on a reputation voting mechanism, which in essence is a variant of the weighted soft voting method.\nReputation-based voting was presented in failure-robust large scale grids (Bendahmane et al., 2014). In our design, we modify (13) to P\u0302(w(k+1)i = 1) = \u2211M m=1 \u03bb (k) m 1 ( w (k,\u03c4) m,i = 1 ) , where \u03bb(k)m is proportional to a credibility score. In Byzantine-resilient FedVote (Byzantine-FedVote), we assume that at least 50% of the workers behave normally and treat the plurality vote result as the correct decision. The credibility score of the mth worker is calculated by counting the number of correct votes it makes: CR(k+1)m = 1 d \u2211d i=1 1 ( w (k,\u03c4) m,i = w (k+1) i ) . Through multiple rounds, we track the credibility of a local worker by taking an exponential moving average over the communication rounds, namely, \u03bd(k+1)m = \u03b2 \u03bd (k) m + (1 \u2212 \u03b2) CR(k+1)m , where \u03b2 \u2208 (0, 1) is a predefined coefficient. The weight \u03bb(k)m is designed as \u03bb (k) m = \u03bd (k) m / \u2211M m=1 \u03bd (k) m .\nEven though the presentation in this section focuses on the binary weights, the scheme can be naturally extended to quantized neural networks of more discrete levels. We will briefly discuss the implementation for ternary weights in Section 6.\n5 ANALYSIS OF ALGORITHM\nIn this section, we present the theoretical analysis of FedVote when local data are independent and identically distributed (i.i.d.). The empirical results in the non-i.i.d. setting will be discussed in Section 6, and the corresponding analyses are left for future work. We use the gradient norm expectation as an indicator of convergence, which is commonly adopted in nonconvex optimization literature (Reisizadeh et al., 2020; Haddadpour et al., 2021). To simplify the notation, we first denote the stochastic local gradient by g\u0303(k,t)m , \u2207hfm(\u03d5(h (k,t) m ); \u03be (k,t) m ). The local true gradient and\nglobal true gradient will be denoted by g(k,t)m , E\u03be [ g\u0303(k,t)m ] , g(k) , \u2207hf(\u03d5(h(k))), respectively. In addition, let \u03b6(k)m , w\u0303 (k,\u03c4) m \u2212w(k,\u03c4)m denote the error introduced by stochastic rounding. According to the unbiased property of stochastic rounding, we have E\u03c0[\u03b6(k)m ] = 0. With the aforementioned notations, we state five assumptions for the convergence analysis.\n5.1 ASSUMPTIONS\nAssumption 1 (Lower bound) \u2200 h \u2208 Rd, w\u0303 \u2208 (\u22121, 1)d, the objective function is lower bounded by a constant f\u2217, i.e., f(w\u0303) > f\u2217 = minh\u2208Rd f(\u03d5(h)). Assumption 2 (L-smoothness) \u2200 w\u03031, w\u03032 \u2208 (\u22121, 1)d, m \u2208 {1, . . . ,M}, there exists some nonnegative L such that \u2016\u2207fm(w\u03031)\u2212\u2207fm(w\u03032)\u20162 6 L \u2016w\u03031 \u2212 w\u03032\u20162. Assumptions 1 to 2 are common for necessary analyses (Wang & Joshi, 2018). We limit the range of the normalized weight w\u0303 while in a typical setting there is no restriction to the model weight. Assumption 3 The normalization function \u03d5 : R \u2192 (\u22121, 1) is strictly increasing. In particular, we assume its first derivative is bounded for all h(k,t)m,i , i.e., d dh\u03d5(h (k,t) m,i ) \u2208 [c1, c2], where c1, c2 are positive parameters independent of k, t, m, and i.\nAssumption 3 is not difficult to satisfy in practice. For example, let \u03d5(h) = tanh(ah), we have \u03d5\u2032(h) = a [ 1\u2212 tanh2(ah) ] , with c2 = a. Note that \u03d5 quickly saturates with a large h in the local updating. On the other hand, the empirical Bernoulli parameter pi will be clipped for stability, which indicates that h(k,t)m,i will be upper bounded by certain hB. In this sense, we have c1 = a [ 1\u2212 tanh2(ahB) ] . The next two assumptions bound the variance of the stochastic gradient and quantization noise.\nAssumption 4 The stochastic gradient has bounded variance, i.e., E \u2225\u2225g(k,t)m \u2212 g\u0303(k,t)m \u2225\u222522 6 \u03c32\u03b5 , where \u03c32\u03b5 is a fixed variance independent of k, t and m.\nAssumption 5 The quantization error \u03b6(k)m has bounded variance, i.e., E \u2225\u2225\u03b6(k)m \u2225\u222522 6 \u03c32k, where \u03c32k is a fixed variance independent of m.\nNote that quantization error is affected by the quantizer type and the corresponding input. For example, if the normalization function \u03d5 approximates the sign function very well, the stochastic quantization error will be close to zero. Formally, the upper bound \u03c32\u03b6 can be viewed as a function of input dimension d, which we formulate in the following lemma. The proof is in Appendix D.3. Lemma 3 Suppose we have an input a \u2208 (\u22121, 1)d for the quantizer Qsr defined in (11), then the quantization error satisfies E\n[\u2225\u2225Qsr(a)\u2212 a\u2225\u222522\u2223\u2223a] = d\u2212 \u2225\u2225a\u2225\u222522. For existing algorithms quantizing the model update \u03b4(k)m , \u03b8\n(k) \u2212 \u03b8(k,\u03c4)m , the quantizer has the property E [\u2225\u2225Q(x)\u2212 x\u2225\u22252 2 \u2223\u2223x] 6 q\u2225\u2225x\u2225\u22252 2 . With a fixed quantization step, q increases when the input dimension d increases (Basu et al., 2020). We state the result for a widely-used quantizer, QSGD (Alistarh et al., 2017), which has been adopted in FedPAQ, in the following lemma. Lemma 4 Suppose we have an input x \u2208 Rd for the QSGD quantizer Q. In the coarse quantization scenario, the quantization error satisfies E [\u2225\u2225Q(x)\u2212 x\u2225\u22252 2 \u2223\u2223x] = O (d 12)\u2225\u2225x\u2225\u22252 2 .\n5.2 CONVERGENCE ANALYSIS\nWe state the convergence results in the following theorem. The proof can be found in Appendix D.5. Theorem 1 For FedVote under Assumptions 1 to 5, let the learning rate \u03b7 = O (\n( c1c2 ) 2 1 L\u03c4 \u221a K\n) , then\nafter K rounds of communication, we have\n1\nK K\u22121\u2211 k=0 c21E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 6\n2 [ f(w\u0303(0))\u2212 f(w\u0303\u2217) ] \u03b7\u03c4K + c22L\u03b7 [ 1 M + c21L\u03b7(\u03c4 \u2212 1) 2 ] \u03c32\u03b5\n+ L\n\u03b7\u03c4KM K\u22121\u2211 k=0 \u03c32k + 2(c22 \u2212 c21) \u03c4MK K\u22121\u2211 k=0 M\u2211 m=1 R(k)m . (15)\nwhere R(k)m , \u2212 \u03c4\u22121\u2211 t=0 \u2211 i/\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207f(w\u0303(k,t)m ))i ] and I(k,t)m , { i | g(k)i g (k,t) m,i > 0 } .\nRemark 1 When there is no normalization function and quantization, i.e., \u03d5(x) = x with c1 = c2 = 1, \u03c32k = 0, Theorem 1 recovers the result obtained in Wang & Joshi (2018).\nTo discuss the impact of quantization error, consider the distribution of different inputs. For the model update \u03b4(k)m , we expect the central limit theorem to render its distribution shape, where each entry \u03b4(k)m,i follows the Gaussian distribution. For the Bernoulli probability\u03c0(k,\u03c4)m , we expect the Beta distribution as the conjugate prior to render its distribution shape, where each entry \u03c0 (k,\u03c4) m,i follows the symmetric Beta distribution. See Figure 3 for the empirical results.\nRemark 2 Following the analysis framework in Theorem 1, the order-wise convergence rate is 1 K \u2211K\u22121 k=0 E \u2225\u2225\u2207f (w(k))\u2225\u22252 2 = O( 1\u221a K ) +E(d), where E(d) is the error introduced by the quantization. For FedVote with the normalized weight w\u0303(k,\u03c4)m as the input, when \u03c0 (k,\u03c4) m,i \u2019s follow the symmetric Beta distribution, it can be shown that E\u2016\u03b6(k)m \u201622 = O (d) based on Lemma 3. For algorithms such as FedPAQ with the model update \u03b4(k)m as the input, when \u03b4 (k) m,i\u2019s follow the Gaussian distribution, it\ncan be shown that E \u2225\u2225Q(\u03b4(k)m )\u2212\u03b4(k)m \u2225\u222522 = O (d3/2) based on Lemma 4. When the weight dimension d is sufficiently large, FedVote converges faster.\nRemark 3 The value of the positive scalar error term R(k)m in (15) depends on the gradient dissimilarity. If the angle between the local gradient \u2207fm(w\u0303(k,t)m ) and the global gradient \u2207f(w\u0303(k,t)) is not large, R(k)m can be treated as a bounded variable.\nRemark 4 The choice of nonlinear function \u03d5 : Rd \u2192 (\u22121, 1)d will affect the convergence. If \u03d5 behaves more like the sign(\u00b7) function, e.g., when a increases in tanh(ax), the quantization error will be reduced. In other words, we expect a smaller \u03c32k according to Lemma 3, which leads to a tighter bound in (15). Meanwhile, a larger c2 will negatively influence the convergence.\n6 EXPERIMENTAL RESULTS\nData and Models. We choose image classification datasets Fashion-MNIST (Xiao et al., 2017) and CIFAR-10 (Krizhevsky, 2009). Both of them have a total of C = 10 classes. We consider two data partition strategies: (i) i.i.d. setting where the whole dataset is randomly shuffled and assigned to each worker without overlap; (ii) non-i.i.d. setting where we follow Hsu et al. (2019) and use the Dirichlet distribution to simulate the heterogeneity. In particular, for the mth worker we draw a random vector qm \u223c Dir(\u03b1), where qm = [qm,1, \u00b7 \u00b7 \u00b7 , qm,C ]> belongs to the standard (C\u22121)-simplex. We then assign data samples from different classes to the mth worker following the distribution of\nTable 1: Test Accuracy in the Byzantine Setting\nDataset Distri-bution signSGD\nCoMed\nProposed\nFashionMNIST i.i.d. 61.5% 77.7% 91.4% non-i.i.d 61.0% 73.4% 89.4%\nCIFAR10 i.i.d. 13.5% 29.3% 76.6% non-i.i.d 11.0% 28.7% 72.0%\nTable 2: Effect of the Normalization Function\nFashionMNIST\na\n0.5 1.5 2.5 10\ni.i.d. float 90.7% 90.6% 90.0% 88.2% binary 88.7% 90.4% 89.9% 88.2%\nnoni.i.d.\nfloat 87.3% 86.9% 85.7% 85.0% binary 83.3% 85.5% 85.2% 84.6%\nqm. We set \u03b1 = 0.5 unless noted otherwise. We use a LeNet-5 architecture for Fashion-MNIST and a VGG-7 architecture for CIFAR-10. Results are obtained over three repetitions.\nImplementation Details. We provide implementation details in the proposed FedVote design. First, following prior works (Shayer et al., 2018; Gong et al., 2019), we keep the weights of the BNN final layer as floating-point values for the sake of the model performance. The weights of the final layer are randomly initialized with a shared seed and will be fixed during the training process. Second, we notice that for quantized neural networks, the batch normalization (BN) (Ioffe & Szegedy, 2015) after the convolutional layer is necessary to scale the activation. We use the static BN without learnable parameters and local statistics (Diao et al., 2021) to ensure the voting aggregation of binary weights. For the normalization function, we choose \u03d5(x) = tanh(3x/2) unless noted otherwise. More details of the experimental setup can be found in Appendix C.1.\nCommunication Efficiency and Convergence Rate. In this experiment, we consider M = 31 workers with full participation and compare FedVote to different methods within N = 100 communication rounds. The results of partial participation can be found in Appendix C.3. The communication cost is calculated as the accumulative uplink message size from all workers. Figure 4 reveals that FedVote outperforms the gradient-quantization-based methods such as signSGD (Bernstein et al., 2018) that quantizes gradients to 1 bit signs, and FedPAQ (Reisizadeh et al., 2020) that quantizes the updates to 2 bits integers. Compared with FedPAQ, signSGD, and FedAvg, FedVote improves the test accuracy by 5\u201310%, 15\u201320%, and 25\u201330%, respectively, given the fixed communication costs of 1.5\u20134.7 GB.\nByzantine Resilience. This experiment validates the effectiveness of Byzantine-FedVote. We consider omniscient attackers who can access the datasets of normal workers and send the opposite of the aggregated results to the server. The number of attackers is 15, and the remaining 16 clients are normal workers. We compare the proposed method with coordinate-wise median based (CoMed) gradient descent (Yin et al., 2018) and signSGD (Bernstein et al., 2018), and report the testing accuracy after 100 communication rounds. We do not compare with FedAvg and FedPAQ, as they are fragile to Byzantine failure. The results are shown in Table 1. It can be observed that ByzantineFedVote exhibits much better resilience to Byzantine attacks with close to half adversaries.\nTable 3: Test Accuracy of TNN and BNN\nDataset Distri-bution BNN TNN\nFashionMNIST i.i.d. 91.1% 91.9% non-i.i.d 88.3% 89.4%\nCIFAR10 i.i.d. 80.5% 82.5% non-i.i.d 74.6% 77.6%\nNormalization Function. From Remark 4, we know that the normalization function can influence the model convergence. We empirically examine the impact in this experiment. For normalization function \u03d5(x) = tanh(ax), we choose a from {0.5, 1.5, 2.5, 10}. We test the model accuracy after 20 communication rounds on Fashion-MNIST. The results are shown in Table 2. As a increases, the linear region of the normalization function shrinks, and the algorithm converges slower due to a larger c2. On the other hand, the gap between the model with normalized weight w\u0303 and the one with binary weight w also decreases due to smaller quantization errors. A good choice of the normalization function should take this trade-off into consideration.\nTernary Neural Network Extension. In the previous sections, we focus on the BNNs. We extend FedVote to ternary neural networks (TNNs) and empirically verify its performance. Training and transmitting the categorical distribution parameters of the ternary weight may bring additional communication and computation cost to edge devices, we therefore simplify the procedure as follows. For each ternary weight w(k,t)m,i , we still keep a latent parameter h (k,t) m,i to optimize. After \u03c4 local steps, we use the stochastic rounding to the normalized weight w\u0303(k,\u03c4)m = \u03d5(h(k,\u03c4)m ) and obtain quantized weight w(k,\u03c4)m . At the aggregation stage on the server, instead of calculating the vote distribution, we directly compute the global normalized weight as w\u0303(k+1) = 1M \u2211M m=1 w (k,\u03c4) m . More details can be found in Appendix C.4. The training results are shown in Table 3. As TNNs can further reduce the quantization error, their performance is better than the BNNs at the cost of additional 1 bit/dimension communication overhead.\nDeployment Efficiency. We highlight the advantages of BNNs during deployment on edge devices. In FedVote, we intend to deploy lightweight quantized neural networks on the workers after the training procedure. BNNs require 32\u00d7 smaller memory size, which can save storage and energy consumption for memory access (Hubara et al., 2016). As we do not quantize the activations, the advantage of BNNs inference mainly lies in the replacement of multiplications by summations. Consider the matrix multiplication in a neural network with an input x \u2208 Rd1 and output y \u2208 Rd2 : y = W>x. For a floating-point weight matrix W \u2208 Rd1\u00d7d2 , the number of multiplications is d1d2, whereas for a binary matrix Wb \u2208 Dd1\u00d7d22 all multiplication operations can be replaced by additions. We investigate the number of real multiplications and additions in the forward pass of different models and present the results in Table 4. We use the CIFAR-10 dataset and set the batch size to 100. As to the energy consumption calculation, we use 3.7 pJ and 0.9 pJ as in Hubara et al. (2016) for each floating-point multiplication and addition, respectively.\n7 CONCLUSION\nIn this work, we have proposed FedVote to jointly optimize communication overhead, learning reliability, and deployment efficiency. In FedVote, the server aggregates neural networks with binary/ternary weights via voting. We have verified that FedVote can achieve good model accuracy even in coarse quantization settings. Compared with gradient quantization, model quantization is a more effective design that achieves better trade-offs between communication efficiency and model accuracy. With the voting-based aggregation mechanism, FedVote enjoys the flexibility to incorporate various voting protocols to increase the resilience against Byzantine attacks. We have demonstrated that Byzantine-FedVote exhibits much better Byzantine resilience in the presence of close to half attackers compared to the existing algorithms.\nB FEDVOTE ALGORITHM\nWe summarize the proposed FedVote method and its Byzantine-resilient variant in Algorithm 1. When there are no attackers involved, we use Option I. When we require the resilience against Byzantine failure, we choose Option II.\nC SETUP AND ADDITIONAL EXPERIMENTS\n\nC.1 HYPERPARAMETERS\nFor the clipping thresholds, we set pmin = 0.001 and pmax = 1\u2212pmin. The thresholds are introduced for numerical stability and have little impact on performance. We use \u03b2 = 0.5 in Byzantine-FedVote. The choice of the smoothing factor \u03b2 has little impact on the final test accuracy, as the credibility score decays exponentially for adversaries over multiple communication rounds. We use the Adam optimizer and search using the learning rate \u03b7 over the set {10\u22124, 3\u00d710\u22124, 10\u22123, 3\u00d710\u22123, 10\u22122, 3\u00d7 10\u22122, 10\u22121, 3\u00d710\u22121}. We set the number of local iterations \u03c4 to 40 and the local batch size to 100.\nC.2 COMPARISON OF VANILLA FEDVOTE AND BYZANTINE-FEDVOTE\nLemma 2 shows that FedVote is related to FedAvg in expectation. Adversaries sending the opposite results will negatively affect the estimation of the weight distribution and impede the convergence in multiple rounds. We compare the test accuracy of Byzantine-FedVote, Vanilla FedVote, and signSGD on the non-i.i.d. CIFAR-10 dataset (\u03b1 = 0.5) with various numbers of omniscient attackers. Figure 6 reveals that the test accuracy of Vanilla FedVote drops severely when the number of adversaries increases, which is consistent with our analysis. In contrast, the drop of accuracy in Byzantine-FedVote is negligible, confirming its resilience to omniscient attackers.\nAlgorithm 1: Binary-Weight FedVote with/without Byzantine Tolerance 1 initialize p(0) and broadcast 2 for k = 0, 1, . . . , N \u2212 1 do 3 on mth worker: 4 receive p(k) from the server 5 initialize latent weight h(k,0)m = \u03d5\u22121(2p(k) \u2212 1) 6 for t = 0 : \u03c4 \u2212 1 do 7 g\u0303(k,t)m = \u2207hfm(\u03d5(h (k,t) m ); \u03be (t,r) m ) 8 h(k,t+1)m = h (k,t) m \u2212 \u03b7(k,t) g\u0303(k,t)m 9 w\u0303 (k,\u03c4) m = \u03d5(h (k,\u03c4))\n10 w (k,\u03c4) m = sto rounding(w\u0303 (k,\u03c4) m ) . Eq. (11) 11 send w(k,\u03c4)m to server 12 on server: 13 {w(k+1),p(k+1)} = vote({w(k,\u03c4)m }Mm=1) 14 broadcast p(k+1) to workers\n15 function vote({w(k,\u03c4)m }Mm=1) 16 for i = 1 : d do\n17 w (k+1) i = sign ( M\u2211 m=1 w (k,\u03c4) m,i )\n18\np (k+1) i = 1 M M\u2211 m=1 1 ( w\u0302 (k,\u03c4) m,i = 1 ) . Option I\n19 p (k+1) i = M\u2211 m=1 \u03bb (k) m 1 ( w (k,\u03c4) m,i = 1 ) . Option II\n20 return {w(k+1),p(k+1)} to the server\nC.3 CONVERGENCE OF FEDVOTE WITH PARTIAL PARTICIPATION\nWe increase the number of workers to 100 and sample 20 of them in each communication round. We compare FedVote with FedAvg McMahan et al. (2017), FedPAQ (Reisizadeh et al., 2020), and signSGD (Bernstein et al., 2018) in Figure 5 on the non-i.i.d. CIFAR-10 dataset (\u03b1 = 0.5) without changing other experimental setups. The observation is consistent with the results in Figure 4. FedVote outperforms gradient quantization methods such as FedPAQ and signSGD.\nC.4 EXTENSION TO TERNARY NEURAL NETWORKS\nThe stochastic rounding used in the ternary neural networks, wi = Qsr(w\u0303), is an extension of (11):\nwi = { +1, with probability \u03c01 = w\u0303i 1(w\u0303i > 0), \u22121, with probability \u03c02 = \u2212w\u0303i 1(w\u0303i < 0), 0, with probability 1\u2212 (\u03c01 + \u03c02).\n(20)\nOne can modify the normalization function to optimize neural networks with multiple quantization levels. For example, consider quaternary weight wi \u2208 {\u22122,\u22121, 1, 2}, the normalization function \u03d5(x) can be modified to \u03d5(x) = 2 tanh(ax).\nC.5 BATCH NORMALIZATION IN FEDVOTE\nBelow we review the commonly-adopted BN function for convenience of presentation. For a onedimensional input x(j) from the current batch B = {x(1), \u00b7 \u00b7 \u00b7 , x(nb)}, the output of BN layer is formulated as\ny , BN\u03b3,b(x(j)) = \u03b3 x(j) \u2212 \u00b5\u221a \u03c32 + + b, (21)\nwhere \u03b3, b are learnable affine transformation parameters, and \u00b5, \u03c32 are the mean and variance calculated over the batch samples. Note that the normal BN layer will introduce the real-valued parameters and track the statistics of the input, all of which may cause problems when being binarized in FedVote. Therefore, we choose to set the parameter-free static BN, i.e.,\ny\u2032 , BN(x(j)) = x\u2212 EB[x(j)]\u221a VarB[x(j)] + . (22)\nD MISSING PROOFS\n\nD.1 PROOF OF LEMMA 1\nLemma 1 (One-Shot FedVote) Let w\u2217 \u2208 Dd2 be the optimal solution defined in (8). For the mth worker, \u03b5m,i , P(w(k,\u03c4)m,i 6= w\u2217i ) denotes the error probability of the voting result of the ith coordinate. Suppose the error events {w(k,\u03c4)m,i 6= w\u2217i }Mm=1 are mutually independent, and the mean error probability si = 1M \u2211M m=1 \u03b5m,i is smaller than 1/2. For the voted weight w (k+1), we have\nP ( w\n(k+1) i 6= w \u2217 i ) 6 [ 2si exp(1\u2212 2si) ]M 2 . (23)\nProof. Let Xm,i , 1 ( w (k,\u03c4) m,i 6= w\u2217i ) , following Bernoulli distribution with parameter \u03b5m,i. Let\nYi = \u2211M m=1Xm,i, we have\nP ( w\n(k+1) i 6= w \u2217 i\n) = P ( Yi > M\n2\n) . (24)\nWith independent vote results from workers, Yi follows Poisson binomial distribution with mean \u00b5Yi = \u2211M m=1 \u03b5m,i. \u2200 a > 0, the Chernoff bound can be derived as\nP ( Yi > M\n2\n) (25a)\n= P(eaYi > e aM 2 ) (25b)\n\u00ac 6 exp ( \u2212aM\n2\n) E [ eaYi ] (25c)\n= exp ( \u2212aM\n2 ) M\u220f m=1 (1\u2212 \u03b5m,i + ea\u03b5m,i) (25d)\n= exp ( \u2212aM\n2 + M\u2211 m=1 ln (1 + \u03b5m,i(e a \u2212 1))\n) (25e)\n 6 exp ( \u2212aM\n2 + M\u2211 m=1 \u03b5m,i(e a \u2212 1)\n) . (25f)\nwhere \u00ac is based on Markov\u2019s inequality.  holds due to ln(1 + x) 6 x for all x \u2208 (\u22121,\u221e).\nBy assumption we have \u00b5Yi < M 2 . Let a = ln M 2\u00b5Yi , we have P ( Yi > M\n2\n) 6 exp ( \u2212\u00b5Yi + M2 )( M\n2\u00b5Yi\n)M 2\n(26a)\n6 ( 2\u00b5Yi M exp ( 1\u2212 2\u00b5Yi M ))M 2 . (26b)\nLet si = \u00b5Yi M = 1 M \u2211M m=1 \u03b5m,i and substitute it into (26b), the proof is complete.\nD.2 PROOF OF LEMMA 2\nLemma 2 (Relationship with FedAvg) For the normalized weights, FedVote recovers FedAvg in expectation: E [ w\u0303(k+1) ] = 1M M\u2211 m=1 w\u0303 (k,\u03c4) m , where w\u0303(k+1) = \u03d5(h(k+1)) and w\u0303 (k,\u03c4) m = \u03d5(h (k,\u03c4) m ).\nProof. From the inverse normalization in (14), we have w\u0303(k+1) = 2p(k+1)\u22121. Recall the definition of the empirical Bernoulli parameter p(k+1) given by (13), we have the elementwise expectation\nE\u03c0 [ w\u0303 (k+1) i ] = 1\nM M\u2211 m=1 ( 2P\u0302(w(k,\u03c4)m,i = 1)\u2212 1 ) (27a)\n\u00ac =\n1\nM M\u2211 m=1 \u03d5(h (k,\u03c4) m,i ), (27b)\nwhere \u00ac follows from stochastic rounding defined in (11). Based on the definition of range normalization in (10), for local normalized weight we have w(k,\u03c4)m,i = \u03d5(h (k,\u03c4) m,i ). Substituting the result into (27b) we have\nE\u03c0 [ w\u0303(k+1) ] = 1\nM M\u2211 m=1 w\u0303(k,\u03c4)m , (28)\nwhich completes the proof.\nD.3 PROOF OF LEMMA 3\nLemma 3 Suppose we have an input a \u2208 (\u22121, 1)d for the quantizer Qsr defined in (11), then the quantization error satisfies E [\u2225\u2225Qsr(a)\u2212 a\u2225\u222522\u2223\u2223a] = d\u2212 \u2225\u2225a\u2225\u222522.\nProof. Let a\u0302 , Qsr(a), we have\nE [\u2225\u2225Qsr(a)\u2212 a\u2225\u222522\u2223\u2223a] = E [ d\u2211 i=1 (a\u0302i \u2212 ai)2 \u2223\u2223a] (29a)\n= d\u2211 i=1 ( E [ a\u03022i \u2223\u2223a]\u2212 a2i ) (29b)\n= d\u2211 r=1 ( 1\u2212 a2i ) (29c)\n= d\u2212 \u2016a\u201622. (29d)\nThe proof is complete.\nD.4 PROOF OF LEMMA 4\nLemma 4 Suppose we have an input x \u2208 Rd for the QSGD quantizer Q. In the coarse quantization scenario, the quantization error satisfies E [\u2225\u2225Q(x)\u2212 x\u2225\u22252 2 \u2223\u2223x] = O (d 12)\u2225\u2225x\u2225\u22252 2 .\nProof. Consider a QSGD quantizer (Alistarh et al., 2017) with s = 1. For detailed results of other quantizers, we refer readers to Basu et al. (2020).\nIn particular, we have Qqsgd (xi) = \u2016x\u20162 \u00b7 sgn (xi) \u00b7 \u03bei(x, s), (30)\nwhere\n\u03bei(x, s)|s=1 = { 0 with prob. 1\u2212 |xi|\u2016x\u20162 , 1 with prob. |xi|\u2016x\u20162 .\n(31)\nThe variance of quantization error is\nE [\u2225\u2225Q(x)\u2212 x\u2225\u22252 2 |x ] = E [ d\u2211 i=1 (x\u0302i \u2212 xi)2 \u2223\u2223x] (32a)\n= d\u2211 i=1 ( E [ x\u03022i \u2223\u2223x]\u2212 x2i ) (32b)\n= \u2016x\u201622 \u2211d i=1 |xi| \u2016x\u20162 \u2212 \u2016x\u201622 (32c)\n= \u2016x\u20162\u2016x\u20161 \u2212 \u2016x\u201622 (32d) 6 ( \u221a d\u2212 1)\u2016x\u201622, (32e)\nwhich completes the proof.\nD.5 PROOF OF THEOREM 1\nWe first introduce some notations for simplicity. Let \u2206(k) denote the difference between two successive global latent weights, i.e.,\n\u2206(k) , w\u0303(k) \u2212 w\u0303(k+1). (33)\nWe use \u03b5(k,t)m to denote the stochastic gradient noise, i.e.,\n\u03b5(k,t) , g\u0303(k,t)m \u2212 g(k,t)m . (34)\nFinally, we let \u2207f(w\u0303) denote the gradient with respect to w\u0303. The following five lemmas are presented to facilitate the proof.\nLemma 5 The global normalized weight w\u0303(k) can be reconstructed as the average of local binary weight, i.e.,\nw\u0303(k+1) = 1\nM M\u2211 m=1 w(k,\u03c4)m . (35)\nProof. See Appendix D.6.\nLemma 6 (Lipschitz continuity) Under Assumption 3, \u2200 x1, x2 \u2208 R, we have\n|\u03d5(x1)\u2212 \u03d5(x2)| 6 c2|x1 \u2212 x2|. (36)\nProof. Without loss of generality, suppose x1 < x2. Based on the mean value theorem, there exists some c \u2208 (x1, x2) such that\n\u03d5\u2032(c) = \u03d5(x2)\u2212 \u03d5(x1)\nx2 \u2212 x1 . (37)\nFor the monotonically increasing function \u03d5, we have\n\u03d5 (x2)\u2212 \u03d5 (x1) = \u03d5\u2032(c) (x2 \u2212 x1) (38a) \u00ac 6 c2 (x2 \u2212 x1) , (38b)\nwhere \u00ac holds due to Assumption 3. The similar result can be obtained by assuming x2 < x1, which completes the proof.\nLemma 7 (Bounded weight divergence) Under Assumption 4, we have\nE \u2225\u2225w\u0303(k,\u03c4)m \u2212 w\u0303(k)\u2225\u222522 6 (c2\u03b7)2\u03c4 ( \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522 + \u03c32\u03b5 ) . (39)\nProof. See Appendix D.7.\nLemma 8 Under Assumptions 2 to 5, we have\nE \u2329 \u2207f(w\u0303(k)), \u2206(t) \u232a > c21\u03b7\u03c4 2 E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 (40)\n+ c21\u03b7\n4M\n( 2\u2212 (c2L)2\u03b72\u03c4(\u03c4 \u2212 1) ) M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522\n\u2212 (c1c2L) 2\u03b73\u03c4(\u03c4 \u2212 1)\n4 \u03c32\u03b5 \u2212 \u03b7(c22 \u2212 c21) M M\u2211 m=1 R(k)m , (41)\nwhere R(k)m , \u2212 \u03c4\u22121\u2211 t=0 \u2211 i/\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207f(w\u0303(k,t)m ))i ] and I(k,t)m , { i | g(k)i g (k,t) m,i > 0 } .\nProof. See Appendix D.8.\nLemma 9 (Bounded global weight difference) Under Assumptions 2 to 5, we have\nE \u2225\u2225\u2206(k)\u2225\u22252\n2 6\n(c2\u03b7) 2\u03c4\nM M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522 + (c2\u03b7)2\u03c4M \u03c32\u03b5 + 1M\u03c32k. (42)\nProof. See Appendix D.9.\nTheorem 1 For FedVote under Assumptions 1 to 5, if the learning let the learning rate \u03b7 = O (\n( c1c2 ) 2 1 L\u03c4 \u221a K\n) , then after K rounds of communication, we have\n1\nK K\u22121\u2211 k=0 c21E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 6\n2 [ f(w\u0303(0))\u2212 f(w\u0303\u2217) ] \u03b7\u03c4K + c22L\u03b7 [ 1 M + c21L\u03b7(\u03c4 \u2212 1) 2 ] \u03c32\u03b5\n+ L\n\u03b7\u03c4KM K\u22121\u2211 k=0 \u03c32k + 2(c22 \u2212 c21) \u03c4MK K\u22121\u2211 k=0 M\u2211 m=1 R(k)m . (43)\nwhere R(k)m , \u2212 \u03c4\u22121\u2211 t=0 \u2211 i/\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207f(w\u0303(k,t)m ))i ] and I(k,t)m , { i | g(k)i g (k,t) m,i > 0 } .\nProof. Consider the difference vector \u2206(k) defined in (33), we expand it as\n\u2206(k) = w\u0303(k) \u2212 w\u0303(k+1) (44a)\n\u00ac = w\u0303(k) \u2212 1\nM M\u2211 m=1 w(k,\u03c4)m (44b)\n = w\u0303(k) \u2212 1\nM M\u2211 m=1 w\u0303(k,\u03c4)m + 1 M M\u2211 m=1 \u03b6(r)m , (44c)\nwhere \u00ac follows from (35), and  holds by substituting the definition of quantization error.\nFrom Assumption 2, we have f(w\u0303(k+1))\u2212 f(w\u0303(k)) 6 \u2212 \u2329 \u2207f(w\u0303(k)), \u2206(k) \u232a + L\n2\n\u2225\u2225\u2206(k)\u2225\u22252 2 . (45a)\nLet (L\u03b7) 2\u03c4(\u03c4\u22121) 2 + L\u03b7\u03c4 c21 6 1 c22 , take the expectation on both sides, and use Lemmas 8\u20139:\nE [ f(w\u0303(k+1))\u2212 f(w\u0303(k)) ] 6 \u2212c 2 1\u03b7\u03c4 2 E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2\n\u2212 (c1c2) 2\u03b7\n4M\n( 2\nc22 \u2212 L2\u03b72\u03c4(\u03c4 \u2212 1)\u2212 2L\u03b7\u03c4 c21 ) M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522\n+ (c2\u03b7)\n2L\u03c4\n4\n( 2\nM + c21L\u03b7(\u03c4 \u2212 1)\n) \u03c32\u03b5 +\n\u03b7(c22 \u2212 c21) M M\u2211 m=1 R(k)m + L 2M \u03c32k (46a)\n\u00ac 6 \u2212c\n2 1\u03b7\u03c4 2 E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 + (c2\u03b7) 2L\u03c4 4\n( 2\nM + c21L\u03b7(\u03c4 \u2212 1)\n) \u03c32\u03b5\n+ \u03b7(c22 \u2212 c21)\nM\nM\u2211 m=1 R(k)m + L 2M \u03c32k, (46b)\nwhere \u00ac follows from the restrictions on learning rate. We rewrite the restriction as\n\u03b7 6 \u2212L\u03c4 c21\n+ \u221a L2\u03c42\nc41 + 2L 2\u03c4(\u03c4\u22121) c22\nL2\u03c4(\u03c4 \u2212 1) (47a)\n\u00ac 6 \u22121 +\n\u221a 1 +\n2c41 c22\nL(\u03c4 \u2212 1)c21 (47b)\n 6 c21 L(\u03c4 \u2212 1)c22 , (47c)\nwhere in \u00ac we use \u03c4(\u03c4 \u2212 1) 6 \u03c42, and  holds due to the Bernoulli inequality (1 + x) 12 6 1 + 12x, \u2200 x \u2208 [\u22121,\u221e). Based on (47c), we set the learning rate \u03b7 = O ( ( c1c2 ) 2 1 L\u03c4 \u221a K ) . Summing\nup over K communication rounds yields\n1\nK K\u22121\u2211 k=0 c21E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 6\n2 [ f(w\u0303(0))\u2212 f(w\u0303\u2217) ] \u03b7\u03c4K + c22L\u03b7 [ 1 M + c21L\u03b7(\u03c4 \u2212 1) 2 ] \u03c32\u03b5\n+ L\n\u03b7\u03c4KM K\u22121\u2211 k=0 \u03c32k + 2(c22 \u2212 c21) \u03c4MK K\u22121\u2211 k=0 M\u2211 m=1 R(k)m . (48a)\nD.6 PROOF OF LEMMA 5\nProof. From the reconstruction rule (14), we have\nw\u0303(k+1) = 2p(k+1) \u2212 1. (49)\nThe ith entry of p(k) is defined in (13) as:\np (k+1) i =\n1\nM M\u2211 m=1 1 ( w (k,\u03c4) m,i = 1 ) . (50)\nSubstituting (50) into (49) yields\nw\u0303 (k+1) i =\n1\nM M\u2211 m=1 [ 21 ( w (k,\u03c4) m,i = 1 ) \u2212 1 ] . (51)\nNote that\n21 ( w\n(k,\u03c4) m,i = 1\n) \u2212 1 =\n{ +1, w (k,\u03c4) m = +1,\n\u22121, w(k,\u03c4)m = \u22121, (52)\nwe have\nw\u0303 (k+1) i =\n1\nM M\u2211 m=1 w (k) i , (53)\nwhich completes the proof.\nD.7 PROOF OF LEMMA 7\nProof. With the local initialization and update method described in Algorithm 1, we have E \u2225\u2225w\u0303(k,\u03c4)m \u2212 w\u0303(k)\u2225\u222522 = E\u2225\u2225\u03d5(h(k,\u03c4)m )\u2212 \u03d5(h(k,0)m )\u2225\u222522 (54a)\n\u00ac 6 c22E \u2225\u2225h(k,\u03c4)m \u2212 h(k,0)m \u2225\u222522 (54b) = c22E \u2225\u2225\u2225\u2225\u2225 \u03c4\u22121\u2211 t=0 \u2212\u03b7 g\u0303(k,t)m \u2225\u2225\u2225\u2225\u2225 2\n2\n(54c)\n= (c2\u03b7) 2E \u2225\u2225\u2225\u2225\u2225 \u03c4\u22121\u2211 t=0 ( g(k,t)m + \u03b5 (k,t) m )\u2225\u2225\u2225\u2225\u2225 2\n2\n(54d)\n= (c2\u03b7) 2 ( E \u2225\u2225\u2225\u2225\u2225 \u03c4\u22121\u2211 t=0 g(k,t)m \u2225\u2225\u2225\u2225\u2225 2\n2\n+ E \u2225\u2225\u2225\u2225\u2225 \u03c4\u22121\u2211 t=0 \u03b5(k,t)m \u2225\u2225\u2225\u2225\u2225 2\n2\n) (54e)\n6 (c2\u03b7) 2\u03c4 ( \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522 + \u03c32\u03b5 ) , (54f)\nwhere \u00ac comes from the Lipschitz condition in Lemma 6.\nD.8 PROOF OF LEMMA 8\nProof. We have\nE \u2329 \u2207f(w\u0303(k)), \u2206(k) \u232a = E \u2329 \u2207f(w\u0303(k)), w\u0303(k) \u2212 1\nM M\u2211 m=1 w\u0303(k,\u03c4)m + 1 M M\u2211 m=1 \u03b6(k)m \u232a (55a)\n\u00ac = E \u2329 \u2207f(w\u0303(k)), w\u0303(k) \u2212 1\nM M\u2211 m=1 w\u0303(k,\u03c4)m \u232a (55b)\n= E \u2329 \u2207f(w\u0303(k)), 1\nM M\u2211 m=1 \u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ) \u232a , (55c)\nwhere \u00ac follows from Assumption 5. Based on the mean value theorem, for h(k)i , h (t,\u03c4) m,i \u2208 R, there exists some c(k)m,i \u2208 H (r) m,i such that\n\u03d5\u2032(c (k) m,i) =\n\u03d5(h (k) i )\u2212 \u03d5(h (k,\u03c4) m,i )\nh (k) i \u2212 h (k,\u03c4) m,i\n, (56)\nwhere H(r)m,i is an open interval with endpoints h (k) i and h (k,\u03c4) m,i :\nH(r)m,i =\n{ (h\n(k) i , h (k,\u03c4) m,i ), if h (k) i < h (k,\u03c4) m,i ,\n(h (k,\u03c4) m,i , h (k) i ), otherwise.\n(57)\nLet C(k)m = diag ( \u03d5\u2032(c (k) m,1), . . . , \u03d5 \u2032(c (k) m,d) ) , we have\n\u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ) = C(k)m (h (k) \u2212 h(k,\u03c4)m ) = \u03b7 C(k)m \u03c4\u22121\u2211 t=0 ( g(k,t)m + \u03b5 (k,t) m ) , (58)\nSubstituting (58) into (55c) yields\nE \u2329 \u2207f(w\u0303(k)), \u2206(k) \u232a = \u03b7\nM M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2329 \u2207f(w\u0303(k)), C(k)m g(k,t)m \u232a . (59)\nAccording to the chain rule, g(k,t)m can be written as\ng(k,t)m = \u2207hfm(\u03d5(h (k,t) m )) = D (k,t) m \u2207w\u0303fm(w\u0303(k,t)m ), (60) where D(k,t)m = diag ( d\u03d5\ndh (k,t) m,1\n, . . . , d\u03d5 dh\n(k,t) m,d\n) . To simplify the notations, let B(k,t)m = C (k) m D (k,t) m .\nNote that B(k,t)m is still a diagonal matrix, where the ith diagonal element b (k,t) m,i is\nb (k,t) m,i , (B (k,t) m )i,i =\nd\u03d5\ndc (k,\u03c4) m,i\nd\u03d5\ndh (k,t) m,i\n. (61)\nSubstituting (60) into (59) we have\nE \u2329 \u2207f(w\u0303(k)), \u2206(k) \u232a = \u03b7\nM M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2329 \u2207f(w\u0303(k)), B(k,t)m \u2207fm(w\u0303(k,t)m ) \u232a . (62)\nWe first focus on the following inner product:\u2329 \u2207f(w\u0303(k)), B(k,t)m \u2207fm(w\u0303(k,t)m ) \u232a = d\u2211 i=1 (\u2207f(w\u0303(k)))i \u00d7 b(k,t)m,i (\u2207fm(w\u0303 (k,t) m ))i, (63)\nwhere (\u2207f)i denotes the ith entry of the gradient vector. Consider an index set I(k,t)m defined as\nI(k,t)m , { i \u2208 {1, . . . , d} | (\u2207f(w\u0303(k)))i(\u2207fm(w\u0303(k,t)m ))i > 0 } . (64)\nSince the sign of (\u2207f(w\u0303(k)))i(\u2207fm(w\u0303(k,t)m ))i is equal to g(k)i g (k,t) m,i , the index set I (k,t) m can also be written as\nI(k,t)m , { i \u2208 {1, . . . , d} | g(k)i g (k,t) m,i > 0 } . (65)\nThe result in (63) can be bounded as\nE \u2329 \u2207f(w\u0303(k)), B(k,t)m \u2207fm(w\u0303(k,t)m ) \u232a (66a)\n\u00ac > c21 \u2211 i\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207fm(w\u0303(k,t)m ))i ] + c22 \u2211 i/\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207fm(w\u0303(k,t)m ))i ] (66b)\n= c21E \u2329 \u2207f(w\u0303(k)), \u2207fm(w\u0303(k,t)m ) \u232a + (c22 \u2212 c21) \u2211 i/\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207fm(w\u0303(k,t)m ))i ] , (66c)\nwhere \u00ac follows from Assumption 3. To simplify the notation, let R(k)m denote the accumulative gradient divergence, namely,\nR(k)m , \u2212 \u03c4\u22121\u2211 t=0 \u2211 i/\u2208I(k,t)m E [ (\u2207f(w\u0303(k)))i(\u2207f(w\u0303(k,t)m ))i ] . (67)\nThe expected inner product in (62) can be bounded as\nE \u2329 \u2207f(w\u0303(k)), \u2206(k) \u232a > c21\u03b7\nM M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2329 \u2207f(w\u0303(k)), \u2207fm(w\u0303(k,t)m ) \u232a \u2212 \u03b7(c 2 2 \u2212 c21) M M\u2211 m=1 R(k)m (68a)\n\u00ac = c21\u03b7\n2M M\u2211 m=1 \u03c4\u22121\u2211 t=0 ( E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 + E \u2225\u2225\u2207f(w\u0303(k,t)m )\u2225\u222522 \u2212 E\n\u2225\u2225\u2207f(w\u0303(k,t)m )\u2212\u2207f(w\u0303(k))\u2225\u222522)\u2212 \u03b7(c22 \u2212 c21)M M\u2211 m=1 R(k)m (68b)\n > c21\u03b7\u03c4\n2\n\u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 + c21\u03b7\n2M M\u2211 m=1 \u03c4\u22121\u2211 t=0 ( E \u2225\u2225\u2207f(w\u0303(k,t)m )\u2225\u222522\n\u2212 L2E \u2225\u2225w\u0303(k,t)m \u2212 w\u0303(k)\u2225\u222522)\u2212 \u03b7(c22 \u2212 c21)M M\u2211 m=1 R(k)m , (68c)\nwhere \u00ac follows from 2 \u2329 a, b \u232a = \u2225\u2225a\u2225\u22252 2 + \u2225\u2225b\u2225\u22252 2 \u2212 \u2225\u2225a \u2212 b\u2225\u22252 2 , and  holds due to Assumption 2. From Lemma 7, we can show that\nE \u2225\u2225w\u0303(k,t)m \u2212 w\u0303(k)\u2225\u222522 6 (c2\u03b7)2t ( t\u22121\u2211 n=0 \u2225\u2225g(k,n)m \u2225\u222522 + \u03c32\u03b5 ) , (69)\nwhere t = 1, . . . , \u03c4 \u2212 1. Substituting (69) in (68c) yields\nE \u2329 \u2207f(w\u0303(k)), \u2206(k) \u232a > c21\u03b7\u03c4\n2 E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 + c21\u03b7 2M M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225\u2207f(w\u0303(k,t)m )\u2225\u222522\n\u2212 (c1c2L) 2\u03b73\n4M\nM\u2211 m=1 \u03c4\u22121\u2211 n=0 (\u03c4(\u03c4 \u2212 1)\u2212 n(n+ 1))E \u2225\u2225g(k,n)m \u2225\u222522\n\u2212 (c1c2L) 2\u03b73\u03c4(\u03c4 \u2212 1)\n4 \u03c32\u03b5 \u2212 \u03b7(c22 \u2212 c21) M M\u2211 m=1 R(k)m (70a)\n\u00ac > c21\u03b7\u03c4 2 E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 + c21\u03b7 2M M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225\u2207f(w\u0303(k,t)m )\u2225\u222522\n\u2212 \u03b7 4M (c1c2L) 2\u03b72\u03c4(\u03c4 \u2212 1) M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522\n\u2212 (c1c2L) 2\u03b73\u03c4(\u03c4 \u2212 1)\n4 \u03c32\u03b5 \u2212 \u03b7(c22 \u2212 c21) M M\u2211 m=1 R(k)m (70b)\n > c21\u03b7\u03c4 2 E \u2225\u2225\u2207f(w\u0303(k))\u2225\u22252 2 + c21\u03b7 4M ( 2\u2212 (c2L)2\u03b72\u03c4(\u03c4 \u2212 1) ) M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225g(k,t)m \u2225\u222522\n\u2212 (c1c2L) 2\u03b73\u03c4(\u03c4 \u2212 1)\n4 \u03c32\u03b5 \u2212 \u03b7(c22 \u2212 c21) M M\u2211 m=1 R(k)m (70c)\nwhere \u00ac follows from \u03c4(\u03c4 \u2212 1) \u2212 n(n + 1) 6 \u03c4(\u03c4 \u2212 1), and  holds due to the chain rule in (60) and Assumption 3.\nD.9 PROOF OF LEMMA 9\nProof. We have\nE \u2225\u2225\u2206(k)\u2225\u22252\n2 = E \u2225\u2225\u2225\u2225\u2225w\u0303(k) \u2212 1M M\u2211 m=1 w\u0303(k,\u03c4)m + 1 M M\u2211 m=1 \u03b6(k)m \u2225\u2225\u2225\u2225\u2225 2\n2\n(71a)\n= E \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 \u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ) + 1 M M\u2211 m=1 \u03b6(k)m \u2225\u2225\u2225\u2225\u2225 2\n2\n(71b)\n= E \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 \u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ) \u2225\u2225\u2225\u2225\u2225 2\n2\n+ E \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 \u03b6(k)m \u2225\u2225\u2225\u2225\u2225 2\n2\n+ 2E\n\u2329 1\nM M\u2211 m=1 \u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ), 1 M M\u2211 m=1 \u03b6(t)m\n\u232a (71c)\n\u00ac = E \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 \u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ) \u2225\u2225\u2225\u2225\u2225 2\n2\n+ E \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 \u03b6(k)m \u2225\u2225\u2225\u2225\u2225 2\n2\n, (71d)\nwhere \u00ac comes from Assumption 5. For the first term in (71d) we have\nE \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 (\u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m ) \u2225\u2225\u2225\u2225\u2225 2\n2\n6 1\nM M\u2211 m=1 E \u2225\u2225\u2225\u03d5(h(k))\u2212 \u03d5(h(k,\u03c4)m )\u2225\u2225\u22252 2 (72a)\n\u00ac 6 c22 M M\u2211 m=1 \u2225\u2225h(k) \u2212 h(k,\u03c4)m \u2225\u222522 (72b)  = (c2\u03b7) 2\nM M\u2211 m=1 \u2225\u2225\u2225\u2225\u2225\u2212\u03b7 \u03c4\u22121\u2211 t=0 g\u0303(k,t)m \u2225\u2225\u2225\u2225\u2225 2\n2\n(72c)\n= (c2\u03b7)\n2\nM M\u2211 m=1 \u2225\u2225\u2225\u2225\u2225 \u03c4\u22121\u2211 t=0 (g(k,t)m + \u03b5 (k,t) m ) \u2225\u2225\u2225\u2225\u2225 2\n2\n(72d)\n= (c2\u03b7)\n2\u03c4\nM M\u2211 m=1 \u03c4\u22121\u2211 t=0 E \u2225\u2225\u2225g(k,t)m \u2225\u2225\u22252 2 + (c2\u03b7) 2\u03c4 M \u03c32\u03b5 , (72e)\nwhere \u00ac is due to Lemma 6 and  comes from (9). For the second term in (71d) we have\nE \u2225\u2225\u2225\u2225\u2225 1M M\u2211 m=1 \u03b6(k)m \u2225\u2225\u2225\u2225\u2225 2\n2\n6 1\nM \u03c32k. (73)\nCombing the results of (72e) and (73) completes the proof.", "ref": {"BIBREF0": {"ref_id": "b0", "title": "QSGD: Communication-efficient SGD via gradient quantization and encoding", "authors": [{"first": "Dan", "middle": [], "last": "Alistarh", "suffix": ""}, {"first": "Demjan", "middle": [], "last": "Grubic", "suffix": ""}, {"first": "Jerry", "middle": [], "last": "Li", "suffix": ""}, {"first": "Ryota", "middle": [], "last": "Tomioka", "suffix": ""}, {"first": "Milan", "middle": [], "last": "Vojnovic", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "1709--1720", "other_ids": {}, "num": null, "urls": [], "raw_text": "Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pp. 1709-1720, 2017.", "links": null}, "BIBREF1": {"ref_id": "b1", "title": "Qsparse-local-sgd: Distributed sgd with quantization, sparsification, and local computations", "authors": [{"first": "Debraj", "middle": [], "last": "Basu", "suffix": ""}, {"first": "Deepesh", "middle": [], "last": "Data", "suffix": ""}, {"first": "Can", "middle": [], "last": "Karakus", "suffix": ""}, {"first": "N", "middle": [], "last": "Suhas", "suffix": ""}, {"first": "", "middle": [], "last": "Diggavi", "suffix": ""}], "year": 2020, "venue": "IEEE Journal on Selected Areas in Information Theory", "volume": "1", "issue": "1", "pages": "217--226", "other_ids": {}, "num": null, "urls": [], "raw_text": "Debraj Basu, Deepesh Data, Can Karakus, and Suhas N Diggavi. Qsparse-local-sgd: Distributed sgd with quantization, sparsification, and local computations. IEEE Journal on Selected Areas in Information Theory, 1(1):217-226, 2020.", "links": null}, "BIBREF2": {"ref_id": "b2", "title": "The effectiveness of reputation-based voting for collusion tolerance in large-scale grids", "authors": [{"first": "Ahmed", "middle": [], "last": "Bendahmane", "suffix": ""}, {"first": "Mohamed", "middle": [], "last": "Essaaidi", "suffix": ""}, {"first": "Ahmed", "middle": ["El"], "last": "Moussaoui", "suffix": ""}, {"first": "Ali", "middle": [], "last": "Younes", "suffix": ""}], "year": 2014, "venue": "IEEE Transactions on Dependable and Secure Computing", "volume": "12", "issue": "6", "pages": "665--674", "other_ids": {}, "num": null, "urls": [], "raw_text": "Ahmed Bendahmane, Mohamed Essaaidi, Ahmed El Moussaoui, and Ali Younes. The effectiveness of reputation-based voting for collusion tolerance in large-scale grids. IEEE Transactions on Dependable and Secure Computing, 12(6):665-674, 2014.", "links": null}, "BIBREF3": {"ref_id": "b3", "title": "signSGD: Compressed optimisation for non-convex problems", "authors": [{"first": "Jeremy", "middle": [], "last": "Bernstein", "suffix": ""}, {"first": "Yu-Xiang", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Kamyar", "middle": [], "last": "Azizzadenesheli", "suffix": ""}, {"first": "Animashree", "middle": [], "last": "Anandkumar", "suffix": ""}], "year": 2018, "venue": "International Conference on Machine Learning", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, 2018.", "links": null}, "BIBREF4": {"ref_id": "b4", "title": "Machine learning with adversaries: Byzantine tolerant gradient descent", "authors": [{"first": "Peva", "middle": [], "last": "Blanchard", "suffix": ""}, {"first": "Rachid", "middle": [], "last": "Guerraoui", "suffix": ""}, {"first": "Julien", "middle": [], "last": "Stainer", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries: Byzan- tine tolerant gradient descent. In Advances in Neural Information Processing Systems, 2017.", "links": null}, "BIBREF5": {"ref_id": "b5", "title": "Distributed training with heterogeneous data: Bridging median-and mean-based algorithms", "authors": [{"first": "Xiangyi", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Tiancong", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Haoran", "middle": [], "last": "Sun", "suffix": ""}, {"first": "Steven", "middle": ["Z"], "last": "Wu", "suffix": ""}, {"first": "Mingyi", "middle": [], "last": "Hong", "suffix": ""}], "year": 2020, "venue": "Neural Information Processing Systems", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Xiangyi Chen, Tiancong Chen, Haoran Sun, Steven Z. Wu, and Mingyi Hong. Distributed training with heterogeneous data: Bridging median-and mean-based algorithms. In Neural Information Processing Systems, 2020.", "links": null}, "BIBREF6": {"ref_id": "b6", "title": "Heterofl: Computation and communication efficient federated learning for heterogeneous clients", "authors": [{"first": "Enmao", "middle": [], "last": "Diao", "suffix": ""}, {"first": "Jie", "middle": [], "last": "Ding", "suffix": ""}, {"first": "Vahid", "middle": [], "last": "Tarokh", "suffix": ""}], "year": 2021, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient federated learning for heterogeneous clients. In International Conference on Learning Represen- tations, 2021.", "links": null}, "BIBREF7": {"ref_id": "b7", "title": "Differentiable soft quantization: Bridging full-precision and low-bit neural networks", "authors": [{"first": "Ruihao", "middle": [], "last": "Gong", "suffix": ""}, {"first": "Xianglong", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Shenghu", "middle": [], "last": "Jiang", "suffix": ""}, {"first": "Tianxiang", "middle": [], "last": "Li", "suffix": ""}, {"first": "Peng", "middle": [], "last": "Hu", "suffix": ""}, {"first": "Jiazhen", "middle": [], "last": "Lin", "suffix": ""}, {"first": "Fengwei", "middle": [], "last": "Yu", "suffix": ""}, {"first": "Junjie", "middle": [], "last": "Yan", "suffix": ""}], "year": 2019, "venue": "International Conference on Computer Vision", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In International Conference on Computer Vision, 2019.", "links": null}, "BIBREF8": {"ref_id": "b8", "title": "Digital Image Processing", "authors": [{"first": "C", "middle": [], "last": "Rafael", "suffix": ""}, {"first": "Richard", "middle": ["E"], "last": "Gonzalez", "suffix": ""}, {"first": "", "middle": [], "last": "Woods", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Rafael C. Gonzalez and Richard E. Woods. Digital Image Processing 3rd Edition. 2014.", "links": null}, "BIBREF9": {"ref_id": "b9", "title": "Federated learning with compression: Unified analysis and sharp guarantees", "authors": [{"first": "Farzin", "middle": [], "last": "Haddadpour", "suffix": ""}, {"first": "Mohammad", "middle": ["Mahdi"], "last": "Kamani", "suffix": ""}, {"first": "Aryan", "middle": [], "last": "Mokhtari", "suffix": ""}, {"first": "Mehrdad", "middle": [], "last": "Mahdavi", "suffix": ""}], "year": 2021, "venue": "International Conference on Artificial Intelligence and Statistics", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Feder- ated learning with compression: Unified analysis and sharp guarantees. In International Confer- ence on Artificial Intelligence and Statistics, 2021.", "links": null}, "BIBREF10": {"ref_id": "b10", "title": "Analysis of quantized models", "authors": [{"first": "Lu", "middle": [], "last": "Hou", "suffix": ""}, {"first": "Ruiliang", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "James T", "middle": [], "last": "Kwok", "suffix": ""}], "year": 2019, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Lu Hou, Ruiliang Zhang, and James T Kwok. Analysis of quantized models. In International Conference on Learning Representations, 2019.", "links": null}, "BIBREF11": {"ref_id": "b11", "title": "Measuring the effects of non-identical data distribution for federated visual classification", "authors": [{"first": "Tzu-Ming Harry", "middle": [], "last": "Hsu", "suffix": ""}, {"first": "Hang", "middle": [], "last": "Qi", "suffix": ""}, {"first": "Matthew", "middle": [], "last": "Brown", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1909.06335"]}, "num": null, "urls": [], "raw_text": "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.", "links": null}, "BIBREF12": {"ref_id": "b12", "title": "Binarized neural networks", "authors": [{"first": "Itay", "middle": [], "last": "Hubara", "suffix": ""}, {"first": "Matthieu", "middle": [], "last": "Courbariaux", "suffix": ""}, {"first": "Daniel", "middle": [], "last": "Soudry", "suffix": ""}, {"first": "Ran", "middle": [], "last": "El-Yaniv", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2016, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In Advances in Neural Information Processing Systems, 2016.", "links": null}, "BIBREF13": {"ref_id": "b13", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": [{"first": "Sergey", "middle": [], "last": "Ioffe", "suffix": ""}, {"first": "Christian", "middle": [], "last": "Szegedy", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1502.03167"]}, "num": null, "urls": [], "raw_text": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.", "links": null}, "BIBREF14": {"ref_id": "b14", "title": "Stochastic-sign sgd for federated learning with theoretical guarantees", "authors": [{"first": "Richeng", "middle": [], "last": "Jin", "suffix": ""}, {"first": "Yufan", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Xiaofan", "middle": [], "last": "He", "suffix": ""}, {"first": "Huaiyu", "middle": [], "last": "Dai", "suffix": ""}, {"first": "Tianfu", "middle": [], "last": "Wu", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2002.10940"]}, "num": null, "urls": [], "raw_text": "Richeng Jin, Yufan Huang, Xiaofan He, Huaiyu Dai, and Tianfu Wu. Stochastic-sign sgd for feder- ated learning with theoretical guarantees. arXiv preprint arXiv:2002.10940, 2020.", "links": null}, "BIBREF15": {"ref_id": "b15", "title": "Advances and open problems in federated learning. Foundations and Trends in Machine Learning", "authors": [{"first": "Peter", "middle": [], "last": "Kairouz", "suffix": ""}, {"first": "Brendan", "middle": [], "last": "Mcmahan", "suffix": ""}, {"first": "Brendan", "middle": [], "last": "Avent", "suffix": ""}, {"first": "Aur\u00e9lien", "middle": [], "last": "Bellet", "suffix": ""}, {"first": "Mehdi", "middle": [], "last": "Bennis", "suffix": ""}, {"first": "Arjun", "middle": ["Nitin"], "last": "Bhagoji", "suffix": ""}, {"first": "Keith", "middle": [], "last": "Bonawitz", "suffix": ""}, {"first": "Zachary", "middle": [], "last": "Charles", "suffix": ""}, {"first": "Graham", "middle": [], "last": "Cormode", "suffix": ""}, {"first": "Rachel", "middle": [], "last": "Cummings", "suffix": ""}], "year": null, "venue": "", "volume": "14", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 14(1), 2021.", "links": null}, "BIBREF16": {"ref_id": "b16", "title": "Learning multiple layers of features from tiny images", "authors": [{"first": "Alex", "middle": [], "last": "Krizhevsky", "suffix": ""}], "year": 2009, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Master thesis, Dept. of Comput. Sci., Univ. of Toronto, Toronto, Canada, 2009.", "links": null}, "BIBREF17": {"ref_id": "b17", "title": "Lagrangian multiplier adaptation for rate-distortion optimization with inter-frame dependency", "authors": [{"first": "Shuai", "middle": [], "last": "Li", "suffix": ""}, {"first": "Ce", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "Yanbo", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Yimin", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Fr\u00e9d\u00e9ric", "middle": [], "last": "Dufaux", "suffix": ""}, {"first": "Ming-Ting", "middle": [], "last": "Sun", "suffix": ""}], "year": 2015, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "volume": "26", "issue": "", "pages": "117--129", "other_ids": {}, "num": null, "urls": [], "raw_text": "Shuai Li, Ce Zhu, Yanbo Gao, Yimin Zhou, Fr\u00e9d\u00e9ric Dufaux, and Ming-Ting Sun. Lagrangian multi- plier adaptation for rate-distortion optimization with inter-frame dependency. IEEE Transactions on Circuits and Systems for Video Technology, 26(1):117-129, 2015.", "links": null}, "BIBREF18": {"ref_id": "b18", "title": "Ensemble distillation for robust model fusion in federated learning", "authors": [{"first": "Tao", "middle": [], "last": "Lin", "suffix": ""}, {"first": "Lingjing", "middle": [], "last": "Kong", "suffix": ""}, {"first": "U", "middle": [], "last": "Sebastian", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Stich", "suffix": ""}, {"first": "", "middle": [], "last": "Jaggi", "suffix": ""}], "year": 2020, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 2020.", "links": null}, "BIBREF19": {"ref_id": "b19", "title": "Communication-efficient learning of deep networks from decentralized data", "authors": [{"first": "H", "middle": [], "last": "", "suffix": ""}, {"first": "Brendan", "middle": [], "last": "Mcmahan", "suffix": ""}, {"first": "Eider", "middle": [], "last": "Moore", "suffix": ""}, {"first": "Daniel", "middle": [], "last": "Ramage", "suffix": ""}, {"first": "Seth", "middle": [], "last": "Hampson", "suffix": ""}, {"first": "Blaise", "middle": [], "last": "Aguera Y Arcas", "suffix": ""}], "year": 2017, "venue": "International Conference on Artificial Intelligence and Statistics", "volume": "", "issue": "", "pages": "1273--1282", "other_ids": {}, "num": null, "urls": [], "raw_text": "H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In International Conference on Artificial Intelligence and Statistics, pp. 1273-1282, 2017.", "links": null}, "BIBREF20": {"ref_id": "b20", "title": "Byzantine-robust federated machine learning through adaptive model averaging", "authors": [{"first": "Luis", "middle": [], "last": "Mu\u00f1oz-Gonz\u00e1lez", "suffix": ""}, {"first": "T", "middle": [], "last": "Kenneth", "suffix": ""}, {"first": "Emil", "middle": ["C"], "last": "Co", "suffix": ""}, {"first": "", "middle": [], "last": "Lupu", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1909.05125"]}, "num": null, "urls": [], "raw_text": "Luis Mu\u00f1oz-Gonz\u00e1lez, Kenneth T Co, and Emil C Lupu. Byzantine-robust federated machine learn- ing through adaptive model averaging. arXiv preprint arXiv:1909.05125, 2019.", "links": null}, "BIBREF21": {"ref_id": "b21", "title": "Binary neural networks: A survey", "authors": [{"first": "Haotong", "middle": [], "last": "Qin", "suffix": ""}, {"first": "Ruihao", "middle": [], "last": "Gong", "suffix": ""}, {"first": "Xianglong", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Xiao", "middle": [], "last": "Bai", "suffix": ""}, {"first": "Jingkuan", "middle": [], "last": "Song", "suffix": ""}, {"first": "Nicu", "middle": [], "last": "Sebe", "suffix": ""}], "year": 2020, "venue": "Pattern Recognition", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. Pattern Recognition, pp. 107281, 2020a.", "links": null}, "BIBREF22": {"ref_id": "b22", "title": "Line-speed and scalable intrusion detection at the network edge via federated learning", "authors": [{"first": "Qiaofeng", "middle": [], "last": "Qin", "suffix": ""}, {"first": "Konstantinos", "middle": [], "last": "Poularakis", "suffix": ""}, {"first": "K", "middle": [], "last": "Kin", "suffix": ""}, {"first": "Leandros", "middle": [], "last": "Leung", "suffix": ""}, {"first": "", "middle": [], "last": "Tassiulas", "suffix": ""}], "year": 2020, "venue": "International Federation for Information Processing Networking Conference", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Qiaofeng Qin, Konstantinos Poularakis, Kin K Leung, and Leandros Tassiulas. Line-speed and scal- able intrusion detection at the network edge via federated learning. In International Federation for Information Processing Networking Conference, 2020b.", "links": null}, "BIBREF23": {"ref_id": "b23", "title": "FedPAQ: A communication-efficient federated learning method with periodic averaging and quantization", "authors": [{"first": "Amirhossein", "middle": [], "last": "Reisizadeh", "suffix": ""}, {"first": "Aryan", "middle": [], "last": "Mokhtari", "suffix": ""}, {"first": "Hamed", "middle": [], "last": "Hassani", "suffix": ""}, {"first": "Ali", "middle": [], "last": "Jadbabaie", "suffix": ""}, {"first": "Ramtin", "middle": [], "last": "Pedarsani", "suffix": ""}], "year": 2020, "venue": "International Conference on Artificial Intelligence and Statistics", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. FedPAQ: A communication-efficient federated learning method with periodic averaging and quan- tization. In International Conference on Artificial Intelligence and Statistics, 2020.", "links": null}, "BIBREF24": {"ref_id": "b24", "title": "Stochastic sign descent methods: New algorithms and better theory", "authors": [{"first": "Mher", "middle": [], "last": "Safaryan", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Richtarik", "suffix": ""}], "year": 2021, "venue": "International Conference on Machine Learning", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Mher Safaryan and Peter Richtarik. Stochastic sign descent methods: New algorithms and better theory. In International Conference on Machine Learning, 2021.", "links": null}, "BIBREF25": {"ref_id": "b25", "title": "On the byzantine robustness of clustered federated learning", "authors": [{"first": "Felix", "middle": [], "last": "Sattler", "suffix": ""}, {"first": "Klaus-Robert", "middle": [], "last": "M\u00fcller", "suffix": ""}, {"first": "Thomas", "middle": [], "last": "Wiegand", "suffix": ""}, {"first": "Wojciech", "middle": [], "last": "Samek", "suffix": ""}], "year": 2020, "venue": "International Conference on Acoustics, Speech and Signal Processing", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Felix Sattler, Klaus-Robert M\u00fcller, Thomas Wiegand, and Wojciech Samek. On the byzantine ro- bustness of clustered federated learning. In International Conference on Acoustics, Speech and Signal Processing, 2020.", "links": null}, "BIBREF26": {"ref_id": "b26", "title": "Learning discrete weights using the local reparameterization trick", "authors": [{"first": "Oran", "middle": [], "last": "Shayer", "suffix": ""}, {"first": "Dan", "middle": [], "last": "Levi", "suffix": ""}, {"first": "Ethan", "middle": [], "last": "Fetaya", "suffix": ""}], "year": 2018, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameteri- zation trick. In International Conference on Learning Representations, 2018.", "links": null}, "BIBREF27": {"ref_id": "b27", "title": "Federated learning over wireless networks: Optimization model design and analysis", "authors": [{"first": "H", "middle": [], "last": "Nguyen", "suffix": ""}, {"first": "Wei", "middle": [], "last": "Tran", "suffix": ""}, {"first": "Albert", "middle": [], "last": "Bao", "suffix": ""}, {"first": "", "middle": [], "last": "Zomaya", "suffix": ""}, {"first": "N", "middle": ["H"], "last": "Minh", "suffix": ""}, {"first": "Choong Seon", "middle": [], "last": "Nguyen", "suffix": ""}, {"first": "", "middle": [], "last": "Hong", "suffix": ""}], "year": 2019, "venue": "International Conference on Computer Communications", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Nguyen H Tran, Wei Bao, Albert Zomaya, Minh NH Nguyen, and Choong Seon Hong. Feder- ated learning over wireless networks: Optimization model design and analysis. In International Conference on Computer Communications, 2019.", "links": null}, "BIBREF28": {"ref_id": "b28", "title": "Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms", "authors": [{"first": "Jianyu", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Gauri", "middle": [], "last": "Joshi", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1808.07576"]}, "num": null, "urls": [], "raw_text": "Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms. arXiv preprint arXiv:1808.07576, 2018.", "links": null}, "BIBREF29": {"ref_id": "b29", "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms", "authors": [{"first": "Han", "middle": [], "last": "Xiao", "suffix": ""}, {"first": "Kashif", "middle": [], "last": "Rasul", "suffix": ""}, {"first": "Roland", "middle": [], "last": "Vollgraf", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1708.07747"]}, "num": null, "urls": [], "raw_text": "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark- ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.", "links": null}, "BIBREF30": {"ref_id": "b30", "title": "Byzantine-robust distributed learning: Towards optimal statistical rates", "authors": [{"first": "Dong", "middle": [], "last": "Yin", "suffix": ""}, {"first": "Yudong", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Ramchandran", "middle": [], "last": "Kannan", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Bartlett", "suffix": ""}], "year": 2018, "venue": "International Conference on Machine Learning", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In International Conference on Machine Learning, 2018.", "links": null}, "BIBREF31": {"ref_id": "b31", "title": "Ensemble methods: foundations and algorithms", "authors": [{"first": "Zhi-Hua", "middle": [], "last": "Zhou", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. Chapman and Hall/CRC, 2019.", "links": null}}, "peer_review": [{"review_text": "The paper presents a scheme for federated learning (FL) called FedVote. In FedVote, clients use binary neural networks where a latent restricted range weight vector\nh\nis learned, and stochastic rounding is applied to obtain quantized weights\n\u2208\n{\n\u2212\n1\n,\n+\n1\n}. The communication cost to the server (up-link) is reduced by using quantized weights. The server aggregates weights by summing over all the weights of the clients at each round of training and applying a sign function. This mechanism is referred to as plurality voting (ties are broken randomly). However, the paper also proposes to use soft voting, which normalizes the number of ones (cf. hard sign function) to get probability values. These probabilities are then quantized with a clipping mechanism to restricted maximum and minimum values, using predefined thresholds, and broadcasted to the clients (down-link). The latent weights of the client models are updated using the soft voting results. The paper also adapts work on reputation-based voting (Bendahmane et al. 2014) to deal with adversarial FL in the form of Byzantine attacks. The paper presents convergence analysis for FedVote under the independent and identically distributed data setting. Empirical evaluations is conducted on two datasets ,Fashion-MNIST and CIFAR-10, and applied to two models, LeNet-5 and VGG-7, respectively.Mainly, the paper has the following weaknesses:\nThe main proposed method (FedVote) is incremental at best, and the proposal on the adversarial aspect (Byzantine-FedVote) is limited. The normalization mechanism potentially requires careful tuning. Binary weights reduce uplink communication, but downlink savings depend on probability quantization scheme.\nConfined to using binary neural networks\nIt is not clear how the quantization of probability values for soft voting are set (i.e., predefined thresholds) and the effects on performance.\nIt is not clear how the predefined coefficients (\n\u03b2\n) are set as it relates to reputation-based voting, and the effects on performance.\nEmpirical evaluations is limited:\nSeveral other machine learning models/benchmarks are not evaluated on (e.g., [LEAF: A Benchmark for Federated Settings, Caldas et al. 2019]).\nFor test accuracy experiments, the number of clients used and the number sampled each round is not given.\nFor communication efficiency experiment, the number of clients used (\nM\n=31) is quite small (e.g., [Hsu et al. 2019] uses 100 clients). Down-link GB not accounted for.\nPreliminaries should be provided on adversarial FL/Byzantine attack.\nInformation on measuring energy usage (Table 4) not provided.\nAre Figure 4 (a) and (b) correctly labeled?The contributions in the paper are limited and not well-focused. Overall, I do not think the paper is at a level for acceptance.", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "3: reject, not good enough", "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."}, {"review_text": "This paper aims to reduce the communication cost for federated learning while ensuring that the aggregation method at the server is tolerant to the presence of Byzantine clients. Towards this instead of communicating the local update/gradients or their quantized version, this paper proposes to communicate the quantized model weights to the server. The server performs the aggregation in the quantized space and communicates the aggregate value back to the clients. To enable faithful quantization of the weights during the communication with the server, each client learns normalized weights (by applying a coordinate-wise normalization function to the latent weights).\nThe paper also proposes a weighted aggregation mechanism at the server that takes the reputations of different clients into account.\nThe paper analyzes the impact of the weight quantization on the convergence of the underlying federated learning algorithm in an i.i.d. setting. Empirical evaluations on Fashion-MNIST and CIFAR-10 show that the proposed method outperforms existing local update/gradient compression-based schemes.Strengths:\nThe paper presents a novel communication efficient federated learning algorithm that quantizes local model weights as opposed to quantizing the local gradients. The paper employs the existing approach of quantizing normalized versions of latent weights to achieve this objective.\nThe paper address the issue of Byzantine clients and proposes a weighted aggregation method to counter such Byzantines.\nTheoretical and empirical results in the paper showcase the utility of the proposed algorithm.\nWeaknesses:\nWhat purpose does Lemma 1 (one-shot FedVote) serve? The assumption that all the error events are i.i.d. would most like not to hold in a multi-round setting. Even in a single round, would one need independent initializations at different clients for the analysis to hold? Also, why can the probabilities\ne\np\ns\ni\nl\no\nn\nm\n,\ni\nnot be arbitrarily close to 1.\nSection 4.3 states that (without weighted aggregation) the proposed method is going to be vulnerable to the presence of Byzantines as (on average) it behaves as the FedAvg (Lemma 2). Is this conclusion correct even when one is ensuring weight normalization at the clients?\nIn Section 6, what is the underlying quantization rule when sending ternary weights to the server.\nPlease make the plot colors consistent across Fig 1(a) - 1(c).\nIn the introduction the authors claim that \"However, directly quantizing the gradient vector does not provide the optimal trade-off between communication efficiency and model accuracy.\" Is there any prior work that supports this blanket statement?The studies an important problem in the context of federated learning, namely designing communication efficient learning algorithms in the presence of Byzantine clients. The paper presents an interesting weight quantization-based algorithm and analyzes its convergence in an i.i.d. setting. The authors also demonstrate the superiority of the proposed algorithm on existing communication efficient federated learning algorithms in the literature on two standard image classification benchmarks.\nThat said, there are some questions that remain about the claims made in the paper (see weaknesses above).", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Empirical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Recommendation": "6: marginally above the acceptance threshold", "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."}, {"review_text": "This paper provides a voting-based quantized model averaging method for federated learning. The convergence is proved and the experimental results on Fashion-MNIST and CIFAR-10 show that the proposed scheme outperforms existing schemes.I like the topic of this paper, but I have some concerns as below.\nExplanation on the huge gap between FedVote and SignSGD with majority vote [Bernstein\u201918].\nIt seems like the basic concept of FedVote is from [Bernstein\u201918]. I\u2019m bit curious why FedVote has some gap compared with [Bernstein\u201918]. Is it because of the weighted voting strategy of FedVote? Can we do some ablation study to check what made FedVote such powerful, compared with [Bernstein\u201918]?\nComparison with other Byzantine-tolerant schemes\nThere have been many previous works on Byzantine-tolerant schemes, e.g., using coding scheme to defend Byzantine attacks, DRACO[Chen\u201918], DETOX[Rajput\u201919], Election coding [Sohn\u201920], SignGuard[Xu\u201921], ByzShield [Konstantinidis\u201921]. Especially, the last four works focus on the voting-based model/gradient averaging. I think this paper should at least mention these works, and also compare the performance.\nThere have been naive methods of using median or mean of median schemes for tolerating Byzantines, starting from 2017, e.g., KRUM, multi-KRUM. I guess this paper didn\u2019t compare with these naive methods also.\nAfter the [Bernstein\u201918], there have been upcoming works [Karimireddy\u201919] suggesting that error feedback fixes the issue of SignSGD. I\u2019m not sure whether the authors compared their work with this scheme.\nSo in general, I thought that the literature search and comparison is bit weak. SignSGD is 3-year old paper, and there have been many other papers using voting-based aggregation for Byzantine tolerance.\n[Bernstein\u201918] https://arxiv.org/abs/1802.04434 [Chen\u201918] https://arxiv.org/abs/1803.09877 [Karimireddy\u201919] http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf [Rajput\u201919] https://proceedings.neurips.cc/paper/2019/hash/415185ea244ea2b2bedeb0449b926802-Abstract.html [Sohn\u201920] https://proceedings.neurips.cc/paper/2020/hash/a7f0d2b95c60161b3f3c82f764b1d1c9-Abstract.html [Xu\u201921] https://arxiv.org/pdf/2109.05872.pdf [Konstantinidis\u201921] https://arxiv.org/abs/2010.04902This is an interesting paper, but the idea is quite similar to SignSGD paper, and has no thorough comparison with existing works. Better comparison & ablation are needed to better understand why FedVote works better than existing schemes.", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "3: reject, not good enough", "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."}, {"review_text": "The author(s) proposed a new algorithm based on plurality voting for federated learning with quantized gradients. Convergence theory is developed and experiments on both iid and non-iid datasets are conducted to evaluate the proposed method.Pros:\nThe paper is well-written and related works are well-address to my knowledge.\nHas a nice introduction on quantized neural networks, which is good for readers that are not familiar with the topic.\nMy Questions: I have some questions on the theory side of the paper.\nThe author(s) described the convergence of FedVote in terms of\n|\n|\nf\n(\nw\n~\n(\nk\n)\n)\n|\n|\n2\n, which is a valid measure of stationarity for continuous problems. I wonder what is the conclusion to the discrete problem? For example, we have a stationary point\nw\n\u2217\nsuch that\n|\n|\nf\n(\nw\n~\n\u2217\n)\n|\n|\n=\n0\n. Then what conclusion can we obtain for the following problem\nmin\nw\n\u2208\nD\nn\n2\nf\n(\nw\n)\n.\nCan we say that we are at a global minimum in the convex case?\nIn Lemma 1, the author(s) define\ns\ni\n=\n2\nM\n\u2211\ni\n=\n1\nM\n\u03f5\nm\n,\ni\n. From the definition we know\ns\ni\n\u2208\n(\n0\n,\n2\n)\n. Why the author(s) conclude that\ns\ni\n\u2208\n(\n0\n,\n1\n)\n?\nTheorem 1 describes the convergence of FedVote for\n\u03b7\nthat satisfies the assumption. I wonder why the author(s) do not give a Corollary on the convergence rate for a particular choice of\n\u03b7\n(which is usually done in the literature, e.g., see the analysis in [1])? It is not straightforward for readers to see the convergence (or non-convergence) without a concrete choice of\n\u03b7\n.\nIn Remark 2, the author(s) mention that the variance of quantization error will impede the convergence, which also holds for FedPAQ. However, I do not see any non-convergence issue of FedPAQ from [Theorem 2, 1]. Could the author(s) explain more on the non-convergence of FedPAQ?\nOn the theory side, could the author(s) elaborate the advantages of the proposed method over FedPAQ?\n[1] Reisizadeh et al. FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization. AISTATS 2020.I have some questions about the theoretical analysis of this paper. I hope that the author(s) could kindly elaborate.", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "5: marginally below the acceptance threshold", "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."}], "Decision": "Reject", "novelty_sentence": ["Empirical evaluations is conducted on two datasets ,Fashion-MNIST and CIFAR-10, and applied to two models, LeNet-5 and VGG-7, respectively.Mainly, the paper has the following weaknesses:\nThe main proposed method (FedVote) is incremental at best, and the proposal on the adversarial aspect (Byzantine-FedVote) is limited.", "Empirical evaluations on Fashion-MNIST and CIFAR-10 show that the proposed method outperforms existing local update/gradient compression-based schemes.Strengths:\nThe paper presents a novel communication efficient federated learning algorithm that quantizes local model weights as opposed to quantizing the local gradients."], "novelty_score": 2}, {"id": "UxTR9Z2DW8R", "paper_content": "Title\nReinforcement Learning State Estimation for High-Dimensional Nonlinear Systems\nAbstract\nIn high-dimensional nonlinear systems such as fluid flows, the design of state estimators such as Kalman filters relies on a reduced-order model (ROM) of the dynamics. However, ROMs are prone to large errors, which negatively affects the performance of the estimator. Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE), a ROM-based estimator in which the data assimilation feedback term is given by a nonlinear stochastic policy trained through reinforcement learning. The flexibility of the nonlinear policy enables the RLROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates.\nN/A\nIn high-dimensional nonlinear systems such as fluid flows, the design of state estimators such as Kalman filters relies on a reduced-order model (ROM) of the dynamics. However, ROMs are prone to large errors, which negatively affects the performance of the estimator. Here, we introduce the reinforcement learning reduced-order estimator (RL-ROE), a ROM-based estimator in which the data assimilation feedback term is given by a nonlinear stochastic policy trained through reinforcement learning. The flexibility of the nonlinear policy enables the RLROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates.\n1 INTRODUCTION\nActive control of turbulent flows has the potential to cut down emissions across a range of industries through drag reduction in aircrafts and ships or improved efficiency of heating and air-conditioning systems, among many other examples (Brunton & Noack, 2015). But real-time feedback control requires inferring the state of the system from sparse measurements using an algorithm called a state estimator, which typically relies on a model for the underlying dynamics (Simon, 2006). Among state estimators, the Kalman filter is by far the most well-known thanks to its optimality for linear systems, which has led to its widespread use in numerous applications (Kalman, 1960; Zarchan & Musoff, 2015). However, continuous systems such as fluid flows are governed by partial differential equations (PDEs) which, when discretized, yield high-dimensional and oftentimes nonlinear dynamical models with hundreds or thousands of state variables. These high-dimensional models are too expensive to integrate with common state estimation techniques, including the Kalman filter or its numerous extensions. Thus, state estimators are instead designed based on a reduced-order model (ROM) of the underlying dynamics (Barbagallo et al., 2009; Rowley & Dawson, 2017).\nA big challenge is that ROMs provide a simplified and imperfect description of the dynamics, which negatively affects the performance of the state estimator. One potential solution is to improve the accuracy of the ROM itself through the inclusion of additional closure terms (Ahmed et al., 2021). In this paper, we leave the ROM untouched and instead propose a new design paradigm for the estimator itself, which we call a reinforcement-learning reduced-order estimator (RL-ROE). The RL-ROE is constructed from the ROM in an analogous way to a Kalman filter, with the crucial difference that the linear filter gain function is replaced by a nonlinear stochastic policy trained through reinforcement learning (RL). The flexibility of the nonlinear policy enables the RL-ROE to compensate for errors of the ROM, while still taking advantage of the imperfect knowledge of the dynamics. We describe how we frame the problem as a stationary Markov decision process in order to enable RL training, which is non-trivial since the RL-ROE must be able to estimate time-varying states. Finally, we show that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM, and displays robust estimation performance with respect to different reference trajectories and initial state estimates. The RL-ROE is the first application of reinforcement learning to state estimation for high-dimensional systems.\nUnder review as a conference paper at ICLR 2022\n2 PROBLEM FORMULATION\n\n2.1 SETUP\nConsider the discrete-time nonlinear system given by\nzk+1 = f(zk), (1a) yk = Czk, (1b)\nwhere zk \u2208 Rn and yk \u2208 Rp are respectively the state and measurement at time k, f : Rn \u2192 Rn is a time-invariant nonlinear map from current to next state, and C \u2208 Rp\u00d7n is a linear map from state to measurement. In this study, we assume that the dynamics given in (1) are obtained from the numerical discretization of a nonlinear partial differential equation (PDE), which typically requires a large number n of state dimensions. Note that we do not account for exogenous control inputs to the system, which will be studied in future extensions of the present work.\n2.2 REDUCED-ORDER MODEL\nBecause the high dimensionality of (1) makes online prediction and control impractical, it is instead customary to formulate a reduced-order model (ROM) of the dynamics (Rowley & Dawson, 2017). First, one chooses a suitable linearly independent set of modes {u1, . . . ,ur}, where ui \u2208 Rn, defining an r-dimensional subspace of Rn in which most of the dynamics is assumed to take place. Stacking these modes as columns of a matrix U \u2208 Rn\u00d7r, one can then express zk ' Uxk, where the reduced-order state xk \u2208 Rr represents the coordinates of zk in the subspace. Finally, one finds a ROM for the dynamics of xk, which is vastly cheaper to evolve than (1) when r n. There exist various ways to find an appropriate set of modes U and corresponding ROM for the dynamics of xk (Taira et al., 2017). In this work, we employ the Dynamic Mode Decomposition (DMD), a purely data-driven algorithm that has found wide applications in fields ranging from fluid dynamics to neuroscience (Schmid, 2010; Kutz et al., 2016). Starting with a collection of snapshots Z = {z0, . . . ,zm} collected along a trajectory of (1a), the DMD seeks a best-fit linear model of the dynamics in the form of a matrixA \u2208 Rn\u00d7n such that zk+1 ' Azk, and computes the modes U as the r leading principal component analysis (PCA) modes of Z. The transformation zk ' Uxk and the orthogonality of U then yield a linear discrete-time ROM of the form\nxk+1 = Arxk +wk, (2a) yk = Crxk + vk, (2b)\nwhereAr = UTAU \u2208 Rr\u00d7r andCr = CU \u2208 Rp\u00d7r are the reduced-order state-transition and observation models, respectively. In order to account for the neglected PCA modes of Z as well as the unmodeled dynamics incurred by the linear approximation zk+1 ' Azk, we add (unknown) nonGaussian process noise wk and observation noise vk. Additional details regarding the calculation ofAr and U are provided in Appendix A.\n2.3 REDUCED-ORDER ESTIMATOR\nThis paper uses reinforcement learning (RL) to solve the following estimation problem: given a sequence of measurements {y0, \u00b7 \u00b7 \u00b7 ,yk} from a reference trajectory {z0, \u00b7 \u00b7 \u00b7 , zk} of (1) and knowing the ROM (2) defined byAr,Cr andU , we want to estimate the high-dimensional state zk at current time k. To this effect, we design a reduced-order estimator (ROE) of the form\nx\u0302k = Arx\u0302k\u22121 + ak, (3a) ak \u223c \u03c0\u03b8( \u00b7 |yk, x\u0302k\u22121), (3b)\nwhere x\u0302k is an estimate of the reduced-order state xk, and ak \u2208 Rr is an action sampled from a stochastic policy \u03c0\u03b8 which depends on the current measurement yk and the previous state estimate x\u0302k\u22121. The subscript \u03b8 denotes the set of parameters that defines the stochastic policy, whose goal is to minimize the mean square error E[zk \u2212 z\u0302k] over a range of reference trajectories and initial reduced-order state estimates. Here, z\u0302k = Ux\u0302k denotes the high-dimensional state estimate reconstructed from x\u0302k. A Kalman filter is a special case of such an estimator, for which the action in (3b) is given by\nak = Kk(yk \u2212CrArx\u0302k\u22121), (4)\nUnder review as a conference paper at ICLR 2022\nwith Kk \u2208 Rr\u00d7p the optimal Kalman gain. Although the Kalman filter is optimal when the statetransition and observation models are known exactly, its performance suffers in the presence of unmodeled dynamics. In our case, such model errors are unavoidable due to the ROM (2) being an inherent approximation of the high-dimensional dynamics (1), which motivates our adoption of the more general form (3b). This form retains the dependence of ak on yk and x\u0302k\u22121 but is more flexible thanks to the nonlinearity of the stochastic policy \u03c0\u03b8, which we train with deep RL in an offline stage. The stochasticity of \u03c0\u03b8 forces the RL algorithm to explore different actions during the training process, in order to find eventually an optimal \u03b8\u2217 such that E[zk \u2212 z\u0302k] is minimized for various reference trajectories and initial estimates. We call the estimator constructed and trained through this process an RL-trained ROE, or RL-ROE for short.\nThus, the methodology we propose consists of two steps. In a first offline stage, a ROM of the form (2) is obtained using high-dimensional snapshots zk from a single trajectory of (1). The RL-ROE (3) is then constructed based on this ROM, and its policy\u03c0\u03b8 is trained using high-dimensional snapshots zk from multiple reference trajectories of (1). Finally, the trained RL-ROE is deployed online to track a reference trajectory of (1). In the online stage, the RL-ROE only requires measurements yk from the reference trajectory, and gives an estimate z\u0302k = Ux\u0302k for the high-dimensional state.\nIn summary, our contributions in this paper are two-fold:\n1. We propose RL-ROE, a reduced-order state estimator for high-dimensional nonlinear systems. The RL-ROE takes the form (3), which combines two unique features: first, the state transition modelAr is a ROM of the high-dimensional dynamics; second, the term ak that assimilates measurements is sampled from a stochastic policy \u03c0\u03b8 trained with RL. The training procedure for \u03c0\u03b8, which involves a non-trivial reformulation of the time-varying tracking problem as a stationary Markov decision process, is described in Section 4.\n2. The performance of the RL-ROE is compared in Section 5 with that of KF-ROE, a Kalman filter constructed from the same ROM. The comparison is performed in the context of the Burgers equation using a range of reference trajectories and initial state estimates.\n3 RELATED WORK\nPrevious studies have already proposed designing state estimators using policies trained through reinforcement learning. Morimoto & Doya (2007) introduced an estimator of the form x\u0302k = f(x\u0302k\u22121) + L(x\u0302k\u22121)(yk\u22121 \u2212Cx\u0302k\u22121), where f(\u00b7) is the state-transition model of the system, and the state-dependent filter gain matrix L(x\u0302k\u22121) is defined using Gaussian basis functions whose parameters are learned through a variant of vanilla policy gradient. Their reward function, however, was calculated using the measurement error instead of the state estimate error, potentially limiting the performance of the trained estimator. Hu et al. (2020) proposed an estimator of the form x\u0302k = f(x\u0302k\u22121) + L(xk \u2212 x\u0302k)(yk \u2212 Cf(x\u0302k\u22121)), where L(xk \u2212 x\u0302k) is approximated by neural networks trained with a modified Soft-Actor Critic algorithm (Haarnoja et al., 2018). Although they derived convergence properties for the estimate error, the dependence of the filter gain L(xk \u2212 x\u0302k) on the reference state xk limits its practical application.\nA major difference between these past studies and our work is that they do not construct a ROM of the dynamics and only consider low-dimensional systems with four state variables at most, in comparison with the hundred or more state dimensions that our RL-ROE can handle. Therefore, RL-ROE represents the first application of reinforcement learning to state estimation for high-dimensional systems, which makes it applicable to systems governed by PDEs such as fluid flows.\n4 TRAINING METHODOLOGY\nIn this section, we describe the offline training process for the policy \u03c0\u03b8 in the RL-ROE (3). In order to train \u03c0\u03b8 with reinforcement learning, we need to formulate the problem as a stationary Markov decision process (MDP). However, this is no trivial task given that the aim of the policy is to minimize the error between the state estimate z\u0302k = Ux\u0302k and a time-dependent reference state zk. At first sight, such trajectory tracking problem requires a time-dependent reward function and, therefore, a time-varying MDP.\nUnder review as a conference paper at ICLR 2022\nTo be able to use off-the-shelf RL algorithms, we introduce a trick to translate this time-varying MDP to an equivalent, extended stationary MDP. Indeed, we show hereafter that the problem can be framed as a stationary MDP by including zk into our definition of the MDP\u2019s state. Letting sk = (zk, x\u0302k\u22121) \u2208 Rn+r denote an augmented state at time k, we can define an MDP consisting of the tuple (S,A,P,R), where S = Rn+r is the augmented state space, A \u2282 Rr is the action space, P(\u00b7|sk,ak) is a transition probability, and R(sk,ak, sk+1) is a reward function. At each time step k, the agent selects an action ak \u2208 A according to the policy \u03c0\u03b8 defined in (3b), which can be expressed as ak \u223c \u03c0\u03b8( \u00b7 |ok), (5) where ok = (yk, x\u0302k\u22121) = (Czk, x\u0302k\u22121) is a partial observation of the current state sk. The state sk+1 = (zk+1, x\u0302k) at the next time step is then obtained from equations (1a) and (3a) as\nsk+1 = (f(zk),Arx\u0302k\u22121 + ak), (6)\nwhich defines the transition model sk+1 \u223c P(\u00b7|sk,ak). Finally, the agent receives the reward\nrk = R(sk,ak, sk+1) = \u2212(zk \u2212Ux\u0302k)TQ(zk \u2212Ux\u0302k)\u2212 aTkRak, (7) where Q \u2208 Rn\u00d7n and R \u2208 Rr\u00d7r are positive semidefinite and positive definite matrices, respectively. The first term in the reward functionR penalizes the difference between the high-dimensional state estimate z\u0302k = Ux\u0302k and the reference zk, which is only partially observed by the agent. The second term favors smaller values for the action ak; such regularization leads to more robust estimation performance in the presence of noise during online deployment of the RL-ROE, as we will see later. Unless indicated otherwise, we will consider Q = I and R = I . Thanks to the incorporation of zk into sk, the reward function (7) has no explicit time dependence and the MDP is therefore stationary.\nThe goal of the RL training process is then to find the optimal policy parameters\n\u03b8\u2217 = arg max \u03b8 E \u03c4\u223c\u03c0\u03b8 [R(\u03c4)], (8)\nwhere the expectation is over trajectories \u03c4 = (s1,a1, s2,a2, . . . ), and R(\u03c4) is the finite-horizon undiscounted return\nR(\u03c4) = K\u2211 k=1 rk, (9)\nwith the integer K denoting the length of each training trajectory. Contrary to conventional RL notation, trajectories here start at time k = 1. Indeed, the environment is initialized at time k = 0 according to the distributions\nz0 \u223c pz0(\u00b7), (10a) x\u03020 \u223c px\u03020(\u00b7), (10b)\nfrom which the augmented state s1 = (z1, x\u03020) = (f(z0), x\u03020) follows immediately. Thus, s1 constitutes the start of the trajectory of agent-environment interactions. This sequence of operations mirrors that of a Kalman filter: the calculation of the current state estimate uses the previous state estimate as well as the current measurement, and therefore begins from the second time step, which is k = 1 in our case.\nTo find the optimal policy parameters \u03b8\u2217, we employ the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017), which belongs to the class of policy gradient methods (Sutton et al., 2000). PPO alternates between sampling data by computing a set of trajectories {\u03c41, \u03c42, \u03c43, . . . } using the most recent version of the policy, and updating the policy parameters \u03b8 in a way that increases the probability of actions that led to higher rewards during the sampling phase. The policy \u03c0\u03b8 encodes a diagonal Gaussian distribution described by a neural network that maps from observation to mean action, \u00b5\u03b8\u2032(ok), together with a vector of standard deviations \u03c3, so that \u03b8 = {\u03b8\u2032,\u03c3}. We utilize the Stable Baselines3 (SB3) implementation of PPO (Raffin et al., 2019) and define our MDP as a custom environment in OpenAI Gym (Brockman et al., 2016). Besides the discount factor \u03b3, all results to follow are obtained with the default PPO hyperparameters in SB3, which demonstrates the robustness of our approach with respect to the RL hyperparameters.\nRemark 1. The offline RL training phase described in this section (to find the optimal parameters \u03b8\u2217) requires knowledge of the high-dimensional state zk from several reference trajectories of (1).\nUnder review as a conference paper at ICLR 2022\nDuring online deployment, however, the RL-ROE (3) only relies on measurements yk, since the trained policy \u03c0\u03b8\u2217 is conditioned on yk and the previous reduced state estimate x\u0302k\u22121.\nRemark 2. Similar to any learning method, the trained RL-ROE is expected to perform well for reference trajectories and initial estimates sampled from the same distributions used during the offline training phase. Thus, the choice of the distributions for z0 and x\u03020 in (10) is critical since it defines the range of initial reference states and initial estimates that the trained RL-ROE will be able to handle during online deployment. This is similar to the state of the art in model-based observer theory, which only guarantees convergence of nonlinear observers locally, i.e., near the true initial states (Benosman & Borggaard, 2021).\n5 RESULTS\nWe evaluate our proposed RL-ROE using simulations of the Burgers equation, a prototypical nonlinear hyperbolic PDE which takes the form\n\u2202u \u2202t + u \u2202u \u2202x \u2212 \u03bd \u2202\n2u\n\u2202x2 = f(x, t), (11)\nwhere u(x, t) is the velocity at position x \u2208 [0, L] and time t \u2208 [0, T ], f(x, t) is a distributed time-dependent forcing, and the scalar parameter \u03bd acts like a viscosity. The boundary conditions are periodic and the initial condition u(x, 0) = u0(x) will be specified later. We choose L = 1, T = 10, and \u03bd = 0.01.\nWe discretize the Burgers equation (11) with a spectral method using n = 256 Fourier modes, and integrate forward in time using a fifth-order Runge-Kutta method. Solution snapshots are saved every \u2206t = 0.05 time unit, yielding the discrete high-dimensional state\nzk = [u(x0, k\u2206t), u(x1, k\u2206t), . . . , u(xn\u22121, k\u2206t)] T \u2208 Rn, (12)\nwhere xi = iL/n are the collocation points, and k = 0, . . . , T/\u2206t is the time index. Further, we place p = 8 sensors at equidistant locations, providing the sparse measurement vector\nyk = [u(x\u03040, k\u2206t), u(x\u03041, k\u2206t), . . . , u(x\u0304p\u22121, k\u2206t)] T \u2208 Rp, (13)\nwhere x\u0304i = iL/p are the sensor locations. Thus, the state zk and measurement yk are governed by a high-dimensional discrete-time nonlinear system of the form given in equation (1).\nWe then follow the procedure outlined in Section 2 to construct offline a ROM and corresponding ROE, which we train offline using PPO following the methodology presented in Section 4. Finally, the trained RL-ROE is deployed online and compared against a Kalman filter under various initial conditions for the reference trajectory as well as the state estimate. Specifically, we adopt the following steps:\n1. Construction of the ROM. Starting from the pulse-shaped initial condition\nu0(x) = 1\ncosh(20(x\u2212 L/2)) , (14)\nwe calculate one solution trajectory of (1) for t \u2208 [0, T/2] = [0, 5], and we denote as ZDMD = {zDMD0 , . . . ,zDMDm } the resulting solution snapshots at times k = 0, . . . ,m = T/2\u2206t. The DMD is then applied to these snapshots, yielding a ROM of the form (2) defined by matrices Ar, Cr and U . The ROM governs the evolution of a reduced-order state xk ' UTzk \u2208 Rr. We pick r = 15 for the dimensionality of the reduced-order subspace (which corresponds in this case to 99.99% of the energy of the snapshots ZDMD being included in the modes U ), giving the ROM a significant computational advantage compared with the high-dimensional system (1) of size n = 256.\n2. Training of the RL-ROE. We train the stochastic policy \u03c0\u03b8 of the RL-ROE (3) using PPO, as described in Section 4. In order for the resulting estimator to perform well under various reference trajectories and initial estimates, we initialize each trajectory of the MDP during the offline training process with\nz0 = \u03b1z DMD 0 , (15a) x\u03020 = U TzDMD0 + \u03b2, (15b)\nUnder review as a conference paper at ICLR 2022\n(a) (b) (c)\n(d) (e) (f)\nwhere \u03b1 \u223c U(0.5, 2) \u2208 R and \u03b2 \u223c N (0, 0.1I) \u2208 Rr, which defines the distributions given in (10). In other words, the reference trajectories are initialized as a randomly scaled up or scaled down version of the pulse defined in (14), and the reduced-order state estimate is initialized as a reduced-order projection of that pulse, polluted with additive Gaussian noise. During training, we limit each trajectory to the same time window t \u2208 [0, T/2] that was used in constructing the ROM \u2013 that is, we pick K = T/2\u2206t in the finite-horizon return (9). We end the training when the return no longer increases on average.\n3. Evaluation of the RL-ROE. We evaluate the trained RL-ROE against a time-dependent Kalman filter constructed from the same ROM, which we refer to as KF-ROE. The KFROE is given by equations (3a) and (4), with the calculation of the time-varying Kalman gain detailed in Appendix B. The RL-ROE and KF-ROE are compared online based on three specific reference trajectories initialized from (15a) using \u03b1 = 0.5, 1, and 2. For each reference trajectory, we consider 20 different initial state estimates sampled from (15b), and feed measurements yk to the RL-ROE and KF-ROE. Their tracking performance of the reference state zk is then evaluated and compared over the full time window t \u2208 [0, T ].\nWe carry out the above procedure for two different choices of the forcing f(x, t), each leading to qualitatively different dynamics: first, the unforced case f(x, t) = 0, and second, a sinusoidal forcing of the form f(x, t) = sin(\u03c9t \u2212 kx) with \u03c9 = \u03c0 and k = 2\u03c0/L. We emphasize that the RL-ROE is not (yet) designed to account for different exogenous control inputs to the system. Here, the forcing f(x, t) is simply considered as a way to generate more complex solutions by altering the dynamics of the PDE. Thus, the two choices for f(x, t) will be treated separately in the results below, and we will construct and train a different RL-ROE in each case.\n5.1 UNFORCED CASE\nBeginning with the unforced case f(x, t) = 0, we first construct the ROM on which the RL-ROE and KF-ROE will be based. Figure 1 depicts the solution of the discretized Burgers equation to the initial condition (14). The solution snapshots belonging to the time window t \u2208 [0, T/2] = [0, 5] constitute the training data ZDMD fed to the DMD algorithm, which yields the ROM (2). Further, this figure illustrates the type of dynamics exhibited by the Burgers equation in the unforced regime: a quickly decaying and widening pulse for t \u2264 2, followed by a slow convergence to a uniform steady state for t > 2.\nBefore training and evaluating the RL-ROE, we quantify the accuracy of the ROM itself \u2013 that is, given knowledge of the true initial condition. For this purpose, we consider three initial conditions defined by (15a) with \u03b1 = 0.5, 1, 2, and we compute the corresponding reference solution (using the discretized Burgers equation) as well as the ROM solution. The results, reported in Appendix C, show that the model error is very low for the initial condition \u03b1 = 1 and time window t \u2208 [0, 5],\nUnder review as a conference paper at ICLR 2022\nROM training data\n(a) (b)\nsince this case corresponds to the same solution trajectory used for constructing the ROM. On the other hand, the model error increases for larger times or other values of \u03b1.\nThe RL-ROE is then trained using PPO following the methodology outlined in Section 4, together with the distributions (15) for trajectory initialization. The RL hyperparameters and learning curve displaying the performance improvement of the RL-ROE during the training process are reported in Appendix D. The trained RL-ROE is now compared with the KF-ROE, a Kalman filter constructed from the same ROM. Figures 2(a,b,c) show the L2 error of the RL-ROE and KF-ROE with respect to specific reference trajectories of the Burgers equation, initialized from (15a) using \u03b1 = 0.5, 1, 2. For each reference trajectory, we consider 20 different initial state estimates sampled from (15b). The curves and shaded area reported in Figures 2(a,b,c) indicate the mean and standard deviation of the error, defined at time step k by |Ux\u0302k\u2212zk|, where x\u0302k is the reduced-order estimate given by the RL-ROE or KF-ROE, and zk is the high-dimensional reference solution. (Appendix E shows the same data for t \u2208 [0, 2].) Figures 2(d,e,f) show the trajectories of the reference, RL-ROE and KFROE solutions in a two-dimensional slice of phase space spanned by the second and third columns1 of U \u2013 in other words, the time history of the second and third components of UTzk and x\u0302k.\nA few important observations emerge from Figure 2. First, when the ROM suffers from large model errors due to initial conditions deviations, as is the case for \u03b1 = 0.5 and \u03b1 = 2, the RL-ROE is able to outperform the KF-ROE. In the time window t \u2264 2 during which most of the transient dynamics take place, the RL-ROE displays up to an order of magnitude lower error than the KF-ROE. Second, when the ROM is very accurate, as is the case for \u03b1 = 1, the KF-ROE gives lower error for most of the time duration. Even then, however, Figure 2(e) shows that the RL-ROE converges faster to the reference trajectory. Last, the RL-ROE manages to keep the error at a low level in the time window t \u2208 [5, 10], despite the fact that it was trained using trajectories that end at t = 5.\n1Since the columns of U approximate the PCA modes of the training snapshots ZDMD without centering, the first column will be dominated by the mean of the data. Thus, we display the trajectory coordinates associated with the second and third columns, which capture the largest amount of variance within the data.\nUnder review as a conference paper at ICLR 2022\n(a) (b) (c)\n(d) (e) (f)\nThe Burgers equation converges to a uniform steady state in this unforced case, which can explain why both the RL-ROE and KF-ROE display low errors at larger times. We now investigate a fundamentally different dynamical regime in the next section by adding nonzero forcing to the Burgers equation, which prevents the solution from settling on a steady state.\n5.2 FORCED CASE\nWe now consider the forced case f(x, t) = sin(\u03c9t \u2212 kx), with \u03c9 = \u03c0 and k = 2\u03c0/L. Repeating the steps of the previous section, we first construct the ROM on which the RL-ROE and KF-ROE will be based. Figure 3 depicts the solution of the discretized Burgers equation with forcing to the initial condition (14). The solution snapshots belonging to the time window t \u2208 [0, T/2] = [0, 5] constitute the training data ZDMD fed to the DMD algorithm for constructing the ROM (2). This figure illustrates the type of dynamics exhibited by the Burgers equation in the forced regime: a transient phase with for t \u2264 2 during which the pulse changes shape into a skewed wave, followed by a limit cycle regime with the wave traveling at constant velocity for t > 2.\nAs in the unforced case, we first quantify the accuracy of the ROM itself. For this purpose, we consider three initial conditions defined by (15a) with \u03b1 = 0.5, 1, 2, and we compute the corresponding reference solution (using the discretized Burgers equation with forcing) as well as the ROM solution. This time, the results reported in Appendix C show that the model error is higher for all cases, which reflects the more complicated nature of the dynamics in this forced regime.\nThe RL-ROE is then trained using PPO following the methodology outlined in Section 4, together with the distributions (15) for trajectory initialization. The RL hyperparameters and learning curve displaying the performance improvement of the RL-ROE during the training process are reported in Appendix D. The trained RL-ROE is now compared with the KF-ROE, a Kalman filter constructed from the same ROM. Figures 4(a,b,c) show the L2 error of the RL-ROE and KF-ROE with respect to specific reference trajectories of the Burgers equation, initialized from (15a) using \u03b1 = 0.5, 1, 2. As before, we consider 20 different initial state estimates sampled from (15b) for each reference trajectory. The curves and the surrounding shade reported in Figures 4(a,b,c) indicate the mean and standard deviation of the resulting error, respectively. (Appendix E shows the same data for t \u2208 [0, 2].) Figures 4(d,e,f) show the trajectories of the reference, RL-ROE and KF-ROE solutions in a two-dimensional slice of phase space spanned by the second and third columns of U \u2013 in other words, the time history of the second and third components of UTzk and x\u0302k.\nThe RL-ROE outperforms the KF-ROE in both the initial transient phase as well as the later limit cycle regime, for all three reference trajectories. This is consistent with our previous observation that the RL-ROE has an advantage over the KF-ROE when the ROM suffers from large model errors. Moreover, the estimation performance of the RL-ROE remains stable in the time window t \u2208 [5, 10], even though trajectories stop at t = 5 during the training process. All together, these\nUnder review as a conference paper at ICLR 2022\nROM training data\n(a) (b)\nresults demonstrate the estimation performance of the RL-ROE, and its robustness with respect to model errors.\nWe include a number of additional results as appendices. In Appendix F, we show that the regularization term in the reward function (7) leads to better estimation performance in the presence of observation noise. In Appendix G, we compare our RL-ROE approach with two alternative datadriven estimators formulated as standard supervised learning problems, and we observe that the RL-ROE is more robust to observation noise and gives smoother predictions.\n6 CONCLUSIONS\nIn this paper, we have introduced the reinforcement learning reduced-order estimator (RL-ROE), a new methodology for estimating the state of a high-dimensional nonlinear dynamical system. Our approach follows the standard practice of constructing a computationally inexpensive reduced-order model (ROM) to approximate the dynamics of the system. The novelty of our contribution lies in the design, based on this ROM, of a reduced-order estimator (ROE) in which the feedback correction term is given by a nonlinear stochastic policy trained through reinforcement learning. To be able to use off-the-shelf RL algorithms, we introduce a trick to translate this trajectory tracking problem, i.e., time-varying MDP, to an equivalent stationary MDP based on an augmented state. We show using simulations of the Burgers equation in two very different dynamical regimes that the trained RL-ROE is able to outperform a Kalman filter designed using the same ROM and displays robust estimation performance with respect to different reference trajectories and initial state estimates.\nThis work opens the door to a number of potential future directions. A logical next step is to evaluate the performance of the RL-ROE on other physical systems, for instance the Navier-Stokes equations governing the motion of fluid flows. Different types of ROM could also be considered, potentially leading to improved performance of the RL-ROE.\nUnder review as a conference paper at ICLR 2022\nA DYNAMIC MODE DECOMPOSITION\nIn this appendix, we describe the DMD algorithm (Schmid, 2010; Tu et al., 2014), which is a popular data-driven method to extract spatial modes and low-dimensional dynamics from a dataset of highdimensional snapshots. Here, we use the DMD to construct a ROM of the form (2) given a snapshot sequence Z = {z0, . . . ,zm} collected along a trajectory of (1a) and an observation model C. Fundamentally, the DMD seeks a best-fit linear model of the dynamics in the form of a matrix A \u2208 Rn\u00d7n such that zk+1 ' Azk. Arranging the snapshots into two time-shifted matrices\nX = {z0, . . . ,zm\u22121}, Y = {z1, . . . ,zm}, (16)\nthe best-fit linear model is given by A = Y X\u2020, where X\u2020 is the pseudoinverse of X . The ROM is then obtained by projecting the matrices A and C onto a basis U consisting of the r leading left singular vectors of X , which approximate the r leading PCA modes of Z. Using the truncated singular value decomposition\nX = U\u03a3V T (17)\nwhere U ,V \u2208 Rn\u00d7r and \u03a3 \u2208 Rr\u00d7r, the resulting reduced-order state-transition and observation models are given by\nAr = U TAU = UTY V \u03a3\u22121, (18a)\nCr = CU . (18b)\nConveniently, the ROM matrices Ar and Cr can be calculated directly from the truncated SVD of X , which avoids forming the large n\u00d7 n matrixA.\nB KALMAN FILTER\nThe time-dependent Kalman filter that we use as a benchmark in this paper, KF-ROE, is based on the same ROM (2) as the RL-ROE, with identical matrices Ar, Cr and U . Similarly to the RL-ROE, the reduced-order estimate x\u0302k is given by equation (3a), from which the high-dimensional estimate is reconstructed as z\u0302k = Ux\u0302k. However, the KF-ROE differs from the RL-ROE in its definition of the action ak in (3a), which is instead given by the linear feedback term (4). The calculation of the optimal Kalman gainKk in (4) requires the following operations at each time step:\nP\u2212k = ArPk\u22121A T r +Qk, (19)\nSk = CrP \u2212 k C T r +Rk, (20)\nKk = P \u2212 k C T r S \u22121 k , (21)\nPk = (I \u2212KkCr)P\u2212k , (22)\nwhere P\u2212k and Pk are respectively the a priori and a posteriori estimate covariance matrices, Sk is the innovation covariance, and Qk and Rk are respectively the covariance matrices of the process noise wk and observation noise vk in the ROM (2). Since these noise covariance matrices are unknown, we choose Qk = Rk = I for all k after verifying empirically that these values yield the best possible results. At time step k = 0, the a posteriori estimate covariance is initialized as P0 = cov(UTz0 \u2212 x\u03020), which can be calculated from the initial reference and estimated state distributions (15).\nRemark 3. We are seeking a Kalman-type observer, which in \u2018filters domain\u2019 is also known as an infinite impulse response filter (IIR). These observers are to be contrasted with the finite impulse filters (FIR). Indeed, the later are well known to be based on a mapping between n previous samples of input/output and the desired observed state at the current instant (e.g., Chaupa et al., equations 2,3, and 12), and lead to exact convergence in finite-time, in the noiseless setting. On the other hand the IIR observer is well known to be based on the last measurement of the output/input only, e.g., (Chaupa et al., equation 8), and lead to an average asymptotic performance, e.g.,maximum likelihood estimate in the our case.\nUnder review as a conference paper at ICLR 2022\nROM training data\n(a) (b)\nC MODEL ERROR\nIn this appendix, we quantify the accuracy of the two ROMs utilized in Sections 5.1 and 5.2 for the unforced and forced Burgers equations, respectively. For this purpose, we consider three initial conditions defined by (15a) with \u03b1 = 0.5, 1, 2, and we compute the corresponding reference solution (using the discretized Burgers equation) as well as the ROM solution. Results pertaining to the unforced and forced cases are shown in Figures 5 and 6, respectively.\nFigures 5(a,b,c) and 6(a,b,c) show the resulting L2 error of the ROM solution, defined at each time step k by |Uxk \u2212 zk|, where xk is the reduced-order state given by the ROM and zk is the highdimensional reference solution. Figures 5(d,e,f) and 6(d,e,f) show the trajectories of the reference and ROM solutions in a two-dimensional slice of phase space spanned by the second and third columns of U \u2013 in other words, the time history of the second and third components of UTzk and xk. The L2 error curves and ROM trajectory curves are colored according to the value of time using the same color scheme, which confirms that most of the dynamics take place during the first two time units, as already observed in Figure 1 for \u03b1 = 1.\nD TRAINING HYPERPARAMETERS AND LEARNING CURVES\nThe stochastic policy \u03c0\u03b8 is trained with PPO using the default hyperparameters from Stable Baselines3, except for the discount factor \u03b3 which we choose as 0.75. The mean output of the stochastic policy and the value function are approximated by two neural networks, each containing two hidden layers with 64 neurons and tanh activation functions. The training process alternates between sampling data for 20 trajectories (of length 100 timesteps each) and updating the policy. Each policy update consists of multiple gradients steps through the most recent data using 10 epochs, a minibatch size of 64 and a learning rate of 0.0003. The policy is trained for a total of one million timesteps, corresponding to 10000 trajectories. Figure 7 reports the learning curves corresponding to the unforced and forced cases. During training, the policy is tested (with stochasticity switched off) after each update using 10 separate test trajectories, and is saved if it outperforms the previous best policy.\nUnder review as a conference paper at ICLR 2022\nROM training data\n(a) (b)\nFinally, the RL-ROE is defined using the latest saved policy upon ending of the training process, and the stochasticity of the policy is switched off during subsequent evaluation of the RL-ROE.\nE ESTIMATION ERROR FOR SHORT TIMES\nFigures 8 and 9 show the same data as the first row in Figures 2 and 4, but for t \u2208 [0, 2]. Focusing on this initial transient phase, it becomes clear that the variance of the RL-ROE estimate decays much faster than the KF-ROE estimate.\nUnder review as a conference paper at ICLR 2022\nROM training data\n(a) (b)\nF EFFECT OF PENALIZING THE ACTION MAGNITUDE IN THE REWARD\nWe investigate the effect of penalizing the magnitude of the action ak in the reward function (7). To this effect, we repeat the experiments of Sections 5.1 and 5.2, this time training the RL-ROE using different magnitudes R for the matrix R = RI . When evaluating the performance of the trained RL-ROE, we consider different amounts of Gaussian observation noise added to the measurements yk. The results for the unforced case are shown in Figures 10, 11, and 12 for observation noise of standard deviation \u03c3 = 0, 0.1, and 0.3, respectively. The results for the forced case are shown in Figures 13, 14, and 15 for observation noise of standard deviation \u03c3 = 0, 0.1, and 0.3, respectively. In absence of noise, the highest estimation accuracy is obtained for R = 0, and decreases as R increases. However, in the presence of noise, Figures 11, 12, and 14, 15, the estimation accuracy is generally highest for R = 10. This confirms that penalizing the magnitude of the action ak in the reward function acts as a regularization that allows the RL-ROE to perform better on noisy measurement data.\nG COMPARISON WITH TWO SUPERVISED LEARNING APPROACHES\nIn this appendix, we compare our proposed RL-ROE with two alternative estimation techniques trained offline in a supervised learning setting.\nThe first approach is purely data-driven. Here, the estimator takes the form\nz\u0302k = UfDNN(yk, . . . ,yk\u2212q), (23)\nwhere the nonlinear function fDNN : Rp\u00d7q \u2192 Rr is a feed-forward deep neural network (DNN) taking a given number q of past observations as input. The DNN is then trained by minimizing the loss given for each data point by the squared estimation error ||z\u0302k \u2212 zk||2, where zk is the groundtruth high-dimensional state. The entire data set consists of the same high-dimensional reference trajectories used for training the RL-ROE \u2013 that is, they are initialized from the distribution (15a) and are limited to the time window t \u2208 [0, T/2] = [0, 5]. This results in a total of 14646 data points for all trajectories and time indices. The DNN consists of 4 hidden layers of 50 neurons each, and\nUnder review as a conference paper at ICLR 2022\nUnder review as a conference paper at ICLR 2022\nUnder review as a conference paper at ICLR 2022\nUnder review as a conference paper at ICLR 2022\nis trained using the Adam optimizer over 1000 epochs with minibatch size of 256. The resulting estimator is denoted DNN in the following results.\nThe second approach is a hybrid ROM-based and data-driven approach, similarly to our proposed RL-ROE. Here, the estimator takes the form\nz\u0302k = U(x\u0302 ROM k + fDNN(x\u0302 ROM k ,yk)), (24)\nwhere x\u0302ROMk is the reduced state generated by the raw ROM (in our case, the DMD model), and the nonlinear function fDNN : Rr+p \u2192 Rr is a DNN acting as a nonlinear correction to the ROM prediction. The DNN is then trained by minimizing the loss given for each data point by the squared estimation error ||z\u0302k \u2212 zk||2, where zk is the ground-truth high-dimensional state. The entire data set consists of pairwise combinations of high-dimensional reference trajectories and reduced ROM trajectories. Similar to the training process for the RL-ROE, the reference and ROM trajectories are initialized from the distributions (15a) and (15b), and are limited to the time window t \u2208 [0, T/2] = [0, 5]. This results in a total of 305020 data points for all pairs of trajectories and time indices. The DNN consists of 4 hidden layers of 80 neurons each, and is trained using the Adam optimizer over 500 epochs with minibatch size of 256. The resulting estimator is denoted ROM+DNN in the following results.\nWe compare in Figures 16 and 17 the estimation performance of the RL-ROE with that of the DNN estimator (23) using q = 3. Since the DNN estimator essentially acts as an interpolation function, it is more interesting to compare them in the presence of observation noise, which we model as Gaussian noise of standard deviation \u03c3 = 0.1 added to the measurements yk. The RLROE produces more accurate estimates with smoother trajectories; such robustness to measurement noise is a consequence of the stochasticity inherent to the RL training process. Furthermore, a clear benefit of the RL-ROE design philosophy is that it yields an accurate dynamic model, since the imperfect ROM dynamics (in our case given by DMD) are corrected by the RL-trained nonlinear policy ak \u223c \u03c0, unlike the DNN estimator design which has no inherent dynamics. This will later allow the RL-ROE to be paired with model-based controllers, opening the door to a wide class of control strategies that are commonly used with Kalman filters.\nWe compare in Figures 18 and 19 the estimation performance of the RL-ROE with that of the ROM+DNN estimator (24). The comparison is also performed in the presence of Gaussian observation noise of standard deviation \u03c3 = 0.1 added to the measurements yk. Once more, the RL-ROE\nUnder review as a conference paper at ICLR 2022\nproduces more accurate estimates with smoother trajectories, especially in the forced case. Finally, we show in Figure 20 that in the absence of observation noise, the RL-ROE has better generalization abilities once the time goes beyond the window t \u2208 [0, 5] seen during training. These comparisons suggest that the RL-ROE design, in which the policy directly corrects the ROM dynamics and is trained in an RL setting that accounts for errors compounding over time, is more robust to data not seen during training and to measurement noise.\nUnder review as a conference paper at ICLR 2022\nUnder review as a conference paper at ICLR 2022", "ref": {"BIBREF0": {"ref_id": "b0", "title": "On closures for reduced order models-a spectrum of first-principle to machine-learned avenues", "authors": [{"first": "E", "middle": [], "last": "Shady", "suffix": ""}, {"first": "Suraj", "middle": [], "last": "Ahmed", "suffix": ""}, {"first": "Omer", "middle": [], "last": "Pawar", "suffix": ""}, {"first": "Adil", "middle": [], "last": "San", "suffix": ""}, {"first": "Traian", "middle": [], "last": "Rasheed", "suffix": ""}, {"first": "Bernd", "middle": ["R"], "last": "Iliescu", "suffix": ""}, {"first": "", "middle": [], "last": "Noack", "suffix": ""}], "year": 2021, "venue": "Physics of Fluids", "volume": "33", "issue": "9", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Shady E Ahmed, Suraj Pawar, Omer San, Adil Rasheed, Traian Iliescu, and Bernd R Noack. On closures for reduced order models-a spectrum of first-principle to machine-learned avenues. Physics of Fluids, 33(9):091301, 2021.", "links": null}, "BIBREF1": {"ref_id": "b1", "title": "Closed-loop control of an open cavity flow using reduced-order models", "authors": [{"first": "Alexandre", "middle": [], "last": "Barbagallo", "suffix": ""}, {"first": "Denis", "middle": [], "last": "Sipp", "suffix": ""}, {"first": "Peter J", "middle": [], "last": "Schmid", "suffix": ""}], "year": 2009, "venue": "Journal of Fluid Mechanics", "volume": "641", "issue": "", "pages": "1--50", "other_ids": {}, "num": null, "urls": [], "raw_text": "Alexandre Barbagallo, Denis Sipp, and Peter J Schmid. Closed-loop control of an open cavity flow using reduced-order models. Journal of Fluid Mechanics, 641:1-50, 2009.", "links": null}, "BIBREF2": {"ref_id": "b2", "title": "Robust nonlinear state estimation for a class of infinitedimensional systems using reduced-order models", "authors": [{"first": "Mouhacine", "middle": [], "last": "Benosman", "suffix": ""}, {"first": "Jeff", "middle": [], "last": "Borggaard", "suffix": ""}], "year": null, "venue": "International Journal of Control", "volume": "94", "issue": "5", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Mouhacine Benosman and Jeff Borggaard. Robust nonlinear state estimation for a class of infinite- dimensional systems using reduced-order models. International Journal of Control, 94(5), 2021.", "links": null}, "BIBREF4": {"ref_id": "b4", "title": "Closed-loop turbulence control: Progress and challenges", "authors": [{"first": "L", "middle": [], "last": "Steven", "suffix": ""}, {"first": "Bernd", "middle": ["R"], "last": "Brunton", "suffix": ""}, {"first": "", "middle": [], "last": "Noack", "suffix": ""}], "year": 2015, "venue": "Applied Mechanics Reviews", "volume": "67", "issue": "5", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Steven L Brunton and Bernd R Noack. Closed-loop turbulence control: Progress and challenges. Applied Mechanics Reviews, 67(5), 2015.", "links": null}, "BIBREF5": {"ref_id": "b5", "title": "Model predictive control using different state observers", "authors": [{"first": "Petr", "middle": [], "last": "Chaupa", "suffix": ""}, {"first": "Jakub", "middle": [], "last": "Nov\u00e1k", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Janu\u0161ka", "suffix": ""}], "year": 2013, "venue": "Recent Advances in Automatic Control, Information and Communications", "volume": "", "issue": "", "pages": "191--196", "other_ids": {}, "num": null, "urls": [], "raw_text": "Petr Chaupa, Jakub Nov\u00e1k, and Peter Janu\u0161ka. Model predictive control using different state observers,. In Recent Advances in Automatic Control, Information and Communications, pages=191-196, year=2013.", "links": null}, "BIBREF6": {"ref_id": "b6", "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "authors": [{"first": "Tuomas", "middle": [], "last": "Haarnoja", "suffix": ""}, {"first": "Aurick", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Pieter", "middle": [], "last": "Abbeel", "suffix": ""}, {"first": "Sergey", "middle": [], "last": "Levine", "suffix": ""}], "year": 2018, "venue": "International conference on machine learning", "volume": "", "issue": "", "pages": "1861--1870", "other_ids": {}, "num": null, "urls": [], "raw_text": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International confer- ence on machine learning, pp. 1861-1870. PMLR, 2018.", "links": null}, "BIBREF7": {"ref_id": "b7", "title": "Lyapunov-based reinforcement learning state estimator", "authors": [{"first": "Liang", "middle": [], "last": "Hu", "suffix": ""}, {"first": "Chengwei", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Wei", "middle": [], "last": "Pan", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2010.13529"]}, "num": null, "urls": [], "raw_text": "Liang Hu, Chengwei Wu, and Wei Pan. Lyapunov-based reinforcement learning state estimator. arXiv preprint arXiv:2010.13529, 2020.", "links": null}, "BIBREF8": {"ref_id": "b8", "title": "A New Approach to Linear Filtering and Prediction Problems", "authors": [{"first": "R", "middle": ["E"], "last": "Kalman", "suffix": ""}], "year": 1960, "venue": "Journal of Basic Engineering", "volume": "82", "issue": "1", "pages": "35--45", "other_ids": {}, "num": null, "urls": [], "raw_text": "R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering, 82(1):35-45, 1960.", "links": null}, "BIBREF9": {"ref_id": "b9", "title": "Dynamic mode decomposition: data-driven modeling of complex systems", "authors": [{"first": "", "middle": [], "last": "Nathan Kutz", "suffix": ""}, {"first": "L", "middle": [], "last": "Steven", "suffix": ""}, {"first": "", "middle": [], "last": "Brunton", "suffix": ""}, {"first": "W", "middle": [], "last": "Bingni", "suffix": ""}, {"first": "Joshua", "middle": ["L"], "last": "Brunton", "suffix": ""}, {"first": "", "middle": [], "last": "Proctor", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "J Nathan Kutz, Steven L Brunton, Bingni W Brunton, and Joshua L Proctor. Dynamic mode decom- position: data-driven modeling of complex systems. SIAM, 2016.", "links": null}, "BIBREF10": {"ref_id": "b10", "title": "Reinforcement learning state estimator", "authors": [{"first": "Jun", "middle": [], "last": "Morimoto", "suffix": ""}, {"first": "Kenji", "middle": [], "last": "Doya", "suffix": ""}], "year": 2007, "venue": "Neural computation", "volume": "19", "issue": "3", "pages": "730--756", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jun Morimoto and Kenji Doya. Reinforcement learning state estimator. Neural computation, 19(3): 730-756, 2007.", "links": null}, "BIBREF11": {"ref_id": "b11", "title": "Stable baselines3", "authors": [{"first": "Antonin", "middle": [], "last": "Raffin", "suffix": ""}, {"first": "Ashley", "middle": [], "last": "Hill", "suffix": ""}, {"first": "Maximilian", "middle": [], "last": "Ernestus", "suffix": ""}, {"first": "Adam", "middle": [], "last": "Gleave", "suffix": ""}, {"first": "Anssi", "middle": [], "last": "Kanervisto", "suffix": ""}, {"first": "Noah", "middle": [], "last": "Dormann", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor- mann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019.", "links": null}, "BIBREF12": {"ref_id": "b12", "title": "Model reduction for flow analysis and control", "authors": [{"first": "W", "middle": [], "last": "Clarence", "suffix": ""}, {"first": "", "middle": [], "last": "Rowley", "suffix": ""}, {"first": "", "middle": [], "last": "Scott", "suffix": ""}, {"first": "", "middle": [], "last": "Dawson", "suffix": ""}], "year": 2017, "venue": "Annual Review of Fluid Mechanics", "volume": "49", "issue": "", "pages": "387--417", "other_ids": {}, "num": null, "urls": [], "raw_text": "Clarence W Rowley and Scott TM Dawson. Model reduction for flow analysis and control. Annual Review of Fluid Mechanics, 49:387-417, 2017.", "links": null}, "BIBREF13": {"ref_id": "b13", "title": "Dynamic mode decomposition of numerical and experimental data", "authors": [{"first": "J", "middle": [], "last": "Peter", "suffix": ""}, {"first": "", "middle": [], "last": "Schmid", "suffix": ""}], "year": 2010, "venue": "Journal of fluid mechanics", "volume": "656", "issue": "", "pages": "5--28", "other_ids": {}, "num": null, "urls": [], "raw_text": "Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics, 656:5-28, 2010.", "links": null}, "BIBREF14": {"ref_id": "b14", "title": "Proximal policy optimization algorithms", "authors": [{"first": "John", "middle": [], "last": "Schulman", "suffix": ""}, {"first": "Filip", "middle": [], "last": "Wolski", "suffix": ""}, {"first": "Prafulla", "middle": [], "last": "Dhariwal", "suffix": ""}, {"first": "Alec", "middle": [], "last": "Radford", "suffix": ""}, {"first": "Oleg", "middle": [], "last": "Klimov", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1707.06347"]}, "num": null, "urls": [], "raw_text": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.", "links": null}, "BIBREF15": {"ref_id": "b15", "title": "Optimal state estimation: Kalman, H infinity, and nonlinear approaches", "authors": [{"first": "Dan", "middle": [], "last": "Simon", "suffix": ""}], "year": 2006, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Dan Simon. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley & Sons, 2006.", "links": null}, "BIBREF16": {"ref_id": "b16", "title": "Policy gradient methods for reinforcement learning with function approximation", "authors": [{"first": "S", "middle": [], "last": "Richard", "suffix": ""}, {"first": "David", "middle": ["A"], "last": "Sutton", "suffix": ""}, {"first": "", "middle": [], "last": "Mcallester", "suffix": ""}, {"first": "P", "middle": [], "last": "Satinder", "suffix": ""}, {"first": "Yishay", "middle": [], "last": "Singh", "suffix": ""}, {"first": "", "middle": [], "last": "Mansour", "suffix": ""}], "year": 2000, "venue": "Advances in neural information processing systems", "volume": "", "issue": "", "pages": "1057--1063", "other_ids": {}, "num": null, "urls": [], "raw_text": "Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural informa- tion processing systems, pp. 1057-1063, 2000.", "links": null}, "BIBREF17": {"ref_id": "b17", "title": "Modal analysis of fluid flows: An overview", "authors": [{"first": "Kunihiko", "middle": [], "last": "Taira", "suffix": ""}, {"first": "L", "middle": [], "last": "Steven", "suffix": ""}, {"first": "", "middle": [], "last": "Brunton", "suffix": ""}, {"first": "T", "middle": ["M"], "last": "Scott", "suffix": ""}, {"first": "Clarence", "middle": ["W"], "last": "Dawson", "suffix": ""}, {"first": "Tim", "middle": [], "last": "Rowley", "suffix": ""}, {"first": "", "middle": [], "last": "Colonius", "suffix": ""}, {"first": "J", "middle": [], "last": "Beverley", "suffix": ""}, {"first": "", "middle": [], "last": "Mckeon", "suffix": ""}, {"first": "T", "middle": [], "last": "Oliver", "suffix": ""}, {"first": "Stanislav", "middle": [], "last": "Schmidt", "suffix": ""}, {"first": "Vassilios", "middle": [], "last": "Gordeyev", "suffix": ""}, {"first": "Lawrence", "middle": ["S"], "last": "Theofilis", "suffix": ""}, {"first": "", "middle": [], "last": "Ukeiley", "suffix": ""}], "year": 2017, "venue": "Aiaa Journal", "volume": "55", "issue": "12", "pages": "4013--4041", "other_ids": {}, "num": null, "urls": [], "raw_text": "Kunihiko Taira, Steven L Brunton, Scott TM Dawson, Clarence W Rowley, Tim Colonius, Bev- erley J McKeon, Oliver T Schmidt, Stanislav Gordeyev, Vassilios Theofilis, and Lawrence S Ukeiley. Modal analysis of fluid flows: An overview. Aiaa Journal, 55(12):4013-4041, 2017.", "links": null}, "BIBREF18": {"ref_id": "b18", "title": "On dynamic mode decomposition: theory and applications", "authors": [{"first": "H", "middle": [], "last": "Jonathan", "suffix": ""}, {"first": "Clarence", "middle": ["W"], "last": "Tu", "suffix": ""}, {"first": "Dirk", "middle": ["M"], "last": "Rowley", "suffix": ""}, {"first": "", "middle": [], "last": "Luchtenburg", "suffix": ""}, {"first": "L", "middle": [], "last": "Steven", "suffix": ""}, {"first": "J Nathan", "middle": [], "last": "Brunton", "suffix": ""}, {"first": "", "middle": [], "last": "Kutz", "suffix": ""}], "year": 2014, "venue": "Journal of Computational Dynamics", "volume": "1", "issue": "2", "pages": "391--421", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jonathan H Tu, Clarence W Rowley, Dirk M Luchtenburg, Steven L Brunton, and J Nathan Kutz. On dynamic mode decomposition: theory and applications. Journal of Computational Dynamics, 1(2):391-421, 2014.", "links": null}, "BIBREF19": {"ref_id": "b19", "title": "Fundamentals of Kalman filtering: a practical approach", "authors": [{"first": "Paul", "middle": [], "last": "Zarchan", "suffix": ""}, {"first": "Howard", "middle": [], "last": "Musoff", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Paul Zarchan and Howard Musoff. Fundamentals of Kalman filtering: a practical approach. Aiaa, 2015.", "links": null}}, "peer_review": [{"review_text": "This paper presents a method for state estimation of high dimensional dynamics by reinforcement learning. The idea is the same as that of Morimoto and Doya (2007) and the method is specialized for the context of estimating a high dimensional state vector based on a reduced dynamics model.Strength: The mathematical derivation looks quite solid. Weakness: The reward based on the ground truth high-dimensional state error rather than observation error would limit practical usage. The use of the the ground truth high-dimensional state also as the state variable would further limit its usability. The problem with reduced dynamics is PDMDP, but treating that as a time-dependent MDP may not be the best way. From the demonstrated result, the state estimator has to be learned for a particular forcing inputs, which would also limit its usage. Generalization over different forms of initial state and inputs should be demonstrated, analyzed or discussed.From the design and demonstration, the applicability of the method seems to be limited to known, simulated systems.", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "3: reject, not good enough", "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."}, {"review_text": "The objective of the paper is to construct an estimator for the state of a high-dimensional nonlinear dynamical system given partial observations of the state. The objective is motivated by applications in fluid mechanics or turbulent flows where the state of the system is large obtained by discretizing a PDE. The proposed approach has two main steps:\n(1) construction of a reduced order model. In particular, the paper proposes the dynamic mode decomposition method which only requires a single trajectory of the nonlinear dynamics.\n(2) formulating the problem of finding the estimator as a MDP problem and application of RL techniques to solve it. In particular, the estimator is modeled as a dynamical system driven by a stochastic control policy that depends on the current value of the estimate and the value of observation. The objective function is modeled with running cost equal to the error in estimating the state and a quadratic penalty on the control. Then, the optimal control policy is learned using a policy gradient method by sampling trajectories from the system.\nThe proposed approach is evaluated on a benchmark example that involves Burger's equation and compared with Kalman filter applied on the reduced order system.The problem addressed in the paper, finding an estimator for high dimensional nonlinear systems, is interesting and important. Application of RL techniques to learn the nonlinear estimator is definitely interesting and has potential.\nHowever, I can not recommend acceptance based on two main objections to the basic formulation of the proposed approach.\nThere is no analysis on why the proposed estimate, even if the MDP is solved accurately, is a meaningful estimate:\nIt is not clear what kind of estimator the paper aims to find. If the objective is to find an estimator that is optimal in mean-squared error, like Kalman filter, then such estimator does not evolve with a control that only depends on the current value of the estimate and observation. The optimal estimator is equal to the conditional expectation of the state given history of observation. It is only true in linear Gaussian setting that the estimator has the form of update law proposed in the paper, i.e. a control that only depends on the estimate and current value of observation. In general nonlinear non-Gaussian setting, which is the main motivation of the paper, one has to evolve the posterior distribution in order to compute the optimal estimate. The update law for the estimate will involve a control that depends on the whole history of the observation, not just the current observation as proposed in the paper. In short, there is no hope that the estimate proposed in the paper be close to the optimal estimate.\nNow, if the objective is not the optimal MSE estimate, it is not clear how solving the proposed MDP problem can produce a meaningful estimate. The paper does not provide any analysis on this.\nThe optimal control policy to the proposed MDP problem is stationary if the control is the function of the state s = (z,\\hat{x}). However, the control is parametrized as a function of observation and \\hat{x}. Therefore, it is not clear why such a formulation lead to a stationary policy.I can not recommend acceptance because there is no analysis on why the proposed estimate is meaningful and the argument on why the proposed MDP problem have a stationary policy is wrong.", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "3: reject, not good enough", "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."}, {"review_text": "This paper proposes a new state estimation method based on reinforcement learning for high-dimensional system obtained by discretizing continuous system originally modeled by partial differential equations (PDEs). The proposed method learns to correct the model errors caused by reduced order model (ROM). In the experiment, the proposed method named RL-ROE performs better than ordinary Kalman filter applied to the ROM.I read the paper with interest. The idea of using reinforcement learning for state estimation instead of Bayesian filtering method such as Kalman filter is very interesting, though it is not completely unique as the authors said in the section of related work. In the experiment, they applied the proposed method RL-ROE to the data set generated by numerically solving the Burgers equation. It outperforms Kalman filter applied to ROM in the forced case where the errors due to ROM is large, while there is not clear advantage in the unforced case where the ROM error is small. This result seems reasonable.\nI have mainly two questions. First, why did you use the reinforcement learning for state estimation ? In the training process, it seems to be assumed that one can access to the high-dimensional state vector\nz\n. So why don't you use supervised learning method to predict\nz\nt\nbased on the history of past observation with a loss function of prediction error on\nz\n? At least, it is necessary to compare RL-ROE with such straightfoward approach.\nSecondly, I think it is unfair to compare the proposed method which is allowed to fully access to the true latent vectors\nz\nt\nduring the training process with Kalman filter which has no training process. In my feeling, it is not surprising that RL-ROE outperforms KF with respect to the prediction error on\nz\nt\nin the test process, because RL-ROE can use the label information in the training process whereas KF cannot.This paper proposes a very interesting state estimation method based on reinforcement learning for high-dimensional system approximated by reduced order model. However the experiment result is not so convincing enough, and it needs comparison with other data-driven approach rather than Kalman filter.", "Correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "5: marginally below the acceptance threshold", "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."}, {"review_text": "This paper proposes using RL to train a policy to output correction terms in the reduced states of reduced order models in order to perform state estimation. In particular the approach is applied to DMD and Burgers equation, but could presumably be applied to other forms of ROMs and dynamical systems. The method was shown to perform well in experiments, and was found to be more robust than baseline methods in the presence of perturbations and noise.Overall I though this was a strong paper. I think this paper presents a reasonable application for RL in a non-traditional domain, and was able to show the approach working effectively in experiments. Having said that, my main concern is that this paper is somewhat lacking in experiments. All in all, though, I'm generally in favor of acceptance.\nSome questions/clarifications/concerns:\nI'm interested in the interplay between the Q and R terms in Eq. 6. How important is it to have the R to penalize the magnitude of actions? When does it help and hurt? My guess is R might perform some kind of regularization that allows the model to perform better in extrapolation. Can you add a comment describing what you've found.\nI understand that it's difficult to run too many experiments, but I think one interesting baseline would be to compare against a neural network trained in a supervised learning setting. The network could have the same inputs and outputs as the policy in this work and the loss could just be the square error between the full state and the full state estimate that you get from adding the network output to the reduced state estimate and mapping back to the state space. The reduced state estimates could be generated by the ROM alone (without any corrections applied). This experiment could provide insight into how much benefit from this method comes from its ability to make nonlinear corrections. The supervised learning approach wouldn't be able to account for the effect of its errors compounding over multiple time steps, but should be easier to train than a policy gradient approach.\nThe system studied here is much lower-dimensional than many systems studied in the fluids domain. Do we have any concerns about this approach scaling to higher-dimensional systems? Will learning a policy become more difficult as the dimensionality of the reduced states grows? Have you tried applying this approach to any other problems? etc.\n(Minor) Using 15 modes for the dimensionality of the ROM seems a little random. Was this chosen based on some kind of criteria with the singular values? A quick explanation for how this was chosen could be helpful.I think this paper presents an interesting approach and demonstrates its usefulness in experiments. While I feel that the experiments could have been more extensive, I think enough has been shown to warrant acceptance.", "Correctness": "4: All of the claims and statements are well-supported and correct.", "Technical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Empirical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Recommendation": "8: accept, good paper", "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."}], "Decision": "Reject", "novelty_sentence": ["The idea of using reinforcement learning for state estimation instead of Bayesian filtering method such as Kalman filter is very interesting, though it is not completely unique as the authors said in the section of related work.", "A quick explanation for how this was chosen could be helpful.I think this paper presents an interesting approach and demonstrates its usefulness in experiments."], "novelty_score": 2}, {"id": "olQbo52II9", "paper_content": "Title\nLearning to Solve Combinatorial Problems via Efficient Exploration\nAbstract\nFrom logistics to the natural sciences, combinatorial optimisation on graphs underpins numerous real-world applications. Reinforcement learning (RL) has shown particular promise in this setting as it can adapt to specific problem structures and does not require pre-solved instances for these, often NP-hard, problems. However, state-of-the-art (SOTA) approaches typically suffer from severe scalability issues, primarily due to their reliance on expensive graph neural networks (GNNs) at each decision step. We introduce ECORD; a novel RL algorithm that alleviates this expense by restricting the GNN to a single pre-processing step, before entering a fast-acting exploratory phase directed by a recurrent unit. Experimentally, we demonstrate that ECORD achieves a new SOTA for RL algorithms on the Maximum Cut problem, whilst also providing orders of magnitude improvement in speed and scalability. Compared to the nearest competitor, ECORD reduces the optimality gap by up to 73 % on 500 vertex graphs with a decreased wall-clock time. Moreover, ECORD retains strong performance when generalising to larger graphs with up to 10 000 vertices.\nN/A\nFrom logistics to the natural sciences, combinatorial optimisation on graphs underpins numerous real-world applications. Reinforcement learning (RL) has shown particular promise in this setting as it can adapt to specific problem structures and does not require pre-solved instances for these, often NP-hard, problems. However, state-of-the-art (SOTA) approaches typically suffer from severe scalability issues, primarily due to their reliance on expensive graph neural networks (GNNs) at each decision step. We introduce ECORD; a novel RL algorithm that alleviates this expense by restricting the GNN to a single pre-processing step, before entering a fast-acting exploratory phase directed by a recurrent unit. Experimentally, we demonstrate that ECORD achieves a new SOTA for RL algorithms on the Maximum Cut problem, whilst also providing orders of magnitude improvement in speed and scalability. Compared to the nearest competitor, ECORD reduces the optimality gap by up to 73 % on 500 vertex graphs with a decreased wall-clock time. Moreover, ECORD retains strong performance when generalising to larger graphs with up to 10 000 vertices.\n1 INTRODUCTION\nCombinatorial optimisation (CO) problems seek to find the ordering, labelling, or subset of discrete elements that maximises some objective function. Despite this seemingly abstract mathematical formulation, CO arises at the heart of many practical applications, from logistics (Yanling et al., 2010) to protein folding (Perdomo-Ortiz et al., 2012) and fundamental science (Barahona, 1982). However, with these tasks often being NP-hard, solving CO problems becomes increasingly challenging for all but the simplest of systems. This combination of conceptual simplicity, computational complexity, and practical importance has made CO a canonical challenge, and motivated significant efforts into developing approximate and heuristic algorithms for these tasks. Whilst approximate methods can offer guarantees on the solution quality, in practice they frequently lack sufficiently strong bounds and have limited scalability (Williamson & Shmoys, 2011). By contrast, well-designed heuristics offer no such guarantees but can prove highly efficient (Halim & Ismail, 2019).\nAs a result, recent years have seen a surge in the application of automated learning methods that parameterise CO heuristics with deep neural networks (Bengio et al., 2021). In particular, reinforcement learning (RL) has become a popular paradigm, as it can facilitate the discovery of novel heuristics without the need for labelled data. Moreover, many CO problems are naturally formulated as Markov decision processes (MDPs) on graphs, where vertices correspond to discrete variables and edges denote their interaction or dependence. Accordingly, graph neural networks (GNNs) have become the de-facto function approximator of choice as they reflect the underlying structure of the problem whilst seamlessly handling variable problem sizes and irregular topological structures.\nHowever, despite the demonstrated success of RL-GNN approaches, scalability remains an outstanding challenge. Running a GNN for every decision results in impractical computational overheads for anything beyond small- to medium-sized problems. This is exacerbated by the fact that directly predicting the solution to an NP-hard problem is typically unrealistic. As such, leading approaches often utilise stochastic exploration or structured search to generate multiple candidate solutions (Chen & Tian, 2019; Joshi et al., 2019; Gupta et al., 2020; Barrett et al., 2020; Bresson & Laurent, 2021) \u2013 which ultimately requires longer solving times and an increased computational burden.\nThe notion of exploratory combinatorial optimisation \u2013 reframing the task from predicting a single solution to exploring the solution space using reversible single-vertex actions (Barrett et al., 2020) \u2013 also hints at a mitigation to the scalability issue it highlights. Intuitively, when any decisions can be reversed, the quality of any single decision is less critical, so long as overall improvements are made over the course of many such actions. In this work, we leverage this notion to introduce a new algorithm, ECORD (Exploratory Combinatorial Optimisation with Rapid Decoding), that combines a single GNN preprocessing step with fast action-decoding that replaces further geometric inference with simple per-vertex observations and a learnt representation of the ongoing optimisation trajectory. The result is a theoretical and demonstrated speed-up over expensive GNN action-decoding with the action-selection time of ECORD being independent of the graph topology and, in practice, near constant regardless of graph size.\nExperimentally, we consider the Maximum Cut (Max-Cut) problem, a canonical CO problem chosen because of its generality (11 of the 21 NP-complete problems presented by Karp (1972) can be reduced to Max-Cut, including graph coloring, clique cover, knapsack and Steiner Tree) and the fact that it presents a challenging problem for scalable CO as the optimal solution requires every vertex to be correctly labelled (rather than simply a subset). This combination of wide-ranging applicability and intractability has motivated significant commercial and research efforts into Max-Cut solvers, from bespoke hardware based on classical (Goto et al., 2019) and quantum annealing (Yamamoto et al., 2017; Djidjev et al., 2018) to hand-crafted (Goemans & Williamson, 1995; Benlic & Hao, 2013) or learnt (Barrett et al., 2020) heuristic algorithms.\nECORD is found to equal or surpass the performance of expensive SOTA RL-GNN methods on graphs with up to 500 vertices (where all methods are computationally feasible), even when compared on number of actions instead of wall-clock time. Moreover, the low computational overhead of ECORD is seen to provide orders of magnitude improvements in speed and scalability, with strong performance, and a nearly 300\u00d7 increase in throughput when compared to conventional RL-GNN methods, demonstrated on graphs with up to 10 000 vertices.\n2 RELATED WORK\nThe application of neural networks to graph-based CO problems dates back to Hopfield & Tank (1985) who considered the Travelling Salesman Problem (TSP). However RL techniques were not applied until a decade later in the work of Zhang & Dietterich (1995) on the NP-hard job-shop problem. More recently, the advancement of deep learning and RL has triggered a resurgence in the ML community\u2019s interest in developing CO solvers, with multiple reviews providing detailed taxonomies (Bengio et al., 2021; Mazyavkina et al., 2020; Vesselinova et al., 2020).\nLearning to solve non-Euclidean CO. This resurgence began by considering Euclidean approaches that did not reflect the underlying graph structure of the problems. In this context, Bello et al. (2016) used RL to train pointer networks (PNs), which treat the discrete variables of the CO problem as a sequence of inputs to a recurrent unit (Vinyals et al., 2015) to solve TSP. By avoiding the need for labelled data sets, they were able to scale beyond the 40 vertex limit of Vinyals et al. (2015), inferring on 20 to 100 vertex graphs. Gu & Yang (2020) further scaled this approach to instances of up to 300, using a hybrid supervised-reinforcement learning framework that combined PNs and A3C (Mnih et al., 2016). However, although PNs can handle graphs of different sizes (with the help of manual input/output engineering, such as zero padding), these Euclidean approaches fail to capture the topological structures and intricate relationships contained within graphs, and typically require a large number of training instances in order to generalise.\nThis issue was addressed by Dai et al. (2017), who trained a Structure-to-Vector (S2V) GNN with DQN to solve TSP and Max-Cut. The resulting algorithm, S2V-DQN, generalised to graphs with different size and structure to the training set and achieved excellent performance across a range of problems without the need for manual algorithmic design, demonstrating the value in exploiting underlying graph structure.\nAdvances in optimality. Various works since Dai et al. (2017) have sought to harness GNN embeddings to improve solution quality. Abe et al. (2019) combined a GNN with a Monte Carlo tree search approach to learn a high-quality constructive heuristic. Ultimately, they demonstrated a greater ability to generalise to more graph types than S2V-DQN on Max-Cut, but their method could only scale\nto 100 vertex Erdo\u030bs-R\u00e9nyi and Barabasi-Albert graphs. Li et al. (2018) combined a graph convolution network (GCN) with guided tree search to synthesise a diverse set of solutions and thereby more fully explore the space of possible solutions. However, they used supervised learning and so required labelled data which limits scalability, and they did not consider Max-Cut. Barrett et al. (2020) proposed ECO-DQN, the SOTA RL algorithm for Max-Cut, which reframed the role of the RL agent to looking to improve on any given solution, rather than directly predict the optimal solution. The key insight is that exploration at test time is beneficial, since individual sub-optimal decisions (which are to be expected for NP-hard problems) need not matter so long as the final solution is of high quality. However, ECO-DQN utilises an expensive GNN at each decision step and extends the overall number of decisions taken to be theoretically limitless, thereby restricting its scalability to be even worse than that of S2V-DQN. ECORD remedies this by using an initial GNN embedding followed by a recurrent unit to balance the richness provided by graph networks with fast-action selection.\nAdvances in scalability. Manchanda et al. (2020) furthered the work of Dai et al. (2017) by first training an embedding GCN in a supervised manner, and then training a Q-network with RL to predict per-vertex action values. By using the GCN embeddings to prune nodes unlikely to be in the solution set, their method provided significantly more scalability than S2V-DQN on the the Maximum Vertex Cover problem. However, it was not applicable to problems whose nodes cannot be pruned, which precludes it from solving some of the most fundamental CO problems such as Max-Cut. Drori et al. (2020) took a different approach, proposing a general RL-GNN framework that uses a graph attention network to create a dense embedding of the input graph followed by a recurrent attention mechanism for action selection. They achieved impressive scalability, reaching instance sizes of 1000 vertices on the minimum spanning tree problem and TSP. However, unlike ECORD, the Drori et al. (2020) framework restricts the decoding stage to condition only on the previously selected action, and considers only node ordering problems in a non-exploratory setting.\n3 METHODS\n\n3.1 BACKGROUND\nMax-Cut problem. The Max-Cut problem seeks a binary labelling of all nodes in a graph, such that the number (or cumulative weight) of edges connecting nodes of opposite labels is maximised. Concretely, for a graph, G(V,E), with vertices V and edges E, we wish to find a subset of vertices, S \u2282 V that maximises the \u2018cut-value\u2019, C(S,G) = \u2211 i\u2208S,j\u2208V \\S w(eij), where w(eij) is the weight of an edge eij \u2208 E. Q-learning. We formulate the optimisation as a Markov Decision Process (MDP) defined by a tuple {S,A, T ,R, \u03b3}, where S andA are the state and action spaces, respectively, T : S\u00d7A\u00d7S \u2192 [0, 1] is the transition function, R : S \u2192 R the reward function and \u03b3 \u2208 [0, 1] the discount factor. Q-learning methods seek to directly learn the Q-value function mapping state-action pairs, (s \u2208 S, a \u2208 A), to the expected discounted sum of immediate and future rewards when following a policy \u03c0 : S \u2192 A, Q\u03c0(s, a) = E\u03c0[ \u2211\u221e t\u2032=t+1 \u03b3 t\u2032\u22121r(s(t \u2032))|s(t)=s, a(t)=a]. Conventional DQN (Mnih et al., 2015) optimises Q\u03b8 by minimising the mean-squared-error between the network predictions and a bootstrapped estimate of the Q-value. By definition, an optimal policy maximises the true Q-value of every selected action, therefore, after training, an approximation of an optimal policy is obtained by acting greedily with respect to the learnt value function, \u03c0\u03b8(s) = arg maxa\u2032 Q\u03b8(s, a \u2032).\nMunchausen DQN. Munchausen DQN (M-DQN) (Vieillard et al., 2020) makes two fundamental adjustments to conventional DQN. Firstly, the Q-values are considered to define a stochastic policy with action probabilities given by \u03c0\u03b8(\u00b7|s) = softmax(Q\u03b8(s,\u00b7)\u03c4 ), where \u03c4 is a temperature parameter. Secondly, M-DQN adds the log-probability of the selected action to the reward at each step. All together, the regression target for the Q-function is modified to\nQm\u2212dqn(s (t), a(t), s(t+1)) = r(s(t+1)) + \u03b1\u03c4 ln\u03c0\u03b8(a (t)|s(t))+ \u03b3Ea\u2032\u223c\u03c0(\u00b7|s(t+1))[Q\u03b8(s(t+1), a\u2032)\u2212 \u03c4 ln\u03c0\u03b8(a\u2032|s(t+1))],\n(1)\nwhere \u03b1 scales the additional log-policy contribution to the reward. Note that as \u03b1 \u2192 0 and \u03c4 \u2192 0 we recover the standard DQN regression target.\n3.2 ECORD\nMDP formulation. The state, s(t) \u2261 (G(V,E), S) \u2208 S , at a given time-step corresponds to the graph and a binary labelling of all vertices. An action selects a single vertex and \u2018flips\u2019 its label to the opposite state, adding or removing it from the solution subset. Rewards correspond to the (normalised and clipped) increase in best cut-value, r(s(t)) = max((C(s(t)) \u2212 C(s\u2217)))/|V | , 0), where C(s\u2217) is the the best solution found in the episode thus far. This choice of reward structure motivates the agent to maximise the cut-value at any step within the episode, without discouraging it from searching through unseen solutions. Naturally, the transition function is known and fully deterministic for our mathematically defined optimisation problem.\nConcretely, we represent the state, s(t), with per-vertex observations, o(t)i \u2208R3 for i\u2208V , and a global observation of the current state, o(t)G \u2208R2. For each vertex, we provide: (i) the current label, (ii) the immediate change in cut-value if the vertex is flipped (which we refer to as a \u2018peek\u2019) and (iii) the number of steps since the node was last flipped. Our global observations are the (normalised) difference in cut-value between the current and best observed state, and the maximum peek of any vertex in the current state.\nImportantly, these features are readily available without introducing any significant computational burden. Indeed, even the peeks \u2013 one-step look-aheads for each action \u2013 can be calculated for all vertices once at the beginning of each episode, after which they can be efficiently updated quickly at each step for only the flipped vertex and it\u2019s immediate neighbours (details can be found appendix A.3). Besides this being a cheap operation, this approach is also the most efficient way to track the cut-value over the course of multiple vertex flips, and thus is a necessary calculation to evaluate each environmental state regardless of whether the peeks form part of the observation.\nArchitecture. ECORD splits an optimisation into two stages (see figure 1 for an overview of the entire architecture). (i) A GNN prepares per-vertex embeddings conditioned on only the geometric structure (i.e. weighted adjacency matrix) of the graph itself. (ii) Starting from a random labelling of the vertices, ECORD sequentially flips vertices between subsets, with each action conditioned on the static GNN embeddings, the previous trajectory steps (through the hidden state of an RNN) and simple observations of the current state (described above). Full details of the architecture can be found in appendix A.1, with this section providing a higher level summary.\nTo generate the per-vertex embeddings, we use a gated graph convolution network (Li et al., 2015). The final embeddings after L rounds of message-passing are linearly projected to per-vertex embeddings, xi = Wpx (L) i where i denotes the vertex, that remain fixed for the remainder of the episode.\nTo select an action at time t, we first combine the GNN outputs with the observation to obtain pervertex embeddings of the form v(t)i = [xi,Woo (t) i ], where square brackets denote concatenation. This local information is then used in conjunction with the RNN hidden state, h(t), to predict the Q-value for flipping vertex i. The Q-network itself uses a duelling architecture (Wang et al., 2016), where separate MLPs estimate the value of the state, V (\u00b7), and advantage for each action, A(\u00b7),\nQ(v (t) i , h (t)) = V (h(t)) +A(v (t) i , h (t)) = MLPV(h(t)) + MLPA([v (t) i ,Whh (t)]). (2)\nh(t) is shared across each vertex when calculating the advantage function as our reward function depends on the current best score within the trajectory\u2019s history, a global feature contained in the state-level embedding. However, before concatenating them together, we project the high dimensional (1024 in our case) hidden state to match the lower dimensional (32) per-vertex embeddings.\nFinally, we update the RNN using the embedding of the selected action, v(t)\u2217 , and our global observation after taking it, o(t+1)G ,\nh(t+1) = GRU(h(t),m(t+1)), where m(t+1) = MLPm([v (t) \u2217 , o (t+1) G ]). (3)\nTraining. ECORD is trained using M-DQN. As the Q-values are conditioned upon the internal state of an RNN, see eq. (2), we train using truncated backpropagation through time (BPTT). Concretely, when calculating the Q-values at time t during training, we reset the environment and the internal state of the RNN to their state at time t\u2212 kBPTT and replay the trajectory to time t. In doing so, we only have to backpropagate through the previous kBPTT time steps when minimising the loss.\nThe intuition behind M-DQN\u2019s modifications are (i) the entropy of the policy is jointly optimised with the returns, in the spirit of maximum entropy RL (Haarnoja et al., 2018), and (ii) the agent is rewarded for being more confident about the correct action. The second point is based on the assumption that an optimal policy is deterministic, since it will always take the action with maximum Q-value. One could observe that as ECORD\u2019s action space is large (equal to the number of vertices in the graph, |V | ) the structured exploration of M-DQN\u2019s stochastic policy may allow for more meaningful trajectories than the standard epsilon-greedy approaches used in DQN. Moreover, the underlying postulate of M-DQN, that despite the stochastic exploration policy, the true optimal policy is deterministic, aligns exactly with our problem setting.\n4 EXPERIMENTS\nBaselines. We compare ECORD to the previous two SOTA RL-GNN algorithms for Max-Cut, S2VDQN (Khalil et al., 2017) and ECO-DQN (Barrett et al., 2020). Our implementation of ECORD contains several speed improvements in comparison to the publicly available implementation of ECO-DQN (e.g. parallelised optimisation trajectories, compiled calculations, sparse matrix operations). To ensure the fairest possible comparison, when directly comparing performance independent of speed (section 4.1), we use the public implementation, however when comparing scalability (section 4.2 and 4.3) we use a re-implementation of ECO-DQN within the same codebase as ECORD.\nAdditionally, a simple heuristic, Max-Cut Approx (MCA), that greedily selects actions that maximise the immediate increase in cut value is also considered. Besides from providing surprisingly strong performance, the choice of MCA baselines is motivated by the observation that the one-step look-ahead \u2018peek\u2019 features make learning a greedy policy straightforward. Moreover, whilst MCA terminates once a locally optimal solution is found (i.e. one from which no single action can further increase the cut value), a network learning to only approximate a greedy policy may fortuitously escape these solutions and ultimately obtain better results. To address this concern, we introduce a simple extension of (and significant improvement over) MCA called MCA-soft, where actions are selected by a soft-greedy policy with the temperature tuned to maximise performance on the target dataset (see appendix A.4 for details).\nDatasets. We consider graph datasets for which the optimal (or best known) solutions are publicly available. The dataset published by Barrett et al. (2020) consists of Erdo\u030bs-R\u00e9nyi (Erdo\u030bs & R\u00e9nyi,\n1960) and Barabasi-Albert (Albert & Barab\u00e1si, 2002) graphs (ER and BA, respectively) with edge weights w(eij) \u2208 {0,\u00b11} and up to 500 vertices. Each graph type and size is represented by 150 random instances, with 50 used for model selection and results reported on the remaining 100 at test time. We refer to these distributions as ER40/BA40 to ER500/BA500.\nTo test on larger graphs we use the GSet (Benlic & Hao, 2013), a well-studied dataset containing multiple distribution, from which we focus on random (ER) graphs with binary edge weights and 800 to 10 000 vertices. To aide model selection and parameter tuning, we generate 10 additional graphs for each distribution in the GSet. We refer to these validation sets as ER800 to ER10000.\nMetrics. Our analysis considers the raw performance, wall-clock speed, and memory usage of algorithms. Following prior work, we use the approximation ratio, given by AR(s\u2217) = C(s\u2217)/Copt where Copt is the best known cut value, as a metric of solution quality. All experiments were performed on the same system with an Nvidia GeForce RTX 2080 Ti 11 GB GPU and 80 processors (Intel(R) Xeon(R) Gold 6248 CPU @ 2.50 GHz).\nReproducability. All datasets considered in this work are either available or linked in the supporting code at https://github.com/[MASKED-FOR-BLIND-REVIEW], along with source code and scripts to reproduce the reported results.\n4.1 COMPARISON TO SOTA RL METHODS\nMethods. Our first set of experiments compares ECORD to both heuristic node-flipping, and SOTA RL, baselines. To facilitate a fair and direct comparison, we use exactly the models, datasets, and results published by Barrett et al. (2020) for S2V-DQN and ECO-DQN. All RL algorithms are trained on 40 vertex ER graphs and evaluated on both ER and BA graphs with up to 500 vertices. S2VDQN is deterministic and incrementally constructs the solution set one vertex at a time, therefore each optimisation trajectory consists of |V | sequential actions. MCA-soft, ECO-DQN and ECORD allow any vertex to be \u2018flipped\u2019 at each step, and therefore can in principle run indefinitely on a target graph. The baselines presented are chosen as they consider the same sequential node-flipping paradigm as ECORD, however their exits multiple algorithms for solving Max-Cut that do not fit this paradigm \u2013 notably simulated annealing (SA) (Tiunov et al., 2019; Leleu et al., 2019), semidefinite programming (SDP) (Goemans & Williamson, 1995) and mixed integer programming (CPLEX, ILOG, 2007). We provide additional results using SOTA or commercial algorithms spanning these paradigms in appendix A.5, where ECORD is found to outperform SDP and MIP and either beat or be competitive with SOTA SA methodologies.\nIn practice, we use ECORD and the MCA heuristics with ECO-DQN\u2019s default settings; 50 optimisation trajectories per graph, each starting from a random node labelling, acting greedily with respect to the learnt Q-values, and terminating after 2|V | sequential actions. We note that this disadvantages ECORD as (i) despite learning a stochastic policy, ECORD acts deterministically at test time, and (ii) ECORD\u2019s significant speed and memory advantages over ECO-DQN are not accounted for.\nResults. Despite the disadvantages described above, ECORD either outperforms or essentially matches ECO-DQN on all tests (see table 1), whilst significantly improving over all other baselines\nand generalising to unseen graph topologies and sizes. This seems surprising when we consider that, in contrast to ECO-DQN, ECORD does not directly condition per-node decisions on the state of other nodes. However, ECORD does have access to the optimisation trajectory via the RNN hidden state, suggesting that in exploratory settings, the temporal structure (in addition to the geometric structure of the graph) is highly informative. This experiment was repeated for a model trained instead on BA40 with the results (provided in appendix A.5) being qualitatively the same.\n4.2 COMPUTATIONAL COMPLEXITY AND PRACTICAL SCALING PERFORMANCE\nWhilst ECORD has already been shown to match or surpass SOTA RL baselines when ignoring computational cost, this is not a sufficient metric of algorithmic utility. In this section, we consider the theoretical complexity and practical scaling cost of ECORD.\nTheoretical complexity. The runtime complexity of any optimisation trajectory scales linearly with the number of actions. In CO problems where every vertex must be correctly labelled, such as MaxCut, episode length scales at best linearly with the number of nodes. A typical approach, which encapsulates both S2V-DQN and ECO-DQN, for applying RL to CO on graphs parameterises the policy or value function with a GNN. The precise complexity of a GNN depends on the chosen architecture and graph topology, however, typically the per-layer performance scales linearly with the number of edges in the graph, as this is the number of unique communication channels along which information must be passed. In practice, this results in a (worst case and typical) polynomial scaling of O(|V | 2) per-action and O(|V | 3) per optimisation, which makes even modest sized graphs with the order of hundreds of nodes very computationally expensive in practice.\nIn contrast, ECORD only runs the GNN once, regardless of the graph size or episode length, and then selects actions without any additional message passing between nodes. As a result, per-action computational cost scales linearly as O(|V | ), regardless of the graph topology, and the entire optimisation scales as O(|V | 2). Moreover, action selection in ECORD also has a far smaller memory footprint than using a GNN over the entire graph, and each vertex can be processed in parallel up to the limits of hardware. Therefore, in practice, we typically obtain a constant, O(1), scaling of the per-action computational cost and, as the single graph network pass is typically negligible compared to the long exploratory phase, complete the entire optimisation in O(|V | ). Practical performance. Figure 2 demonstrates the practical performance realised from these theoretical improvements. In 2a, the action time of ECORD and ECO-DQN are compared on graphs with up to 500 k vertices. Whilst ECORD is always faster, both take near constant time for small graphs where batched inference across all nodes is still efficient. However, even when ECO-DQN begins to increase in cost (|V | \u2248500) and eventually fills the entire GPU memory (|V | \u224810 k), ECORD retains a fixed low-cost inference which only appreciably begins to increase for large graphs (|V |>100 k).\nMoreover, the reduced memory footprint of ECORD also allows for more (randomly initialised) optimisation trajectories to be run in parallel. Figure 2b contrasts ECORD and ECO-DQN by running as many parallel trajectories as possible for a single graph with up to |V |=10 k, and plotting the effective step time (action selection plus environmental step) per trajectory. The practical increase in throughput of ECORD compared to ECO-DQN increases with graph size from a modest 1.8\u00d7 with |V |=30 to 100\u00d7 for |V |=3 k and 298.9\u00d7 for |V |=100 k.\n4.3 SCALING TO LARGE GRAPHS\nMethods. ECORD is trained on graphs with binary edge weights, w(eij) \u2208 {0, 1} and |V | = 500. The optimal parameters are selected according to the performance of the greedy policy on the generated ER10000 set. For the computational reasons discussed previously, ECO-DQN was trained on ER200 graphs with binary edge weights, and used ER500 graphs for model selection. The temperature of MCA-soft was tuned independently on each graph, as detailed in appendix A.4.\nStochastic exploration. Despite ECORD using a stochastic soft-greedy policy during training, at test time we previously used a deterministic policy (which, ultimately, still provided near optimal performance on small and intermediate graphs). To investigate the performance of a soft-greedy policy, we evaluated ECORD on the large-graph datasets ER5000, ER7000 and ER10000 \u2013 using 20 trajectories of 4|V | steps for all graphs in the test sets \u2013 across a range of temperatures.\nThe results are summarised in figure 3. The key points are that a non-zero temperature is optimal for larger graphs, and that the optimal temperature decreases with graph size. The interpretation is that even an exploratory agent such as ECORD can still eventually get stuck in closed trajectories when acting deterministically. Therefore, introducing some stochasticity allows ECORD to escape these regions of the solution space and ultimately continue to search for improved configurations. However, this must be balanced with the need for a sufficiently deterministic policy that a good solution is found with high probability. Intuitively, longer sequences of actions are required to reach a good configuration for larger graphs, which aligns with the observed dependence of the optimal temperature with graph size.\nGSet results. We validate ECORD\u2019s performance on large graphs by testing it on all GSet graphs with random binary edges, as summarised in table 2. For each graph we run 20 parallel optimisation trajectories for a maximum of 180 s. For G5000, G7000 and G10000 we use tuned temperatures, \u03c4 , of 3.5e\u22124, 2e\u22124 and 1e\u22124 respectively, and set \u03c4=5e\u22124 for all other graphs. ECORD shows consistently strong performance, matching or beating ECO-DQN and MCA-soft on every instance by a margin that increases with graph size. The speed of ECORD is emphasised by the fact that it obtains approximation ratios of above 0.99 on all graphs with |V | \u2264 2 k in under 10 s. Whilst in principle it is possible to reach the optimal solution in |V | actions, on larger graphs the obtained cut values consistently increase with longer exploration (for reference, ECORD takes >28 k actions in 180 s when |V |=10 k). This clearly demonstrates that ECORD learns to search through the solution space without getting stuck, rather than generating a single mode of solutions.\nA natural final question is whether the slight decrease in performance on the largest graphs is due to an insufficient time budget, or the learnt policy being less suitable for these problems. To test this, ECORD was allowed to continue optimising G70 for 1 h. Whilst the obtained solution improved to have an approximation ratio of 0.978 (obtained in only 233 s), the optimal solution was not found. Ultimately, whilst previous RL-GNN methods were limited by the scalability of GNNs, now we find ourselves limited by the ability of the agent to reason about larger systems \u2013 opening the door to, and defining a challenge for, future research.\nAblations. ECORD\u2019s key components are: (i) the use of a single (GNN) encoding step to embed the problem structure, (ii) rapid decoding steps where per-vertex actions are conditioned on only local observations and an (RNN) leant embedding of the optimisation trajectory, and (iii) an exploratory CO setting where actions can be reversed. Ablations (detailed in appendix A.5) of the GNN and RNN show that both are necessary for strong performance. ECORD\u2019s contribution to the exploratory CO setting is evidencing that a suitably stochastic policy outperforms a deterministic one. To emphasise that this is not simply because of algorithmic improvements in M-DQN compared to DQN, a deterministic-acting ECORD trained with DQN is shown to also not match the performance reported in table 2.\n5 DISCUSSION\nWe present ECORD, a new SOTA RL algorithm for the Max-Cut problem in term of both performance and scalability. ECORD\u2019s demonstrated efficacy on graphs with up 10 k vertices, and highly favourable computational complexity, suggests even larger problems could be tackled. By replacing multiple expensive GNN operations with a single embedding stage and rapid action-selection directed by a recurrent unit, this work highlights the importance of, and a method to achieve, efficient exploration when solving CO problems, all within the broader pursuit of scalable geometric learning.\nAlgorithmic improvements are a possible direction for further research. An adaptive (or learnt) temperature schedule could better trade-off stochastic exploration and deterministic solution improvement. ECORD also runs multiple optimisation trajectories in parallel, and so utilising information from other trajectories to either better inform future decisions or ensure sufficient diversity between them, would be another approach for improving performance.\nWith regards to further scaling improvements, any algorithm that labels vertices sequentially will at best have O(|V |) complexity. An interesting prospect for future work would be to note that ECORD does not rely on observing the current state of the neighbourhood of a vertex to evaluate the action quality \u2013 therefore one could envisage flipping multiple vertices in a single step, with a single centralised, or decentralised multi-agent, system.\nAn alternative direction would be to apply our algorithm to other complex problems and, in principle, ECORD could be applied to any vertex-labelling CO problem defined on a graph. However, its use of \u2018peeks\u2019 (one-step look aheads for each action), does not directly translate to problems where the quality of intermediate actions are not naturally evaluated (e.g. those where an arbitrary node labelling may be invalid such as maximum clique). Considering only valid actions or utilising indirect metrics of solution quality are possible solutions, but this remains a topic for future research.\nA APPENDIX\nA.1 ARCHITECTURE DETAILS\nHere we provide further details on the network architecture described in section 3.2 and figure 1 of the main text. For simplicity, we implicitly drop any bias terms in the below equations.\nGraph Neural Network. The gated graph convolution network of Li et al. (2015) is modified to include layer normalisation, LN(\u00b7), where the 16-dimensional embedding of node i at layer l + 1 is given by,\nx (l+1) i = LN(GRU(x (l) i ,m (l+1) i )), m (l+1) i =\n1 |N(i)| \u2211\nj\u2208N (i) w(eij)Wgx\n(l) j , (4)\nwhereWg \u2208 R16\u00d716. The final embeddings after 4 rounds of message-passing are linearly projected to per-vertex embeddings, xi = Wpx (4) i with Wp \u2208 R16\u00d716, that remain fixed for the remainder of the episode.\nValue network. Recall from the main text that Q-values at time t are predicted as\nQ(v (t) i , h (t)) = V (h(t)) +A(v (t) i , h (t)) = MLPV(h(t)) + MLPA([v (t) i ,Whh (t)]). (5)\nwhere h(t) \u2208 R1024 is the hidden state of the RNN and v(t)i = [xi,Woo (t) i ] \u2208 R32 are the per-node embeddings with xi \u2208 R16, Wo \u2208 R16\u00d73 and Wh \u2208 R32\u00d71014. The value head, MLPV(\u00b7) : R1024 \u2192 R, is a 2-layer network that applies a tanh activation to the input and leaky ReLU to the intermediate activations.\nThe advantage head, MLPA(\u00b7) : R64 \u2192 R, is a 2-layer network that applies layer norm and leaky ReLU to the intermediate activations.\nRecurrent network. The RNN hidden state is 1024-dimensional and initialised to zeros. It is updated using the embedding of the selected action, v(t)\u2217 , and our global observation from the next step,\nh(t+1) = GRU(h(t),m(t+1)), where m(t+1) = LeakyReLU(Wm([v (t) \u2217 , o (t+1) G ])), (6)\nwhere Wm \u2208 R64\u00d734.\nA.2 TRAINING DETAILS\nThe ECORD pseudocode is given in algorithm 1. Note that to update the network at time t we make use of truncated backpropagation through time (BPTT) (Werbos, 1990; Sutskever, 2013) using the previous tBPTT experiences. We perform network updates every fupd time steps, where we update the online network parameters \u03b8 using stochastic gradient descent (SGD) on the M-DQN loss and the target network parameters \u03b8 using a soft update (Lillicrap et al., 2015) with update parameter \u03c4upd. The hyperparameters used are summarised in table 3.\nA.3 EFFICIENT RE-CALCULATION OF CUT VALUE AND PEEKS\nGiven a graph, G(V,E), with edge weights wij \u2261 w(eij) for eij \u2208 E, and a node labelling represented as a binary vector, z \u2208 {0, 1}|V |, the cut value is given by,\nC(z|G) = 1 2 \u2211 ij wij (zi(1\u2212 zj) + (1\u2212 zi)zj) ,\n= 1\n2 \u2211 ij wij(zi + zj \u2212 2zizj). (7)\nIt is straightforward to decompose this into the sum of \u2018local\u2019 cuts\nC(z|G) = 1 2 \u2211 i Ci, Ci = \u2211 j\u2208N (i) wij(zi + zj \u2212 2zizj), (8)\nwhere Ci, the cut value of the sub-graph containing node only i and its neighbours,N (i). Similarly, we can define the total weight of un-cut edges connected to each vertex as\nCi = \u2211\nj\u2208N (i) wij(1\u2212 zi \u2212 zj + 2zizj). (9)\nAlgorithm 1: Training ECORD Initialise experience replay memoryM. for each batch of episodes do\nSample BG graphs G(V,E) from distribution D Calculate per-vertex embeddings using GNN Initialise a random solution set for each graph, S0 \u2282 V for each step t in the episode do\nk\u2032 = min(t\u2212 kBPTT, 0) for each graph Gj in the batch do\n// Flip a vertex.\na(t)\u223c\n{ randomly from V with prob. \u03b5\nsoftmax(Q\u03b8(s (t),\u00b7) \u03c4 ) with prob. 1\u2212 \u03b5\nS(t+1) :=\n{ S(t) \u222a {a(t)}, if a(t) /\u2208 S(t)\nS(t) \\ {a(t)}, if a(t) \u2208 S(t)\n// Add experience to buffer.\nAdd tuple m(t) = (s(t\u2212k \u2032), . . . , s(t+1), a(t\u2212k \u2032), . . . , a(t), r(t), d) toM if t mod fupd == 0 then\n// Sample batch of experiences from buffer.\nM (t) \u2282M // Update online network. Update \u03b8 with one SGD step using BPTT from t down to t\u2212 k\u2032 on Lm\u2212dqn (??) // Update target network.\n\u03b8 \u2190 \u03b8\u03c4upd + \u03b8(1\u2212 \u03c4upd) end\nend end\nend return \u03b8\nThe change in cut value (referred to as the \u2018peek\u2019 feature in the main text) if the label of vertex i is flipped is then then given by\n\u2206Ci = Ci \u2212 Ci, = \u2211\nj\u2208N (i) wij(4zizj \u2212 2(zi + zj) + 1),\n= \u2211\nj\u2208N (i) wij(2zi \u2212 1)(2zj \u2212 1).\n(10)\nCalculating these one-step look-aheads to the change in cut for each action clearly has the same complexity as calculating the cut-value itself (equation (7)). Moreover, they only have to be calculated once at the start of each episode, as when vertex i is flipped from zi to zi, only the \u2018peeks\u2019 of vertrices i and j \u2208 N (i) need to be updated. These updates follow directly from the above and are given by\n\u2206Ci \u2192 \u2212\u2206Ci, \u2206Cj \u2192 \u2206Cj \u2212 wij(2zi \u2212 1)(2zj \u2212 1). (11)\nA.4 MCA-SOFT\nMCA-soft attempts to upper bound the performance simple policies that condition actions based solely on the provided \u2018peeks\u2019 for each action. Denoting the known change in cut value if vertex i was to be flipped as \u2206Ci (see eq. (11)), MCA-soft follows a stochastic policy given by\na(t) \u223c softmax (\n\u2206Ci \u03c4mca\n) . (12)\nTo maximise the performance of MCA-soft, the temperature, \u03c4mca \u2208 R, is independently tuned to maximise performance on every set of graphs considered. In practice, this process is a grid search over \u03c4mca \u2208 {0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3} for results in table 1 and 4, and \u03c4mca \u2208 {0, 0.0001, 0.001, 0.01, 0.1, 1} for results in table 2.\nA.5 EXTENDED RESULTS\nTraining on BA graphs. For completeness, we repeat the experiments shown in the main manuscript but now training on 40-vertex BA graphs and evaluating on both ER and BA graphs with up to 500 vertices, with the results shown in Table 4.\nBaseline comparison. Here we conduct a thorough solver comparison by training and inferring on both ER and BA graphs of up to 500 vertices. We compare ECORD to seven baselines; six taken directly from Barrett et al. (2020) (ECO-DQN, S2V-DQN, MCA, CPLEX, SimCIM, and Leleu et al. (2019); refer to Barrett et al. (2020) for implementation details), and one an extension of MCA (MCA-soft, as described in this manuscript). The results are summarised in Table 5, with the approximation ratios shown taken from averaging the solvers\u2019 performances across 100 BA and ER graphs of up to 500 vertices.\nSemidefinite programming comparison. In addition to heuristics, another important branch of Max-Cut solver research is that of approximation algorithms. Such algorithms can offer a theoretical guarantee on the approximation ratio while still solving a relaxed formulation of the original problem in polynomial time. One such approximation method is the canonical semidefinite programming (SDP) approach of Goemans & Williamson (1995). Goemans & Williamson (1995) first formulate Max-Cut as an SDP by framing the objective as a linear function of a symmetric matrix subject to linear equality constraints (as in a linear programme) but with the additional constraint that the matrix must be positive semidefinite (whereby, for an n \u00d7 n matrix A, \u2200x \u2208 Rn, xtAx \u2265 0). This relaxed Max-Cut SDP formulation can be solved efficiently using algorithms such as a generalised Simplex method (P\u00f3lik & Terlaky, 2010). The insight of Goemans & Williamson (1995) was to then apply a geometric randomised rounding technique to convert the SDP solution into a feasible Max-Cut solution. Crucially, the randomised rounding method gives a guarantee to be within at least 0.87856 times the optimal Max-Cut value.\nTo the best of our knowledge, no open-access Goemans & Williamson (1995) solver exists which can handle Max-Cut problems with negatively weighted edges (Hong, 2008). However, G1-5 of the GSet graphs used in Table 2 of this manuscript have all-positive edge weights, therefore we ran the opensource cvx solver (https://github.com/hermish/cvx-graph-algorithms), which implements Goemans & Williamson (1995), on these 5 problems to obtain approximation ratios of 0.971, 0.970, 0.975, 0.970, and 0.967 in 449, 484, 496, 521, and 599 s respectively. In addition to ECORD outperforming Goemans & Williamson (1995) on these 5 GSet graphs (see Table 2) in both solving time and optimality, we note that ECORD also exceeds the 0.87856 approximation ratio guarantee across all the GSet, ER, and BA graphs examined in our work (see Tables 1, 2, 4, and 5).\nAblations. We provide ablations to further investigate the key components of ECORD, highlighted in the main text as\n1. a GNN to encode the spatial structure of the problem, 2. a rapid decoding conditioned on local per-node observations and an RNN\u2019s internal state\nthat represents the optimisation trajectory, 3. an exploratory CO setting.\nWe ablate the GNN and RNN by using fixed zero-vectors in-place of the static per-node embeddings, xi, and the RNN hidden state, h(t), respectively. As the advantage of exploratory CO has already been demonstrated in prior works (Barrett et al., 2020), we instead ablate the stochastic policies we find provide further improved exploration at test time (see section 4.3 of the main text). Specifically, we train the agent using DQN instead of M-DQN, as this optimises for a deterministic policy.\nFor all ablations, we otherwise use the same procedure as the full ECORD agent from section 4.3 of the main text, including tuning temperature of the agents policy with a grid-search. DQN is, unsurprisingly, found to be best at \u03c4 = 0, as is the agent with the GNN ablated. The agent with the RNN ablated uses \u03c4 of 1e\u22124, 8.5e\u22125 and 5e\u22125 for For G55, G60 and G70, respectively, with \u03c4=5e\u22124 for all other graphs. Results are presented in table 6 with ECORD significantly outperforming all ablations on larger graphs.\nGraph network timing. As stated in the main text, the single pass of the graph neural network is negligible compared to the overall run time of ECORD. To be concrete, on the largest graph for which results are reported (G70 with |V | = 10 k nodes), our embedding stage takes (1.96\u00b1 0.09) ms, compared to the tens of seconds of exploratory decoding.", "ref": {"BIBREF0": {"ref_id": "b0", "title": "Solving NP-Hard Problems on Graphs by Reinforcement Learning without Domain Knowledge", "authors": [{"first": "Kenshin", "middle": [], "last": "Abe", "suffix": ""}, {"first": "Zijian", "middle": [], "last": "Xu", "suffix": ""}, {"first": "Issei", "middle": [], "last": "Sato", "suffix": ""}, {"first": "Masashi", "middle": [], "last": "Sugiyama", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1905.11623"]}, "num": null, "urls": [], "raw_text": "Kenshin Abe, Zijian Xu, Issei Sato, and Masashi Sugiyama. Solving NP-Hard Problems on Graphs by Reinforcement Learning without Domain Knowledge. arXiv:1905.11623, 2019.", "links": null}, "BIBREF1": {"ref_id": "b1", "title": "Statistical mechanics of complex networks", "authors": [{"first": "R\u00e9ka", "middle": [], "last": "Albert", "suffix": ""}, {"first": "Albert-L\u00e1szl\u00f3", "middle": [], "last": "Barab\u00e1si", "suffix": ""}], "year": 2002, "venue": "Reviews of Modern Physics", "volume": "74", "issue": "1", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "R\u00e9ka Albert and Albert-L\u00e1szl\u00f3 Barab\u00e1si. Statistical mechanics of complex networks. Reviews of Modern Physics, 74(1):47, 2002.", "links": null}, "BIBREF2": {"ref_id": "b2", "title": "On the computational complexity of Ising spin glass models", "authors": [{"first": "Francisco", "middle": [], "last": "Barahona", "suffix": ""}], "year": 1982, "venue": "Journal of Physics A: Mathematical and General", "volume": "15", "issue": "10", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Francisco Barahona. On the computational complexity of Ising spin glass models. Journal of Physics A: Mathematical and General, 15(10):3241, 1982.", "links": null}, "BIBREF3": {"ref_id": "b3", "title": "Exploratory combinatorial optimization with reinforcement learning", "authors": [{"first": "Thomas", "middle": [], "last": "Barrett", "suffix": ""}, {"first": "William", "middle": [], "last": "Clements", "suffix": ""}, {"first": "Jakob", "middle": [], "last": "Foerster", "suffix": ""}, {"first": "Alex", "middle": [], "last": "Lvovsky", "suffix": ""}], "year": 2020, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "34", "issue": "", "pages": "3243--3250", "other_ids": {}, "num": null, "urls": [], "raw_text": "Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3243-3250, 2020.", "links": null}, "BIBREF5": {"ref_id": "b5", "title": "Machine learning for combinatorial optimization: A methodological tour d'horizon", "authors": [{"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}, {"first": "Andrea", "middle": [], "last": "Lodi", "suffix": ""}, {"first": "Antoine", "middle": [], "last": "Prouvost", "suffix": ""}], "year": 2021, "venue": "European Journal of Operational Research", "volume": "290", "issue": "2", "pages": "405--421", "other_ids": {"ISSN": ["0377-2217"]}, "num": null, "urls": [], "raw_text": "Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial opti- mization: A methodological tour d'horizon. European Journal of Operational Research, 290(2): 405-421, 2021. ISSN 0377-2217.", "links": null}, "BIBREF6": {"ref_id": "b6", "title": "Breakout Local Search for the Max-Cut problem", "authors": [{"first": "Una", "middle": [], "last": "Benlic", "suffix": ""}, {"first": "Jin-Kao", "middle": [], "last": "Hao", "suffix": ""}], "year": 2013, "venue": "Engineering Applications of Artificial Intelligence", "volume": "26", "issue": "3", "pages": "1162--1173", "other_ids": {"ISSN": ["0952-1976"]}, "num": null, "urls": [], "raw_text": "Una Benlic and Jin-Kao Hao. Breakout Local Search for the Max-Cut problem. Engineering Appli- cations of Artificial Intelligence, 26(3):1162 -1173, 2013. ISSN 0952-1976.", "links": null}, "BIBREF7": {"ref_id": "b7", "title": "The transformer network for the traveling salesman problem", "authors": [{"first": "Xavier", "middle": [], "last": "Bresson", "suffix": ""}, {"first": "Thomas", "middle": [], "last": "Laurent", "suffix": ""}], "year": 2021, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2103.03012"]}, "num": null, "urls": [], "raw_text": "Xavier Bresson and Thomas Laurent. The transformer network for the traveling salesman problem. arXiv:2103.03012, 2021.", "links": null}, "BIBREF8": {"ref_id": "b8", "title": "Learning to perform local rewriting for combinatorial optimization", "authors": [{"first": "Xinyun", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Yuandong", "middle": [], "last": "Tian", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1810.00337"]}, "num": null, "urls": [], "raw_text": "Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimiza- tion. arXiv:1810.00337, 2019.", "links": null}, "BIBREF9": {"ref_id": "b9", "title": "11.0 user's manual. ILOG SA", "authors": [{"first": "Ilog", "middle": [], "last": "Cplex", "suffix": ""}], "year": 2007, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "CPLEX, ILOG. 11.0 user's manual. ILOG SA, Gentilly, France, pp. 32, 2007.", "links": null}, "BIBREF10": {"ref_id": "b10", "title": "Learning combinatorial optimization algorithms over graphs", "authors": [{"first": "Hanjun", "middle": [], "last": "Dai", "suffix": ""}, {"first": "Elias", "middle": ["B"], "last": "Khalil", "suffix": ""}, {"first": "Yuyu", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Bistra", "middle": [], "last": "Dilkina", "suffix": ""}, {"first": "Le", "middle": [], "last": "Song", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17", "volume": "", "issue": "", "pages": "6351--6361", "other_ids": {}, "num": null, "urls": [], "raw_text": "Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pp. 6351-6361, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.", "links": null}, "BIBREF11": {"ref_id": "b11", "title": "Efficient combinatorial optimization using quantum annealing", "authors": [{"first": "N", "middle": [], "last": "Hristo", "suffix": ""}, {"first": "Guillaume", "middle": [], "last": "Djidjev", "suffix": ""}, {"first": "Georg", "middle": [], "last": "Chapuis", "suffix": ""}, {"first": "Guillaume", "middle": [], "last": "Hahn", "suffix": ""}, {"first": "", "middle": [], "last": "Rizk", "suffix": ""}], "year": 2018, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1801.08653"]}, "num": null, "urls": [], "raw_text": "Hristo N Djidjev, Guillaume Chapuis, Georg Hahn, and Guillaume Rizk. Efficient combinatorial optimization using quantum annealing. arXiv preprint arXiv:1801.08653, 2018.", "links": null}, "BIBREF12": {"ref_id": "b12", "title": "Learning to solve combinatorial optimization problems on real-world graphs in linear time", "authors": [{"first": "Iddo", "middle": [], "last": "Drori", "suffix": ""}, {"first": "Anant", "middle": [], "last": "Kharkar", "suffix": ""}, {"first": "William", "middle": ["R"], "last": "Sickinger", "suffix": ""}, {"first": "Brandon", "middle": [], "last": "Kates", "suffix": ""}, {"first": "Qiang", "middle": [], "last": "Ma", "suffix": ""}, {"first": "Suwen", "middle": [], "last": "Ge", "suffix": ""}, {"first": "Eden", "middle": [], "last": "Dolev", "suffix": ""}, {"first": "Brenda", "middle": [], "last": "Dietrich", "suffix": ""}, {"first": "David", "middle": ["P"], "last": "Williamson", "suffix": ""}, {"first": "Madeleine", "middle": [], "last": "Udell", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2006.03750"]}, "num": null, "urls": [], "raw_text": "Iddo Drori, Anant Kharkar, William R. Sickinger, Brandon Kates, Qiang Ma, Suwen Ge, Eden Dolev, Brenda Dietrich, David P. Williamson, and Madeleine Udell. Learning to solve combina- torial optimization problems on real-world graphs in linear time. arXiv:2006.03750, 2020.", "links": null}, "BIBREF13": {"ref_id": "b13", "title": "On the Evolution of Random Graphs", "authors": [{"first": "Paul", "middle": [], "last": "Erd\u0151s", "suffix": ""}, {"first": "Alfr\u00e9d", "middle": [], "last": "R\u00e9nyi", "suffix": ""}], "year": 1960, "venue": "Publ. Math. Inst. Hung. Acad. Sci", "volume": "5", "issue": "1", "pages": "17--60", "other_ids": {}, "num": null, "urls": [], "raw_text": "Paul Erd\u0151s and Alfr\u00e9d R\u00e9nyi. On the Evolution of Random Graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17-60, 1960.", "links": null}, "BIBREF14": {"ref_id": "b14", "title": "Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems using Semidefinite Programming", "authors": [{"first": "X", "middle": [], "last": "Michel", "suffix": ""}, {"first": "David P", "middle": [], "last": "Goemans", "suffix": ""}, {"first": "", "middle": [], "last": "Williamson", "suffix": ""}], "year": 1995, "venue": "Journal of the ACM (JACM)", "volume": "42", "issue": "6", "pages": "1115--1145", "other_ids": {}, "num": null, "urls": [], "raw_text": "Michel X Goemans and David P Williamson. Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems using Semidefinite Programming. Journal of the ACM (JACM), 42(6):1115-1145, 1995.", "links": null}, "BIBREF15": {"ref_id": "b15", "title": "Combinatorial optimization by simulating adiabatic bifurcations in nonlinear hamiltonian systems", "authors": [{"first": "Hayato", "middle": [], "last": "Goto", "suffix": ""}, {"first": "Kosuke", "middle": [], "last": "Tatsumura", "suffix": ""}, {"first": "Alexander R", "middle": [], "last": "Dixon", "suffix": ""}], "year": 2019, "venue": "Science advances", "volume": "5", "issue": "4", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Hayato Goto, Kosuke Tatsumura, and Alexander R Dixon. Combinatorial optimization by simulat- ing adiabatic bifurcations in nonlinear hamiltonian systems. Science advances, 5(4):eaav2372, 2019.", "links": null}, "BIBREF16": {"ref_id": "b16", "title": "A Deep Learning Algorithm for the Max-Cut Problem Based on Pointer Network Structure with Supervised Learning and Reinforcement Learning Strategies", "authors": [{"first": "Shenshen", "middle": [], "last": "Gu", "suffix": ""}, {"first": "Yue", "middle": [], "last": "Yang", "suffix": ""}], "year": 2020, "venue": "", "volume": "8", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Shenshen Gu and Yue Yang. A Deep Learning Algorithm for the Max-Cut Problem Based on Pointer Network Structure with Supervised Learning and Reinforcement Learning Strategies. Mathemat- ics, 8(2), 2020. ISSN 2227-7390.", "links": null}, "BIBREF17": {"ref_id": "b17", "title": "Hybrid models for learning to branch", "authors": [{"first": "Prateek", "middle": [], "last": "Gupta", "suffix": ""}, {"first": "Maxime", "middle": [], "last": "Gasse", "suffix": ""}, {"first": "Elias", "middle": ["B"], "last": "Khalil", "suffix": ""}, {"first": "M", "middle": ["Pawan"], "last": "Kumar", "suffix": ""}, {"first": "Andrea", "middle": [], "last": "Lodi", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2006.15212"]}, "num": null, "urls": [], "raw_text": "Prateek Gupta, Maxime Gasse, Elias B. Khalil, M. Pawan Kumar, Andrea Lodi, and Yoshua Bengio. Hybrid models for learning to branch. arXiv:2006.15212, 2020.", "links": null}, "BIBREF18": {"ref_id": "b18", "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "authors": [{"first": "Tuomas", "middle": [], "last": "Haarnoja", "suffix": ""}, {"first": "Aurick", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Pieter", "middle": [], "last": "Abbeel", "suffix": ""}, {"first": "Sergey", "middle": [], "last": "Levine", "suffix": ""}], "year": 2018, "venue": "International Conference on Machine Learning", "volume": "", "issue": "", "pages": "1861--1870", "other_ids": {}, "num": null, "urls": [], "raw_text": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Confer- ence on Machine Learning, pp. 1861-1870. PMLR, 2018.", "links": null}, "BIBREF19": {"ref_id": "b19", "title": "Combinatorial optimization: Comparison of heuristic algorithms in travelling salesman problem", "authors": [{"first": "A", "middle": [], "last": "Halim", "suffix": ""}, {"first": "I", "middle": [], "last": "Ismail", "suffix": ""}], "year": 2019, "venue": "Archives of Computational Methods in Engineering", "volume": "26", "issue": "", "pages": "367--380", "other_ids": {}, "num": null, "urls": [], "raw_text": "A. Halim and I. Ismail. Combinatorial optimization: Comparison of heuristic algorithms in travel- ling salesman problem. Archives of Computational Methods in Engineering, 26:367-380, 2019.", "links": null}, "BIBREF20": {"ref_id": "b20", "title": "Inapproximability of the Max-cut Problem with Negative Weights", "authors": [{"first": "Sung-Pil", "middle": [], "last": "Hong", "suffix": ""}], "year": 2008, "venue": "International Journal of Management Science", "volume": "14", "issue": "1", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Sung-Pil Hong. Inapproximability of the Max-cut Problem with Negative Weights. International Journal of Management Science, 14(1), 2008.", "links": null}, "BIBREF21": {"ref_id": "b21", "title": "Neural\" Computation of Decisions in Optimization Problems", "authors": [{"first": "J", "middle": [], "last": "John", "suffix": ""}, {"first": "David", "middle": ["W"], "last": "Hopfield", "suffix": ""}], "year": 1985, "venue": "Biological Cybernetics", "volume": "52", "issue": "3", "pages": "141--152", "other_ids": {}, "num": null, "urls": [], "raw_text": "John J Hopfield and David W \"Neural\" Computation of Decisions in Optimization Problems. Biological Cybernetics, 52(3):141-152, 1985.", "links": null}, "BIBREF22": {"ref_id": "b22", "title": "An efficient graph convolutional network technique for the travelling salesman problem", "authors": [{"first": "K", "middle": [], "last": "Chaitanya", "suffix": ""}, {"first": "Thomas", "middle": [], "last": "Joshi", "suffix": ""}, {"first": "Xavier", "middle": [], "last": "Laurent", "suffix": ""}, {"first": "", "middle": [], "last": "Bresson", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1906.01227"]}, "num": null, "urls": [], "raw_text": "Chaitanya K. Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network technique for the travelling salesman problem. arXiv:1906.01227, 2019.", "links": null}, "BIBREF23": {"ref_id": "b23", "title": "Reducibility Among Combinatorial Problems", "authors": [{"first": "M", "middle": [], "last": "Richard", "suffix": ""}, {"first": "", "middle": [], "last": "Karp", "suffix": ""}], "year": 1972, "venue": "Complexity of Computer Computations", "volume": "", "issue": "", "pages": "85--103", "other_ids": {}, "num": null, "urls": [], "raw_text": "Richard M Karp. Reducibility Among Combinatorial Problems. In Complexity of Computer Com- putations, pp. 85-103. Springer, 1972.", "links": null}, "BIBREF24": {"ref_id": "b24", "title": "Learning Combinatorial Optimization Algorithms over Graphs", "authors": [{"first": "Elias", "middle": [], "last": "Khalil", "suffix": ""}, {"first": "Hanjun", "middle": [], "last": "Dai", "suffix": ""}, {"first": "Yuyu", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Bistra", "middle": [], "last": "Dilkina", "suffix": ""}, {"first": "Le", "middle": [], "last": "Song", "suffix": ""}], "year": 2017, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "6348--6358", "other_ids": {}, "num": null, "urls": [], "raw_text": "Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning Combinatorial Op- timization Algorithms over Graphs. In Advances in Neural Information Processing Systems, pp. 6348-6358, 2017.", "links": null}, "BIBREF25": {"ref_id": "b25", "title": "Destabilization of local minima in analog spin systems by correction of amplitude heterogeneity", "authors": [{"first": "Timoth\u00e9e", "middle": [], "last": "Leleu", "suffix": ""}, {"first": "Yoshihisa", "middle": [], "last": "Yamamoto", "suffix": ""}, {"first": "Peter", "middle": ["L"], "last": "Mcmahon", "suffix": ""}, {"first": "Kazuyuki", "middle": [], "last": "Aihara", "suffix": ""}], "year": 2019, "venue": "Phys. Rev. Lett", "volume": "122", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Timoth\u00e9e Leleu, Yoshihisa Yamamoto, Peter L. McMahon, and Kazuyuki Aihara. Destabilization of local minima in analog spin systems by correction of amplitude heterogeneity. Phys. Rev. Lett., 122:040607, 2019.", "links": null}, "BIBREF26": {"ref_id": "b26", "title": "Gated graph sequence neural networks", "authors": [{"first": "Yujia", "middle": [], "last": "Li", "suffix": ""}, {"first": "Daniel", "middle": [], "last": "Tarlow", "suffix": ""}, {"first": "Marc", "middle": [], "last": "Brockschmidt", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Zemel", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1511.05493"]}, "num": null, "urls": [], "raw_text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv:1511.05493, 2015.", "links": null}, "BIBREF27": {"ref_id": "b27", "title": "Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search", "authors": [{"first": "Zhuwen", "middle": [], "last": "Li", "suffix": ""}, {"first": "Qifeng", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Vladlen", "middle": [], "last": "Koltun", "suffix": ""}], "year": 2018, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "539--548", "other_ids": {}, "num": null, "urls": [], "raw_text": "Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial Optimization with Graph Convolu- tional Networks and Guided Tree Search. In Advances in Neural Information Processing Systems, pp. 539-548, 2018.", "links": null}, "BIBREF28": {"ref_id": "b28", "title": "Continuous control with deep reinforcement learning", "authors": [{"first": "P", "middle": [], "last": "Timothy", "suffix": ""}, {"first": "Jonathan", "middle": ["J"], "last": "Lillicrap", "suffix": ""}, {"first": "Alexander", "middle": [], "last": "Hunt", "suffix": ""}, {"first": "Nicolas", "middle": [], "last": "Pritzel", "suffix": ""}, {"first": "Tom", "middle": [], "last": "Heess", "suffix": ""}, {"first": "Yuval", "middle": [], "last": "Erez", "suffix": ""}, {"first": "David", "middle": [], "last": "Tassa", "suffix": ""}, {"first": "Daan", "middle": [], "last": "Silver", "suffix": ""}, {"first": "", "middle": [], "last": "Wierstra", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1509.02971"]}, "num": null, "urls": [], "raw_text": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015.", "links": null}, "BIBREF29": {"ref_id": "b29", "title": "Learning heuristics over large graphs via deep reinforcement learning", "authors": [{"first": "Sahil", "middle": [], "last": "Manchanda", "suffix": ""}, {"first": "Akash", "middle": [], "last": "Mittal", "suffix": ""}, {"first": "Anuj", "middle": [], "last": "Dhawan", "suffix": ""}, {"first": "Sourav", "middle": [], "last": "Medya", "suffix": ""}, {"first": "Sayan", "middle": [], "last": "Ranu", "suffix": ""}, {"first": "Ambuj", "middle": [], "last": "Singh", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1903.03332"]}, "num": null, "urls": [], "raw_text": "Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, and Ambuj Singh. Learning heuristics over large graphs via deep reinforcement learning. arXiv:1903.03332, 2020.", "links": null}, "BIBREF30": {"ref_id": "b30", "title": "Reinforcement learning for combinatorial optimization: A survey", "authors": [{"first": "Nina", "middle": [], "last": "Mazyavkina", "suffix": ""}, {"first": "Sergey", "middle": [], "last": "Sviridov", "suffix": ""}, {"first": "Sergei", "middle": [], "last": "Ivanov", "suffix": ""}, {"first": "Evgeny", "middle": [], "last": "Burnaev", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2003.03600"]}, "num": null, "urls": [], "raw_text": "Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. arXiv:2003.03600, 2020.", "links": null}, "BIBREF31": {"ref_id": "b31", "title": "Human-level control through deep reinforcement learning", "authors": [{"first": "Volodymyr", "middle": [], "last": "Mnih", "suffix": ""}, {"first": "Koray", "middle": [], "last": "Kavukcuoglu", "suffix": ""}, {"first": "David", "middle": [], "last": "Silver", "suffix": ""}, {"first": "Andrei", "middle": ["A"], "last": "Rusu", "suffix": ""}, {"first": "Joel", "middle": [], "last": "Veness", "suffix": ""}, {"first": "G", "middle": [], "last": "Marc", "suffix": ""}, {"first": "Alex", "middle": [], "last": "Bellemare", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Graves", "suffix": ""}, {"first": "Andreas", "middle": ["K"], "last": "Riedmiller", "suffix": ""}, {"first": "Georg", "middle": [], "last": "Fidjeland", "suffix": ""}, {"first": "", "middle": [], "last": "Ostrovski", "suffix": ""}], "year": 2015, "venue": "Nature", "volume": "518", "issue": "7540", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.", "links": null}, "BIBREF32": {"ref_id": "b32", "title": "Asynchronous methods for deep reinforcement learning", "authors": [{"first": "Volodymyr", "middle": [], "last": "Mnih", "suffix": ""}, {"first": "Adria", "middle": ["Puigdomenech"], "last": "Badia", "suffix": ""}, {"first": "Mehdi", "middle": [], "last": "Mirza", "suffix": ""}, {"first": "Alex", "middle": [], "last": "Graves", "suffix": ""}, {"first": "Timothy", "middle": [], "last": "Lillicrap", "suffix": ""}, {"first": "Tim", "middle": [], "last": "Harley", "suffix": ""}, {"first": "David", "middle": [], "last": "Silver", "suffix": ""}, {"first": "Koray", "middle": [], "last": "Kavukcuoglu", "suffix": ""}], "year": 2016, "venue": "International Conference on Machine Learning", "volume": "", "issue": "", "pages": "1928--1937", "other_ids": {}, "num": null, "urls": [], "raw_text": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928-1937. PMLR, 2016.", "links": null}, "BIBREF33": {"ref_id": "b33", "title": "Finding low-energy conformations of lattice protein models by quantum annealing", "authors": [{"first": "Alejandro", "middle": [], "last": "Perdomo-Ortiz", "suffix": ""}, {"first": "Neil", "middle": [], "last": "Dickson", "suffix": ""}, {"first": "Marshall", "middle": [], "last": "Drew-Brook", "suffix": ""}, {"first": "Geordie", "middle": [], "last": "Rose", "suffix": ""}, {"first": "Al\u00e1n", "middle": [], "last": "Aspuru-Guzik", "suffix": ""}], "year": 2012, "venue": "Scientific Reports", "volume": "2", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Alejandro Perdomo-Ortiz, Neil Dickson, Marshall Drew-Brook, Geordie Rose, and Al\u00e1n Aspuru- Guzik. Finding low-energy conformations of lattice protein models by quantum annealing. Sci- entific Reports, 2:571, 2012.", "links": null}, "BIBREF34": {"ref_id": "b34", "title": "Interior Point Methods for Nonlinear Optimization", "authors": [{"first": "Imre", "middle": [], "last": "P\u00f3lik", "suffix": ""}, {"first": "Tam\u00e1s", "middle": [], "last": "Terlaky", "suffix": ""}], "year": null, "venue": "", "volume": "", "issue": "", "pages": "215--276", "other_ids": {}, "num": null, "urls": [], "raw_text": "Imre P\u00f3lik and Tam\u00e1s Terlaky. Interior Point Methods for Nonlinear Optimization, pp. 215-276.", "links": null}, "BIBREF36": {"ref_id": "b36", "title": "Training Recurrent Neural Networks", "authors": [{"first": "Ilya", "middle": [], "last": "Sutskever", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Ilya Sutskever. Training Recurrent Neural Networks. PhD thesis, CAN, 2013. AAINS22066.", "links": null}, "BIBREF37": {"ref_id": "b37", "title": "Annealing by simulating the coherent Ising machine", "authors": [{"first": "S", "middle": [], "last": "Egor", "suffix": ""}, {"first": "Alexander", "middle": ["E"], "last": "Tiunov", "suffix": ""}, {"first": "A", "middle": ["I"], "last": "Ulanov", "suffix": ""}, {"first": "", "middle": [], "last": "Lvovsky", "suffix": ""}], "year": 2019, "venue": "Optics Express", "volume": "27", "issue": "7", "pages": "10288--10295", "other_ids": {}, "num": null, "urls": [], "raw_text": "Egor S Tiunov, Alexander E Ulanov, and AI Lvovsky. Annealing by simulating the coherent Ising machine. Optics Express, 27(7):10288-10295, 2019.", "links": null}, "BIBREF38": {"ref_id": "b38", "title": "Learning combinatorial optimization on graphs: A survey with applications to networking", "authors": [{"first": "Natalia", "middle": [], "last": "Vesselinova", "suffix": ""}, {"first": "Rebecca", "middle": [], "last": "Steinert", "suffix": ""}, {"first": "Daniel", "middle": ["F"], "last": "Perez-Ramirez", "suffix": ""}, {"first": "Magnus", "middle": [], "last": "Boman", "suffix": ""}], "year": 2020, "venue": "IEEE Access", "volume": "8", "issue": "", "pages": "120388--120416", "other_ids": {}, "num": null, "urls": [], "raw_text": "Natalia Vesselinova, Rebecca Steinert, Daniel F. Perez-Ramirez, and Magnus Boman. Learning combinatorial optimization on graphs: A survey with applications to networking. IEEE Access, 8:120388-120416, 2020.", "links": null}, "BIBREF39": {"ref_id": "b39", "title": "Munchausen reinforcement learning", "authors": [{"first": "Nino", "middle": [], "last": "Vieillard", "suffix": ""}, {"first": "Olivier", "middle": [], "last": "Pietquin", "suffix": ""}, {"first": "Matthieu", "middle": [], "last": "Geist", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2007.14430"]}, "num": null, "urls": [], "raw_text": "Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Munchausen reinforcement learning. arXiv:2007.14430, 2020.", "links": null}, "BIBREF40": {"ref_id": "b40", "title": "Pointer Networks", "authors": [{"first": "Oriol", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "Meire", "middle": [], "last": "Fortunato", "suffix": ""}, {"first": "Navdeep", "middle": [], "last": "Jaitly", "suffix": ""}], "year": 2015, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issue": "", "pages": "2692--2700", "other_ids": {}, "num": null, "urls": [], "raw_text": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer Networks. In Advances in Neural Information Processing Systems, pp. 2692-2700, 2015.", "links": null}, "BIBREF41": {"ref_id": "b41", "title": "Dueling network architectures for deep reinforcement learning", "authors": [{"first": "Ziyu", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Tom", "middle": [], "last": "Schaul", "suffix": ""}, {"first": "Matteo", "middle": [], "last": "Hessel", "suffix": ""}, {"first": "Hado", "middle": [], "last": "Hasselt", "suffix": ""}, {"first": "Marc", "middle": [], "last": "Lanctot", "suffix": ""}, {"first": "Nando", "middle": [], "last": "Freitas", "suffix": ""}], "year": 2016, "venue": "International Conference on Machine Learning", "volume": "", "issue": "", "pages": "1995--2003", "other_ids": {}, "num": null, "urls": [], "raw_text": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pp. 1995-2003. PMLR, 2016.", "links": null}, "BIBREF42": {"ref_id": "b42", "title": "Backpropagation through time: what it does and how to do it", "authors": [{"first": "P", "middle": ["J"], "last": "Werbos", "suffix": ""}], "year": 1990, "venue": "Proceedings of the IEEE", "volume": "78", "issue": "10", "pages": "1550--1560", "other_ids": {"DOI": ["10.1109/5.58337"]}, "num": null, "urls": [], "raw_text": "P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550-1560, 1990. doi: 10.1109/5.58337.", "links": null}, "BIBREF43": {"ref_id": "b43", "title": "The Design of Approximation Algorithms", "authors": [{"first": "P", "middle": [], "last": "David", "suffix": ""}, {"first": "David", "middle": ["B"], "last": "Williamson", "suffix": ""}, {"first": "", "middle": [], "last": "Shmoys", "suffix": ""}], "year": 2011, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "David P. Williamson and David B. Shmoys. The Design of Approximation Algorithms. Cambridge University Press, 2011. ISBN 978-0-521-19527-0.", "links": null}, "BIBREF44": {"ref_id": "b44", "title": "Coherent Ising machines-optical neural networks operating at the quantum limit", "authors": [{"first": "Yoshihisa", "middle": [], "last": "Yamamoto", "suffix": ""}, {"first": "Kazuyuki", "middle": [], "last": "Aihara", "suffix": ""}, {"first": "Timothee", "middle": [], "last": "Leleu", "suffix": ""}, {"first": "Ken-Ichi", "middle": [], "last": "Kawarabayashi", "suffix": ""}, {"first": "Satoshi", "middle": [], "last": "Kako", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Fejer", "suffix": ""}, {"first": "Kyo", "middle": [], "last": "Inoue", "suffix": ""}, {"first": "Hiroki", "middle": [], "last": "Takesue", "suffix": ""}], "year": 2017, "venue": "npj Quantum Information", "volume": "3", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Yoshihisa Yamamoto, Kazuyuki Aihara, Timothee Leleu, Ken-ichi Kawarabayashi, Satoshi Kako, Martin Fejer, Kyo Inoue, and Hiroki Takesue. Coherent Ising machines-optical neural networks operating at the quantum limit. npj Quantum Information, 3(1):49, 2017.", "links": null}, "BIBREF45": {"ref_id": "b45", "title": "Logistics supply chain management based on multiconstrained combinatorial optimization and extended simulated annealing", "authors": [{"first": "Wang", "middle": [], "last": "Yanling", "suffix": ""}, {"first": "Yang", "middle": [], "last": "Deli", "suffix": ""}, {"first": "Yan", "middle": [], "last": "Guoqing", "suffix": ""}], "year": 2010, "venue": "2010 International Conference on Logistics Systems and Intelligent Management (ICLSIM)", "volume": "1", "issue": "", "pages": "188--192", "other_ids": {}, "num": null, "urls": [], "raw_text": "Wang Yanling, Yang Deli, and Yan Guoqing. Logistics supply chain management based on multi- constrained combinatorial optimization and extended simulated annealing. In 2010 International Conference on Logistics Systems and Intelligent Management (ICLSIM), volume 1, pp. 188-192, 2010.", "links": null}, "BIBREF46": {"ref_id": "b46", "title": "A reinforcement learning approach to job-shop scheduling", "authors": [{"first": "Wei", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Thomas", "middle": ["G"], "last": "Dietterich", "suffix": ""}], "year": 1995, "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence", "volume": "2", "issue": "", "pages": "1114--1120", "other_ids": {}, "num": null, "urls": [], "raw_text": "Wei Zhang and Thomas G. Dietterich. A reinforcement learning approach to job-shop scheduling. In Proceedings of the 14th International Joint Conference on Artificial Intelligence -Volume 2, IJCAI'95, pp. 1114-1120, San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1558603638.", "links": null}}, "peer_review": [{"review_text": "To deal with the efficiency and scalability issues in current RL approaches for Max-Cut problem, this paper proposes a new method called ECORD, based on Munchausen DQN\u2019s exploration with GNN pre-computed embedding of each vertex. Running Time Complexity is analyzed. Compared with current baselines, time complexity is reduced from O(|V|^3) to O(|V|^2). Experiments, are conducted on published dataset, show its superiority in efficiency and performance.Although this paper has excellent motivation for improving the efficiency of current GNN based RL for CO problems, I have some concerns about the approach and experiments. (1) Although Maximum Cut problem is one of classic CO problem, it cannot represent all CO problem. For this reason, the title is much larger than the content of this paper, unless the authors can show the generalization of their method for other CO problem. (2) Restricting the GNN in the preprocessing step for embedding is a good idea for reducing the complexity in running time. According to experimental results, the paper states the performance is better than ECO-DQN. Then, it is very natural to raise the question: whether is the contribution of GNN very little for the performance? To answer this question, the authors should give some evidence of comparison with GNN-free graph embedding methods. (3) In the experiments, the paper only compared with ECO-DQN and two simple heuristics, which cannot support to claim that the proposed method is better than current SOTA of approximate algorithms. Some of them are efficient for large-scaled problem too. (4) If a max-cut problem has multiple best solutions, I wonder whether the convergence of proposed approach can be guarantee with limited learning steps? Are there some test cases for this point in experiments? (5) The terminating condition will affect the convergence and performance of the given approach largely. The authors used 2|V| for small-scaled instances and 4|V| for large-scaled ones. Could the authors give some theoretical analysis for the necessary number of learning steps. If could not, the algorithm is hard to be used in practice. (6) The largest problem targeted in experiments is about just 10000 nodes. In the era of big data, the scale for testing the scalability is not enough. Many real problem graphs have millions of nodes.Although this paper has excellent motivation for improving the efficiency of current GNN based RL for CO problems, I have some concerns about the approach and experiments.", "Correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "5: marginally below the acceptance threshold", "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."}, {"review_text": "This paper proposes a scalable deep neural network (DNN)-based solver for the maximum cut problem. The main idea is to use (1) a single graph neural network (GNN) pass to acquire node embeddings and (2) use sequential decoding (without using GNN) to acquire the maximum cut solution. The experiments demonstrate good performance of the proposed algorithm, in particular for speeding up the generation of solutions.Pros:\nThe proposed approach is solid. I think this algorithm can easily extended for solving combinatorial optimization problems other than the maximum cut problem.\nThe empirical comparison is thorough with respect to the considered baselines.\nCons:\nI am mainly concerned with the empirical comparison.\nThe maximum cut problem is a representative application of semi definite programming solvers. See [1] for an example. I think this baseline is necessary for practitioners to see whether if the proposed algorithm has any useful-ness in real-world applications.\nGset (considered in this paper) is a popular benchmark for the maximum cut problem and there are several non-DNN-based results that can easily compared with the proposed solvers. For example, the authors can compare with [2].\nTo my knowledge, [3] is a relatively new GNN-based combinatorial solver that can solve maxcut. Their algorithm can be applied to maximum cut with a minor changes (they solve graph partitioning problem). This comparison is important since [2] is also a GNN-based solver with running time linear with respect to the problem size. They use a single GNN-pass and decode the solution.\nIt seems that this paper is very similar to [4] in a sense that they both use a GNN to extract node embeddings for the given problems and applies DQN-based training for a node-wise predictor to generate solutions. I think the authors can provide a more thorough comparison between two methods. My current understanding is that [4] pretrains a GNN using supervised learning and ECORD use end-to-end truncated backpropagation.\nMinor comments:\nAblation studies on using DQN over M-DQN would be useful to see whether if the empirical improvement comes from the proposed scheme, or simply using a better RL algorithm.\nI hope the authors could provide more reference and related works on ways to solve the maximum cut problem without using DNN.\n[1] Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming, 1995 [2] Breakout Local Search for the Max-Cut Problem, 2012 [3] Erdos Goes Neural: an Unsupervised Learning \u02dd Framework for Combinatorial Optimization on Graphs, 2020 [4] Learning heuristics over large graphs via deep reinforcement learning, 2019I think this paper is based on a solid idea and shows good empirical performance. However, at the current state, it is hard to access the significance of this paper since baseline algorithms used in real-world is missing. Furthermore, I would like to see description on the proposed algorithm's difference to existing work of Manchanda et al., (2019) to further support novelty of the proposed method.", "Correctness": "4: All of the claims and statements are well-supported and correct.", "Technical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Empirical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Recommendation": "6: marginally above the acceptance threshold", "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."}, {"review_text": "This paper presents a RL-based algorithm for solving the graph-based combinational optimization problem of Max-Cut. The key idea is to formulate the problem as an MDP, where the state is the embedings of all the vertices and the actions are the vertices to be flipped. The idea is implemented with a graph neural network to learn the embeddings and a recurrent encoding to encode the state representations. The network is trained with M-DQN. The proposed algorithm, named ECORD, is shown to be comparable to the previous state-of-the-art ECO-DQN and be significantly faster than ECO-DQN.Postiive points:\nCombinational problem is very general and could have many applications.\nECORD is shown to be able to generalize to larger graphs.\nECORD is shown to be more efficient than the previous methods\nThe neural architecture is well designed with both GNN to learn vertex embeddings and value network with GRU to learn state embedding.\nThe paper is very well written.\nNegative points:\nIt is unclear to me how general the Max-Cut problem is. The Max-Cut is based on graph and only considers two labels. It is unclear whether ECORD can be applied to problems without graph structure or with more than two labels. In particular, when there are more than two labels, the actions space can be ill-defined since flipping will not make sense with more than two labels.\nIt is unclear whether Max-Cut is hard enough to use RL. In table 1, even greedy heuristics can have very strong performance. In terms of efficiency, the greedy heuristics should be even more efficient than ECORD.\nIt is unclear how ECORD improves the efficiency of ECO-DQN. The methodology section only introduces how ECORD is trained but never compares it with ECO-DQN.Overall, it remains unclear what combinational problems ECORD can address. The improvement over ECO-DQN or heuristics is not significant. It is unclear how ECORD improves ECO-DQN in terms of efficiency, which is the major contribution of this work.", "Correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.", "Recommendation": "5: marginally below the acceptance threshold", "Confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."}, {"review_text": "This paper describes an RL based method for learning to solve the weighted maximum cut problem. Specifically, it proposes to process an input (fully connected?) graph using a gated graph recurrent network, and use the resulting embeddings as input for a computationally cheap RNN based policy that starts from a random cut (partition of nodes into two subsets) and subsequently 'flips' one of the nodes to move it to the other subset/other side of the cut. This policy ('encoder' + RNN) is trained using a variant of DQN to maximize the result obtained after 2*|V| flips, where V is the number of nodes in the graph. The policy makes use of a so called 'peek', a one-step lookahead of the result of each flip action on the current objective. At test time, the policy constructs multiple (deterministic) trajectories in parallel starting from different (random) initial solutions, where (I assume) the best overall result is returned. In experiments, the resulting algorithm, which is called ECORD, is shown to perform favorably when compared against ECO-DQN (Barrett et al. 2020), which, from skimming the paper, is very similar in strategy but uses a less efficient architecture and a different variant of DQN.Strengths\nWell written paper, it was nice and easy to read\nThe paper presents a sensible approach and good motivation, where it specifically designs an architecture to minimize the computational burden of the algorithm\nThe paper addresses scaling, which is an important challenge in neural combinatorial optimization\nThe paper applies reasonable baselines such as the heuristic strategy (MCA/MCA-soft)\nThe paper takes care to conduct fair comparison (e.g. own implementation with optimizations also for ECO-DQN baseline)\nWeakness\nThe paper only considers maximum cut, while title suggests general combinatorial optimization. As multiple aspects (flipping, peek feature) are specific to max-cut it is unclear how the proposed method can generalize to other problems. I think 'Learning to Solve Maximum-Cut' would be a more suitable title for this work.\nThe approach, presented as novel RL algorithm, is very similar to ECO-DQN, but the differences are not explained, so it is unclear what are contributions and what is 'reused' from ECO-DQN. From skimming the ECO-DQN paper it seems the major differences are the architecture (GNN 'encoder' + cheap RNN policy rather than expensive GNN for every step) and algorithm (different DQN variant).\nThe term SOTA is confusing, maybe even misleading. I don't think RL/GNN is clearly SOTA for maximum cut in general (see e.g. the Leleu et al. baseline in the ECO-DQN paper), which is suggested in the abstract and title of section 4.1. To avoid confusion, I think the authors should use 'best RL-based method' and keep the term SOTA for the best exact/heuristic/RL solver in general. When claiming SOTA, I think the paper should also refer to relevant max-cut literature.\nThe experiments do not support the strong claims: generalization is only compared against ECO-DQN and not against heuristic baselines. Also, some general baselines are missing, especially LeLeu et al. from the ECO-DQN paper.\nThe theoretical novelty is limited as the 'learning to explore' idea and max-cut setup is from ECO-DQN (Barrett et al.). The idea of encoder + (rapid) decoder architecture for combinatorial problems is similar to Drori et al. (and some earlier works on neural combinatorial optimization).\nOverall, I like the paper but I think it is too incremental and too narrowly focused on max-cut to be published at ICLR.\nDetailed comments/questions/suggestions:\n'to equal or surpasses' -> 'to equal or surpass'\nDo MCA-soft and MCA also use 50 trajectories? (I assume so but it is not mentioned)\nFig 2b displays linear step time, but you also run 2|V| steps so more steps for larger graphs so overall time is quadratic? Also, does blue correspond to ECORD? (labels in Fig 2a?)\nIf you did a reproduction of ECO-DQN (which is a great thing!) I think it would be helpful to list both original and reproduced results for clarity in Table 1 (unless they are very similar)\n'Heurisitcs' typo Table 1\nIt may be good to mention other solution strategies are possible as well, e.g. rounding a relaxed solution (https://www.cl.cam.ac.uk/teaching/1617/AdvAlgo/maxcut.pdf)\nHow does MCA and MCA-soft perform in Table 2?\nWhat defines exactly ECORD when claiming/suggesting that 'in principle it could be applied to any CO problem defined on a graph' (in discussion)?\nFor the GNN 'encoder', is the input graph fully connected? If so, how does the 'encoder' scale?\nGiven the focus on max-cut only, I think some more related work on max-cut (non-RL) could be included.Overall, I like the paper, which is quite nice to read and presents a sensible and effective approach for solving the maximum cut problem using RL. Especially, the authors specifically addressed scaling to larger instances by making the architecture more efficient, which is an important and challenging topic.\nUnfortunately, I still think the paper should be rejected given that\nit is too much focused on max-cut and unclear how this framework can solve general combinatorial problems, as the title suggests\nit is quite incremental to Barrett et al. (2020) AND not clear about the differences and\nsome important baselines are missing.", "Correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.", "Technical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Empirical Novelty And Significance": "2: The contributions are only marginally significant or novel.", "Recommendation": "3: reject, not good enough", "Confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."}], "Decision": "Reject", "novelty_sentence": ["The approach, presented as novel RL algorithm, is very similar to ECO-DQN, but the differences are not explained, so it is unclear what are contributions and what is 'reused' from ECO-DQN.", "The theoretical novelty is limited as the 'learning to explore' idea and max-cut setup is from ECO-DQN (Barrett et al.).", "Overall, I like the paper but I think it is too incremental and too narrowly focused on max-cut to be published at ICLR.", "Unfortunately, I still think the paper should be rejected given that\nit is too much focused on max-cut and unclear how this framework can solve general combinatorial problems, as the title suggests\nit is quite incremental to Barrett et al."], "novelty_score": 2}, {"id": "GrvigKxc13E", "paper_content": "Title\nGradient play in stochastic games: stationary points, convergence, and sample complexity\nAbstract\nWe study the performance of the gradient play algorithm for stochastic games (SGs), where each agent tries to maximize its own total discounted reward by making decisions independently based on current state information which is shared between agents. Policies are directly parameterized by the probability of choosing a certain action at a given state. We show that Nash equilibria (NEs) and first-order stationary policies are equivalent in this setting, and give a local convergence rate around strict NEs. Further, for a subclass of SGs called Markov potential games (which includes the cooperative setting with identical rewards among agents as an important special case), we design a sample-based reinforcement learning algorithm and give a non-asymptotic global convergence rate analysis for both exact gradient play and our sample-based learning algorithm. Our result shows that the number of iterations to reach an -NE scales linearly, instead of exponentially, with the number of agents. Local geometry and local stability are also considered, where we prove that strict NEs are local maxima of the total potential function and fully-mixed NEs are saddle points.\n1 INTRODUCTION\nMulti-agent systems find applications in a wide range of societal systems, e.g. electric grid, traffic networks, smart building and smart cities etc. Given the complexity of these systems, multi-agent reinforcement learning (MARL) has gained increasing attention in recent years (Daneshfar & Bevrani, 2010; Shalev-Shwartz et al., 2016; Vidhate & Kulkarni, 2017; Xu et al., 2020). Among MARL algorithms, policy gradient-type methods are highly popular because of their flexibility and capability to incorporate structured state and action spaces. However, while many recent works (Zhang et al., 2018; Chen et al., 2018; Wai et al., 2018; Li et al., 2019; Qu et al., 2020) have studied the sample complexity of multi-agent policy gradient algorithms, due to a lack of understanding of the optimization landscape in these multi-agent learning problems, most works can only show convergence to a first-order stationary point. Deeper understanding of the quality of these stationary points is missing even in the simple identical-reward multi-agent RL setting.\nIn this paper, we examine this problem from a game-theoretic perspective. We model the multi-agent system as a stochastic game (SG) where agents can have different reward functions, and study the dynamical behavior of first-order (gradient-based) learning methods. The study of SGs dates back to as early as the 1950s by Shapley (1953) with a series of followup works on developing NE-seeking algorithms, especially in the RL setting (e.g. (Littman, 1994; Bowling & Veloso, 2000; Shoham et al., 2003; Bus\u0327oniu et al., 2010; Lanctot et al., 2017; Zhang et al., 2019a) and citations within). While well-known classical algorithms for solving SGs are mostly value-based, such as Nash-Q learning (Hu & Wellman, 2003), Hyper-Q learning (Tesauro, 2003), and WoLF-PHC (Bowling & Veloso, 2001), gradient-based algorithms have also started to gain popularity in recent years due to their advantages as mentioned earlier (e.g. (Abdallah & Lesser, 2008; Zhang & Lesser, 2010; Foerster et al., 2017)). In this work, we aim to gain a deeper understanding of the structure and quality of first-order stationary points for these gradient-based methods, with a particular focus on answering the following questions: 1) How do the first-order stationary points relate to the NEs of the underlying game?, 2) Do gradient-based algorithms guarantee convergence to a NE?, 3) What is the stability of the individual NEs?, and 4) How should agents learn using local samples from the environment?.\nThese questions have already been widely discussed in other settings, e.g., one-shot (stateless) finiteaction games (Shapley, 1964; Crawford, 1985; Jordan, 1993; Krishna & Sjo\u0308stro\u0308m, 1998; Shamma &\nArslan, 2005; Kohlberg & Mertens, 1986; Van Damme, 1991), one-shot continuous games (Mazumdar et al., 2020), zero-sum linear quadratic (LQ) games Zhang et al. (2019b), etc. There are both negative and positive results depending on the settings. For one-shot continuous games, (Mazumdar et al., 2020) proved a negative result suggesting that gradient flow has stationary points (even local maxima) that are not necessarily NEs. Conversely, Zhang et al. (2019b) designed projected nested-gradient methods that provably converge to NEs in zero-sum LQ games. However, much less is known in the tabular setting of SGs with finite state-action spaces.\nContributions. In our paper, we consider the gradient play algorithm for the infinite time-discounted reward SGs where an agent\u2019s individual policy is directly parameterized by the probability of choosing an action from the agent\u2019s own action space at a given state. We focus on the tabular setting where state and action spaces are finite. Through generalizing the gradient domination property in (Agarwal et al., 2020) to the multi-agent setting, we first establish the equivalence of first-order stationary policies and Nash equilibria. We then show that strict NEs are locally asymptotically stable under gradient play and provide a local convergence rate analysis.\nAdditionally, we study the global convergence for a special class of SGs called Markov potential games (MPGs) (Gonza\u0301lez-Sa\u0301nchez & Herna\u0301ndez-Lerma, 2013; Macua et al., 2018; Leonardos et al., 2021), which includes identical reward multi-agent RL (Tan, 1993; Claus & Boutilier, 1998; Panait & Luke, 2005) as an important special case. In this setting, we first show that exact gradient play can find an -Nash equilibrium within O ( |S| \u2211 i |Ai| 2 ) steps. Then, we design a sample-based gradient\nplay algorithm and show that it can find an -Nash equilibrium with high probability in a fullydecentralized manner using O\u0303 ( n 6 poly ( 1 1\u2212\u03b3 , |S|,maxi |Ai| ))\nsamples, where |S|, |Ai| denote the size of the state space and action space of agent i respectively. The convergence rate shows that the number of iterations to reach an -NE scales linearly with the number of agents. In the sample-based learning, agents only need to observe the state, their own actions, and their own rewards. The key enabler of the learning is the existence of an underlying averaged MDP for each agent when other agents\u2019 policies are fixed. Our learning method can be viewed as a model-based policy evaluation method with respect to agents\u2019 averaged MDPs. This averaged MDP concept could be applied to design many other MARL algorithms, especially policy-evaluation-based methods. We also study the local geometry around some special classes of equilibrium points, showing that strict NEs are local maxima of the total potential function and that fully mixed NEs are saddle points. Lastly, all the algorithms studied in this paper have been numerical tested and results are provided in Appendix A.\nComparison to other MARL algorithms: For MPGs with continuous state and action spaces, there are studies about learning either the open-loop (Gonza\u0301lez-Sa\u0301nchez & Herna\u0301ndez-Lerma, 2013; Zazo et al., 2016) or the closed-loop (Macua et al., 2018) NEs for MPGs. These works generally assume full model information and solve the problem via optimal control. There are two recent arXiv preprints (Mguni, 2020; Leonardos et al., 2021; Mguni et al., 2021) studying MPGs that are similar to our MPG setting. In particular, Leonardos et al. (2021) also studies gradient play for MPG. Both of our papers share similar results on MPGs but the sample-based methods are designed from different perspectives.1 In addition, our papers studies general SGs besides MPG. Moreover, our concept of \u201caveraged\u201d MDPs could also serve as a useful tool for the design and analysis of other MARL algorithms. Beyond these MPG works, decentralized Q-learning introduced in Arslan & Yu\u0308ksel (2016) might be the closest to the setting considered in this paper. They consider the identical interest case and only show asymptotic convergence to the set of NEs. There are other recent works that also study learning for general-sum or zero-sum stochastic games. However the settings they consider are different from our setting, for example, Daskalakis et al. (2021) considers convergence to NE for two player zero-sum games, while Song et al. (2021) considers convergence to correlated equilibrium for finite time horizon general-sum games. On the other hand, Zhang et al. (2018); Li et al. (2019); Qu et al. (2019) consider slightly different MARL settings, where agents collaboratively maximize the summation of agents\u2019 reward with either full or partial state observation. They also require communication between neighboring agents for a better global coordination.\n1Leonardos et al. (2021) considers Monte Carlo, model-free gradient estimation. The sample complexity is derived under the condition that the estimation is unbiased, which is difficult to hold in general. Interestingly, both sample complexities are O( 1\n6 ). It is an interesting question to study whether such dependence is fundamental\nor not. We also remark that Leonardos et al. (2021) and this work are done in parallel.\n2 PROBLEM SETTING AND PRELIMINARIES\nWe consider a stochastic game (SG)M = (N,S, A = A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 An, P, r = (r1, . . . , rn), \u03b3, \u03c1) with n agents (Shapley, 1953) which is specified by an agent set N = {1, 2, . . . , n}, a finite state space S, a finite action space Ai for each agent i \u2208 N , a transition model P where P (s\u2032|s, a) = P (s\u2032|s, a1, . . . , an) is the probability of transitioning into state s\u2032 upon taking action a := (a1, . . . , an) in state s where ai \u2208 Ai is action of agent i, agent i\u2019s reward function ri : S \u00d7A \u2192 [0, 1], a discount factor \u03b3 \u2208 [0, 1), and an initial state distribution \u03c1 over S. A stochastic policy \u03c0 : S \u2192 \u2206(A) (where \u2206(A) is the probability simplex over A) specifies a strategy in which agents choose their actions jointly based on the current state in a stochastic fashion, i.e. Pr(at|st) = \u03c0(at|st). A distributed stochastic policy is a special subclass of stochastic policies, with \u03c0 = \u03c01\u00d7 . . .\u00d7\u03c0n, where \u03c0i : S \u2192 \u2206(Ai). For distributed stochastic policies, each agent takes its action based on the current state s independently of other agents\u2019 choices of actions, i.e.:\nPr(at|st) = \u03c0(at|st) = n\u220f i=1 \u03c0i(ai,t|st), at = (a1,t, . . . , an,t).\nFor notational simplicity, we define: \u03c0I(aI |s) := \u220f i\u2208I \u03c0i(ai|s), where I \u2286 N is an index set. Further, we use the notation \u2212i to denote the index set N\\{i}. We consider direct distributed policy parameterization, where agent i\u2019s policy is parameterized by \u03b8i:\n\u03c0i,\u03b8i(ai|s) = \u03b8i,(s,ai), i = 1, 2, . . . , n. (1)\nFor notational simplicity, we abbreviate \u03c0i,\u03b8i(ai|s) as \u03c0\u03b8i(ai|s), and \u03b8i,(s,ai) as \u03b8s,ai . Here \u03b8i \u2208 \u2206(Ai)|S|, i.e. \u03b8i is subject to the constraints \u03b8s,ai \u2265 0 and \u2211 ai\u2208Ai \u03b8s,ai = 1 for all s \u2208 S.\nThe global joint policy is given by: \u03c0\u03b8(a|s) = \u220fn i=1 \u03c0\u03b8i(ai|s) = \u220fn i=1 \u03b8s,ai . We use Xi := \u2206(Ai)|S|,X := X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xn to denote the feasible region of \u03b8i and \u03b8. Agent i\u2019s value function V \u03b8i : S \u2192 R, i \u2208 N is defined as the discounted sum of future rewards starting at state s via executing \u03c0\u03b8, i.e.\nV \u03b8i (s) := E [ \u221e\u2211 t=0 \u03b3tri(st, at) \u2223\u2223 \u03c0\u03b8, s0 = s] ,\nwhere the expectation is with respect to the random trajectory \u03c4 = (st, at, ri,t)\u221et=0 where at \u223c \u03c0\u03b8(\u00b7|st), st+1 = P (\u00b7|st, at). We denote agent i\u2019s total reward starting from initial state s0 \u223c \u03c1 as:\nJi(\u03b8) = Ji(\u03b81, . . . , \u03b8n) := Es0\u223c\u03c1V \u03b8i (s0).\nIn the game setting, Nash equilibrium is often used to characterize the performance of agents\u2019 policies.\nDefinition 1. (Nash equilibrium) A policy \u03b8\u2217 = (\u03b8\u22171 , . . . , \u03b8\u2217n) is called a Nash equilibrium (NE) if\nJi(\u03b8 \u2217 i , \u03b8 \u2217 \u2212i) \u2265 Ji(\u03b8\u2032i, \u03b8\u2217\u2212i), \u2200\u03b8\u2032i \u2208 Xi, i \u2208 N\nThe equilibrium is called a strict NE if the inequality holds strictly for all \u03b8\u2032i \u2208 Xi and i \u2208 N . The equilibrium is called a pure NE if \u03b8\u2217 corresponds to a deterministic policy. The equilibrium is called a mixed NE if it is not pure. Further, the equilibrium is called a fully mixed NE if every entry of \u03b8\u2217 is strictly positive, i.e.: \u03b8\u2217s,ai > 0, \u2200 ai \u2208 Ai, \u2200 s \u2208 S, i \u2208 N\nWe define the discounted state visitation distribution d\u03b8 of a policy \u03c0\u03b8 given an initial state distribution \u03c1 as:\nd\u03b8(s) := Es0\u223c\u03c1(1\u2212 \u03b3) \u221e\u2211 t=0 \u03b3tPr\u03b8(st = s|s0), (2)\nwhere Pr\u03b8(st = s|s0) is the state visitation probability that st = s when executing \u03c0\u03b8 starting at state s0. Throughout the paper, we make the following assumption on the SGs we study.\nAssumption 1. The stochastic gameM satisfies: d\u03b8(s) > 0, \u2200s \u2208 S, \u2200\u03b8 \u2208 X .\nAssumption 1 requires that every state is visited with positive probability, which is a standard assumption for convergence proofs in the RL literature (e.g. (Agarwal et al., 2020; Mei et al., 2020)).\nSimilar to centralized RL, we define agent i\u2019sQ-functionQ\u03b8i : S\u00d7A \u2192 R and its advantage function A\u03b8i : S \u00d7A \u2192 R as:\nQ\u03b8i (s, a) := E [ \u221e\u2211 t=0 \u03b3tri(st, at) \u2223\u2223 \u03c0\u03b8, s0 = s, a0 = a] , A\u03b8i (s, a) := Q\u03b8i (s, a)\u2212 V \u03b8i (s).\n\u2018Averaged\u2019 Markov decision process (MDP): We further define agent i\u2019s \u2018averaged\u2019 Q-function Q\u03b8i : S \u00d7Ai \u2192 R and \u2018averaged\u2019 advantage-function A\u03b8i : S \u00d7Ai \u2192 R as:\nQ\u03b8i (s, ai) := \u2211 a\u2212i \u03c0\u03b8\u2212i(a\u2212i|s)Q\u03b8i (s, ai, a\u2212i), A\u03b8i (s, ai) := \u2211 a\u2212i \u03c0\u03b8\u2212i(a\u2212i|s)A\u03b8i (s, ai, a\u2212i). (3)\nSimilarly, we define agent i\u2019s \u2018averaged\u2019 transition probability distribution P \u03b8i : S \u00d7 S \u00d7Ai \u2192 R, and \u2018averaged\u2019 reward r\u03b8i : S \u00d7Ai \u2192 R as:\nP \u03b8i (s \u2032|s, ai) := \u2211 a\u2212i \u03c0\u03b8\u2212i(a\u2212i|s)P (s\u2032|s, ai, a\u2212i), r\u03b8i (s, ai) := \u2211 a\u2212i \u03c0\u03b8\u2212i(a\u2212i|s)ri(s, ai, a\u2212i)\nFrom its definition, the averaged Q-function satisfies the following Bellman equation: Lemma 1. Q\u03b8i satisfies: Q\u03b8i (s, ai) = r\u03b8i (s, ai) + \u03b3 \u2211 s\u2032,a\u2032i \u03c0\u03b8i(a \u2032 i|s\u2032)P \u03b8i (s \u2032|s, ai)Q\u03b8i (s \u2032, a\u2032i) (4)\nLemma 1 suggests that the averaged Q-function Q\u03b8i is indeed the Q-function for the MDP defined on action space Ai, with r\u03b8i , P \u03b8i as its stage reward and transition probability respectively. We define this MDP as the \u2018averaged\u2019 MDP of agent i, i.e., M\u03b8i = (S,Ai, P \u03b8i , r\u03b8i , \u03b3, \u03c1). The notion of an \u2018averaged\u2019 MDP will serve as an important intuition when designing the sample-based algorithm. Note that the \u2018averaged\u2019 MDP is only well-defined when the policies of the other agents \u03b8\u2212i are kept fixed. When this is indeed the case, agent i can be treated as an independent learner with respect to its own \u2018averaged\u2019 MDP. Thus, various classical policy evaluation RL algorithms can then be applied.\n3 EXACT GRADIENT PLAY FOR GENERAL STOCHASTIC GAMES\nUnder direct distributed parameterization, the gradient play algorithm is given by:\nExact Gradient Play: \u03b8(t+1)i = ProjXi(\u03b8 (t) i + \u03b7\u2207\u03b8iJi(\u03b8 (t) i )), \u03b7 > 0. (5)\nGradient play can be viewed as a \u2018better response\u2019 strategy, where agents update their own parameters by gradient ascent with respect to their own rewards. A first-order stationary point is defined as such:\nDefinition 2. (First-order stationary policy) A policy \u03b8\u2217 = (\u03b8\u22171 , . . . , \u03b8\u2217n) is called a first-order stationary policy if (\u03b8\u2032i \u2212 \u03b8\u2217i )>\u2207\u03b8iJi(\u03b8\u2217) \u2264 0, \u2200\u03b8\u2032i \u2208 Xi, i \u2208 N .\nIt is not hard to verify that \u03b8\u2217 is a first-order stationary policy if and only if it is a fixed point under gradient play (Equation (5)). Comparing Definition 1 (of NE) and Definition 2, we know that NEs are first-order stationary policies, but not necessarily vice versa. For each agent i, first-order stationarity does not imply that \u03b8\u2217i is optimal among all possible \u03b8i given \u03b8 \u2217 \u2212i. However, interestingly, we will show that NEs are equivalent to first-order stationary policies due to a gradient domination property that we will show later. Before that, we first calculate the explicit form of the gradient\u2207\u03b8iJi. Policy gradient theorem (Sutton et al., 1999) gives an efficient formula for the gradient:\n\u2207\u03b8Es0\u223c\u03c1V \u03b8i (s0) = 1\n1\u2212 \u03b3 Es\u223cd\u03b8Ea\u223c\u03c0\u03b8(\u00b7|s)[\u2207\u03b8 log \u03c0\u03b8(a|s)Q \u03b8 i (s, a)], i \u2208 N. (6)\nApplying Equation (6), the gradient\u2207\u03b8iJi can be written explicitly as follows: Lemma 2. (Proof in Appendix D) For direct distributed parameterization (Equation (1)),\n\u2202Ji(\u03b8) \u2202\u03b8s,ai = 1 1\u2212 \u03b3 d\u03b8(s)Q\u03b8i (s, ai) (7)\nGradient domination and the equivalence between NE and first-order stationary policy. Lemma 4.1 in Agarwal et al. (2020) established gradient domination for centralized tabular MDP under direct parameterization. We can show that a similar property still holds for stochastic games.\nLemma 3. (Gradient domination, proof in Appendix E.) For direct distributed parameterization (Equation (1)), we have that for any \u03b8 = (\u03b81, . . . , \u03b8n) \u2208 X :\nJi(\u03b8 \u2032 i, \u03b8\u2212i)\u2212 Ji(\u03b8i, \u03b8\u2212i) \u2264 \u2225\u2225\u2225\u2225d\u03b8\u2032d\u03b8 \u2225\u2225\u2225\u2225 \u221e max \u03b8i\u2208Xi (\u03b8i \u2212 \u03b8i)>\u2207\u03b8iJi(\u03b8), \u2200\u03b8\u2032i \u2208 Xi, i \u2208 N (8)\nwhere \u2225\u2225\u2225d\u03b8\u2032d\u03b8 \u2225\u2225\u2225\u221e := maxs d\u03b8\u2032 (s)d\u03b8(s) , and \u03b8\u2032 = (\u03b8\u2032i, \u03b8\u2212i).\nFor the single-agent case (n = 1), Equation (8) is consistent with the result in Agarwal et al. (2020), i.e.: J(\u03b8\u2032) \u2212 J(\u03b8) \u2264 \u2225\u2225\u2225d\u03b8\u2032d\u03b8 \u2225\u2225\u2225\u221emax\u03b8\u2208X (\u03b8 \u2212 \u03b8)>\u2207J(\u03b8). However, when there are multiple agents, the condition is much weaker because the inequality requires \u03b8\u2212i to be fixed. When n = 1, gradient domination rules out the existence of stationary points that are not global optima. For the multi-agent case, the property can no longer guarantee the equivalence between first-order stationarity and global optimality; instead, it links the stationary points with NEs as shown in the next theorem whose proof is in Appendix E.\nTheorem 1. Under Assumption 1, first-order stationary policies and NEs are equivalent. Local convergence for strict NEs Although the equivalence of NEs and stationary points under gradient play has been established, it is in fact difficult to show that gradient play converges to these stationary points. Even in the simpler static (stateless) game setup, gradient play might fail to converge (Shapley, 1964; Crawford, 1985; Jordan, 1993; Krishna & Sjo\u0308stro\u0308m, 1998). One major difficulty is that the vector field {\u2207\u03b8iJi(\u03b8)}ni=1 is not a conservative vector field. Accordingly, its dynamics may display complicated behavior. Thus, as a preliminary study, instead of looking at global convergence, we focus on the local convergence and restrict our study to a special subset of NEs - the strict NEs. We begin by giving the following characterization of strict NEs:\nLemma 4. Given a stochastic gameM, any strict NE \u03b8\u2217 is pure, meaning that for each i and s, there exist one a\u2217i (s) such that \u03b8 \u2217 s,ai = 1{ai = a \u2217 i (s)}. Additionally,\ni) a\u2217i (s) = arg max ai A\u03b8 \u2217 i (s, ai), ii) A \u03b8\u2217 i (s, a \u2217 i (s)) = 0; iii) A \u03b8\u2217 i (s, ai) < 0, \u2200 ai 6= a \u2217 i (s) (9)\nBased on this lemma, we define the following for studying the local convergence of a strict NE \u03b8\u2217:\n\u2206\u03b8 \u2217\ni (s) := min ai 6=a\u2217i (s)\n\u2223\u2223\u2223A\u03b8\u2217i (s, ai)\u2223\u2223\u2223 , \u2206\u03b8\u2217 := min i min s 1 1\u2212 \u03b3 d\u03b8\u2217(s)\u2206 \u03b8\u2217 i (s) > 0. (10)\nTheorem 2. (Local finite time convergence around strict NE) Define the metric of policy parameters as: D(\u03b8||\u03b8\u2032) := max1\u2264i\u2264n maxs\u2208S \u2016\u03b8i,s \u2212 \u03b8\u2032i,s\u20161, where \u2016 \u00b7 \u20161 denote the `1- norm. Suppose \u03b8\u2217 is a strict Nash equilibrium, then for any \u03b8(0) such that D(\u03b8(0)||\u03b8\u2217)\u2264 \u2206 \u03b8\u2217 (1\u2212\u03b3)3\n8n|S|( \u2211n i=1 |Ai|) , running\ngradient play (Equation (5)) will guarantee D(\u03b8(t+1)||\u03b8\u2217) \u2264 max { D(\u03b8(t)||\u03b8\u2217)\u2212 \u03b7\u2206 \u03b8\u2217 2 , 0 } , which means that gradient play is going to converge within d 2D(\u03b8 (0)||\u03b8\u2217)\n\u03b7\u2206\u03b8\u2217 e steps.\nProofs of Lemma 4 and Theorem 2 are provided in Appendix F. The convergence only requires a finite number of steps and the stepsize \u03b7 can be chosen arbitrarily large so that exact convergence can happen in even just one step. However, the caveat is that we need to assume that the initial policy is sufficiently close to \u03b8\u2217. For numerical stability considerations, one should pick reasonable stepsizes to run the algorithm to accommodate random initializations. Theorem 2 also shows that the radius of region of attraction for strict NEs is at least \u2206 \u03b8\u2217 (1\u2212\u03b3)3\n8n|S|( \u2211n i=1 |Ai|) , and thus \u03b8\u2217 with a larger \u2206\u03b8 \u2217 , i.e., a\nlarger value gap between the optimal action and other actions, will have a larger region of attraction. We would like to further remark that Theorem 2 only focuses on the local convergence property, the way to interpret the theorem is that, if there exists a strict NE, then it is locally asymptotic stable under gradient play. However, it does not claim to solve the global existence or convergence of the strict NEs.\n4 GRADIENT PLAY FOR MARKOV POTENTIAL GAMES\nWe have discussed that the main problem for the global convergence of gradient play for general SGs is that the vector field {\u2207\u03b8iJi(\u03b8)}ni=1 is not conservative. Thus, in this section, we restrict our analysis to a special subclass where the vector field is conservative, which in turn enjoys global convergence. This subclass is generally referred to as a Markov potential game (MPG) in the literature.\nDefinition 3. (Markov potential game (Macua et al., 2018)) A stochastic gameM is called a Markov potential game if there exists a potential function \u03c6 : S \u00d7A1\u00d7 \u00b7 \u00b7 \u00b7 \u00d7An\u2192R such that for any agent i and any pair of policy parameters (\u03b8\u2032i, \u03b8\u2212i), (\u03b8i, \u03b8\u2212i) :\nE [ \u221e\u2211 t=0 \u03b3tri(st, at) \u2223\u2223\u03c0 = (\u03b8\u2032i, \u03b8\u2212i), s0 = s ] \u2212 E [ \u221e\u2211 t=0 \u03b3tri(st, at) \u2223\u2223\u03c0 = (\u03b8i, \u03b8\u2212i), s0 = s]\n=E [ \u221e\u2211 t=0 \u03b3t\u03c6(st, at) \u2223\u2223\u03c0 = (\u03b8\u2032i, \u03b8\u2212i), s0 = s ] \u2212 E [ \u221e\u2211 t=0 \u03b3t\u03c6(st, at) \u2223\u2223\u03c0 = (\u03b8i, \u03b8\u2212i), s0 = s] , \u2200 s.\nAs shown in the definition, the condition of a MPG is admittedly rather strong and difficult to verify for general SGs. Macua et al. (2018); Gonza\u0301lez-Sa\u0301nchez & Herna\u0301ndez-Lerma (2013) found that continuous MPGs can model applications such as the great fish war (Levhari & Mirman, 1980), the stochastic lake game (Dechert & O\u2019Donnell, 2006), medium access control (Macua et al., 2018) etc. There are also efforts attempting to identify conditions such that a SG is a MPG, e.g., Macua et al. (2018); Leonardos et al. (2021); Mguni (2020). In Appendix B, we provide a more detailed discussion on MPGs, including a necessary condition (Lemma 5) of MPG, counterexamples of stage-wise potential games that are not MPG, sufficient conditions for a SG to be a MPG, and application examples of MPG. Nevertheless, identifying sufficient and necessary conditions and broadening the applications of MPG are important furture directions.\nGiven a policy \u03b8, we define the \u2018total potential function\u2019 \u03a6(\u03b8) := Es0\u223c\u03c1(\u00b7) [\u2211\u221e t=0 \u03b3 t\u03c6(st, at) \u2223\u2223 \u03c0\u03b8] for a MPG. The following proposition guarantees a MPG has at least one NE and it is a pure NE.\nProposition 1. (Proof in Appendix G) For a Markov potential game, there is at least one global maximum \u03b8\u2217 of the total potential function \u03a6, i.e.: \u03b8\u2217 \u2208 arg max\u03b8\u2208X \u03a6(\u03b8) that is a pure NE.\nFrom the definition of the total potential function we obtain the following relationship\nJi(\u03b8 \u2032 i, \u03b8\u2212i)\u2212 Ji(\u03b8i, \u03b8\u2212i) = \u03a6(\u03b8\u2032i, \u03b8\u2212i)\u2212 \u03a6(\u03b8i, \u03b8\u2212i). (11)\nThus, \u2207\u03b8iJi(\u03b8) = \u2207\u03b8i\u03a6(\u03b8), which means that gradient play (Equation (5)) is equivalent to running projected gradient ascent with respect to the total potential function \u03a6, i.e.: \u03b8(t+1) = ProjX (\u03b8(t) + \u03b7\u2207\u03b8\u03a6(\u03b8(t)i )), \u03b7 > 0. To measure the convergence to a NE, we define an -Nash equilibrium as follows:\nDefinition 4. ( -Nash equilibrium) Define the \u2018NE-gap\u2019 of a policy \u03b8 as:\nNE-gapi(\u03b8) := max \u03b8\u2032i\u2208Xi\nJi(\u03b8 \u2032 i, \u03b8\u2212i)\u2212 Ji(\u03b8i, \u03b8\u2212i); NE-gap(\u03b8) := max\ni NE-gapi(\u03b8).\nA policy \u03b8 is an -Nash equilibrium if: NE-gap(\u03b8) \u2264 .\nWe further assume that the MPG satisfies the following assumption.\nAssumption 2. For \u03b8 \u2208 X , the total potential function \u03a6(\u03b8) is bounded by: \u03a6min \u2264 \u03a6(\u03b8) \u2264 \u03a6max.\n4.1 EXACT GRADIENT PLAY - GLOBAL CONVERGENCE AND LOCAL GEOMETRY\nIn this section, we first focus on the global convergence for exact gradient play (5), where the gradient \u2207\u03b8iJi can be calculated exactly by agent i. The convergence result is given as follows: Theorem 3. (Global convergence to Nash equilibria, proof in Appendix H.) Given a MPG with potential function \u03c6(s, a), suppose the total potential function \u03a6 satisfies Assumption 2. Then with\nstepsize \u03b7 = (1\u2212\u03b3) 3 2 \u2211n i=1 |Ai| , \u03b8(t) asymptotically converge to a NE under gradient play (Equation (5)), i.e., limt\u2192\u221e NE-gap(\u03b8(t)) = 0. Further, we have:\n1\nT \u2211 1\u2264t\u2264T NE-gap(\u03b8(t))2 \u2264 2, whenever T \u2265 64M2(\u03a6max \u2212 \u03a6min)|S| \u2211n i=1 |Ai| (1\u2212 \u03b3)3 2 , (12)\nwhere M := max\u03b8,\u03b8\u2032\u2208X \u2225\u2225\u2225 d\u03b8d\u03b8\u2032 \u2225\u2225\u2225\u221e (by Assumption 1, we know that this quantity is well-defined).\nThe factor M is also known as the distribution mismatch coefficient that characterizes how the state visitation varies with the policies. Given an initial state distribution \u03c1 that has positive measure on every state, M can be at least bounded by M \u2264 11\u2212\u03b3 max\u03b8 \u2225\u2225\u2225d\u03b8\u03c1 \u2225\u2225\u2225\u221e\u2264 11\u2212\u03b3 1mins \u03c1(s) . Also note that Inequality (12) on the average term 1T \u2211 1\u2264t\u2264T NE-gap(\u03b8\n(t))2 could be translated to a constant probability guarantee on single NE-gap(\u03b8(t)). For instance, if we randomly pick one \u03b8(t) from 1 \u2264 t \u2264 T , then it guarantees that NE-gap(\u03b8(t))2 \u2264 3 \u00b7 2 with probability at least 23 .\n2 As a comparison with centralized learning, if we parameterize the policy in a centralized way, the size of the action space will be |A| = \u220fn i=1 |Ai| and the projected gradient ascent would needO ( |S| \u220fn i=1 |Ai| 2 ) steps to find an -optimal policy (Agarwal et al., 2020); whereas we only needO ( |S| \u2211n i=1 |Ai| 2 ) steps\nto find an -NE, which scales linearly with n. However, centralized parameterization can provably find a global optimum, while distributed parameterization can only find a NE.\nThough gradient play is guaranteed to converge to a NE, the exact NE which it converges to is uncertain, and depends on the initial point as well as the local geometry around the various NEs. As a preliminary study, we have the following characterization for two special types of NEs. More future investigation is needed for general NEs. Theorem 4. For Markov potential game with \u03a6min<\u03a6max (i.e., \u03a6 is not a constant function):\n\u2022 A strict NE \u03b8\u2217 is equivalent to a strict local maximum of the total potential function \u03a6, i.e.: \u2203 \u03b4, s.t. \u2200 \u03b8 6=\u03b8\u2217 that satisfies \u2016\u03b8 \u2212 \u03b8\u2217\u2016 \u2264 \u03b4, \u03b8 \u2208 X , we have \u03a6(\u03b8) < \u03a6(\u03b8\u2217).\n\u2022 Any fully mixed NE \u03b8\u2217 is a saddle point with regard to the total potential function \u03a6, i.e.: \u2200 \u03b4 > 0, \u2203 \u03b8, s.t. \u2016\u03b8\u2212\u03b8\u2217\u2016 \u2264 \u03b4 and \u03a6(\u03b8)>\u03a6(\u03b8\u2217).\nTheorem 4 implies that strict NEs are asymptotically stable under first-order methods such as gradient play; while the fully mixed NEs are not stable under gradient play. We remark that the conclusion about strict NE in Theorem 4 does not hold for settings other than tabular MPG; for instance, for continuous games, one can use quadratic functions to construct simple counterexamples (Mazumdar et al., 2020). Also, similar to the remark after Theorem 2, this theorem focuses on the local geometry of the NEs but does not claim the global existence or convergence of either strict NEs or fully mixed NEs.\n4.2 SAMPLE-BASED LEARNING: ALGORITHM DESIGN AND SAMPLE COMPLEXITY\nIn this section, we no longer assume access to the exact gradient, but instead estimate it via samples. Throughout the section, we make the following additional assumption on MPGs: Assumption 3. ((\u03c4, \u03c3S)-Sufficient exploration on states) There exist a positive integer \u03c4 and a \u03c3S \u2208 (0, 1) such that for any policy \u03b8 and any initial state-action pair (s, ai), \u2200i, we have\nPr(s\u03c4 |s0 = s, a0 = a) \u2265 \u03c3S , \u2200s\u03c4 . (13)\nIt says that every state has a positive probability of being visited after some time. This assumption is common in proving finite time convergence (e.g. (Qu et al., 2019; Srikant & Ying, 2019)).\nWe further introduction the state transition probability under \u03b8 P \u03b8S : S \u00d7 S \u2192 R as:\nP \u03b8S(s \u2032|s) := \u2211 a \u03c0\u03b8(a|s)P (s\u2032|s, a).\nWe consider fully decentralized learning, where agent i\u2019s observation only includes state st, its own action ai,t, and its own reward ri,t := ri(st, at) at time t. Such fully decentralized learning is\n2Here 3, 2 3 could be replaced by 1 1\u2212p , p where p \u2208 (0, 1) is a probability.\nplausible due to the fact that when \u03b8\u2212i is fixed, agent i can be treated as an independent learner with the underlying MDP being the \u2018averaged\u2019 MDP described in Section 2. With this key observation, we design a two-timescale \u2018model-based\u2019 on-policy learning algorithm, where agents perform policy evaluation in the inner loop and gradient ascent at the outer loop. The algorithm is provided in Algorithm 1. Roughly, it consists of three main steps: 1) (Inner loop) Estimate the averaged transition probability and reward using on-policy samples P \u03b8i , r \u03b8 i , P \u03b8 S . 2) (Inner loop) Calculate averaged Q-function Q\u03b8i and discounted state visitation distribution d\u03b8, and compute the estimated gradient accordingly, 3) (Outer loop) Running projected gradient ascent with estimated gradients. Before discussing our algorithm in more detail, we highlight that the idea of using the \u201caveraged\u201d MDP can be used to design other learning methods including model-free methods, e.g., using the temporal difference methods to perform policy evaluation. One caveat is that the \u201caveraged\u201d MDP is only welldefined when all the other agents use fixed policies. This makes it difficult to extend the two-timescale framework to single-timescale settings, which is an interesting future direction.\nAlgorithm 1 Sample-based learning Require: learning rate \u03b7, greedy parameter \u03b1, sample trajectory length TJ , total iteration steps TG\nFor each agent i for k = 0, 1 . . . , TG \u2212 1 do\nfor t = 0, 1, . . . , TJ do Implement policy \u03b8(k) and collect trajectory D(k)i : D (k) i \u2190D\n(k) i \u222a{st, ai,t, ri,t}, ai,t\u223c\u03c0\u03b8(k)i (\u00b7|s)\nend for Estimate P\u0302 \u03b8i , r\u0302 \u03b8 i , P\u0302 \u03b8 S , M\u0302 \u03b8 i by Equation (14), Equation (15), Equation (16) respectively.\nCalculate Q\u0302\u03b8i , d\u0302\u03b8 by Equation (17), Equation (18) respectively. Estimate the gradient by Equation (19): Run projected gradient ascent as in Equation (20)\nend for\nStep 1: empirical estimation of P \u03b8i , r\u03b8i , P \u03b8S : Given a sequence {st, ai,t, ri,t} TJ t=0 generated by a\npolicy \u03b8 := (\u03b8i, \u03b8\u2212i), the empirical estimation P\u0302 \u03b8i of P \u03b8 i is given by:\nP\u0302 \u03b8i (s \u2032|s, ai) :=  \u2211TJ\u22121 t=0 1{st+1=s \u2032,st=s,ai,t=ai}\u2211TJ\u22121 t=1 1{st=s,ai,t=ai} , \u2211TJ\u22121 t=1 1{st = s, ai,t = ai} \u2265 1\n1{s\u2032 = s}, \u2211TJ\u22121 t=1 1{st = s, ai,t = ai} = 0\n(14)\nHere we separately treat the special case where the state and action pair is not visited through the whole trajectory, i.e., \u2211TJ\u22121 t=1 1{st = s, ai,t = ai} = 0 to make P\u0302 \u03b8i well-defined.\nSimilarly, the estimates r\u0302\u03b8i , P\u0302 \u03b8 S of r \u03b8 i , P \u03b8 S are given by:\nr\u0302\u03b8i (s, ai) :=  \u2211TJ t=0 }]